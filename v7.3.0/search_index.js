var documenterSearchIndex = {"docs":
[{"location":"samples/#Sampling","page":"Samples","title":"Sampling","text":"Sampling methods are provided by the QuasiMonteCarlo package.\n\nThe syntax for sampling in an interval or region is the following:\n\nsample(n, lb, ub, S::SamplingAlgorithm)\n\nwhere lb and ub are, respectively, the lower and upper bounds. There are many sampling algorithms to choose from:\n\nGrid sample\n\nsample(n, lb, ub, GridSample())\n\nUniform sample\n\nsample(n, lb, ub, RandomSample())\n\nSobol sample\n\nsample(n, lb, ub, SobolSample())\n\nLatin Hypercube sample\n\nsample(n, lb, ub, LatinHypercubeSample())\n\nLow Discrepancy sample\n\nsample(n, lb, ub, HaltonSample())\n\nSample on section\n\nsample(n, lb, ub, SectionSample())","category":"section"},{"location":"samples/#Adding-a-new-sampling-method","page":"Samples","title":"Adding a new sampling method","text":"Adding a new sampling method is a two-step process:\n\nAdding a new SamplingAlgorithm type\nOverloading the sample function with the new type.\n\nExample\n\nstruct NewAmazingSamplingAlgorithm{OPTIONAL} <: QuasiMonteCarlo.SamplingAlgorithm end\n\nfunction sample(n, lb, ub, ::NewAmazingSamplingAlgorithm)\n    if lb isa Number\n        ...\n        return x\n    else\n        ...\n        return Tuple.(x)\n    end\nend","category":"section"},{"location":"tutorials/#Surrogates-101","page":"Basics","title":"Surrogates 101","text":"Let's start with something easy to get our hands dirty. Let's say we want to build a surrogate for f(x) = log(x) cdot x^2+x^3.","category":"section"},{"location":"tutorials/#RBF","page":"Basics","title":"RBF","text":"We will first use the radial basis surrogate for demonstrations.\n\n# Importing the package\nusing Surrogates\n\n# Defining the function\nf = x -> log(x) * x^2 + x^3\n\n# Sampling points from the function\nlb = 1.0\nub = 10.0\nx = sample(50, lb, ub, SobolSample())\ny = f.(x)\n\n# Constructing the surrogate\nmy_radial_basis = RadialBasis(x, y, lb, ub)\n\n# Predicting at x=5.4\napprox = my_radial_basis(5.4)\n\nWe can plot to see how well the surrogate performs compared to the true function.\n\nusing Plots\n\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lb, ub), legend = :top)\nxs = 1.0:0.001:10.0\nplot!(xs, f.(xs), label = \"True function\", legend = :top)\nplot!(xs, my_radial_basis.(xs); label = \"RBF\", legend = :top)\n\nIt fits quite well! Now, let's now see an example in 2D.\n\nusing Surrogates\nusing LinearAlgebra\n\nf = x -> x[1] * x[2]\n\nlb = [1.0, 2.0]\nub = [10.0, 8.5]\nx = sample(50, lb, ub, SobolSample())\ny = f.(x)\n\nmy_radial_basis = RadialBasis(x, y, lb, ub)\n\n# Predicting at x=(1.0,1.4)\napprox = my_radial_basis((1.0, 1.4))","category":"section"},{"location":"tutorials/#Kriging","page":"Basics","title":"Kriging","text":"Let's now use the Kriging surrogate, which is a single-output Gaussian process. This surrogate has a nice feature - not only does it approximate the solution at a point, it also calculates the standard error at such a point.\n\nusing Surrogates\n\nf = x -> exp(-x) * x^2 + x^3\n\nlb = 0.0\nub = 10.0\nx = sample(50, lb, ub, RandomSample())\ny = f.(x)\n\np = 1.9\nmy_krig = Kriging(x, y, lb, ub, p = p)\n\n# Predicting at x=5.4\napprox = my_krig(5.4)\n\n# Predicting error at x=5.4\nstd_err = std_error_at_point(my_krig, 5.4)\n\nLet's now optimize the Kriging surrogate using the lower confidence bound method. This is just a one-liner:\n\nsurrogate_optimize!(\n    f, LCBS(), lb, ub, my_krig, RandomSample(); maxiters = 10, num_new_samples = 10)\n\nSurrogate optimization methods have two purposes: they both sample the space in unknown regions and look for the minima at the same time.","category":"section"},{"location":"tutorials/#Lobachevsky-integral","page":"Basics","title":"Lobachevsky integral","text":"The Lobachevsky surrogate has the nice feature of having a closed formula for its integral, which is something that other surrogates are missing. Let's compare it with QuadGK:\n\nusing Surrogates\nusing QuadGK\n\nobj = x -> 3 * x + log(x)\na = 1.0\nb = 4.0\nx = sample(2000, a, b, SobolSample())\ny = obj.(x)\nalpha = 2.0\nn = 6\nmy_loba = LobachevskySurrogate(x, y, a, b, alpha = alpha, n = n)\n\n#1D integral\nint_1D = lobachevsky_integral(my_loba, a, b)\nint = quadgk(obj, a, b)\nint_val_true = int[1] - int[2]\nprintln(int_1D)\nprintln(int_val_true)","category":"section"},{"location":"tutorials/#NeuralSurrogate","page":"Basics","title":"NeuralSurrogate","text":"Basic example of fitting a neural network on a simple function of two variables.\n\nusing Surrogates\nusing Flux\nusing Statistics\n\nf = x -> x[1]^2 + x[2]^2\n# Flux models are in single precision by default.\n# Thus, single precision will also be used here for our training samples.\nbounds = Float32[-1.0, -1.0], Float32[1.0, 1.0]\n\nx_train = sample(500, bounds..., SobolSample())\ny_train = f.(x_train)\n\n# Perceptron with one hidden layer of 20 neurons.\nmodel = Chain(Dense(2, 20, relu), Dense(20, 1))\n\n# Training of the neural network\nlearning_rate = 0.1\noptimizer = Descent(learning_rate)  # Simple gradient descent. See Flux documentation for other options.\nn_epochs = 50\nsgt = NeuralSurrogate(x_train, y_train, bounds..., model = model,\n    opt = optimizer, n_epochs = n_epochs)\n\n# Testing the new model\nx_test = sample(30, bounds..., RandomSample())\ntest_error = mean(abs2, sgt(x)[1] - f(x) for x in x_test)","category":"section"},{"location":"InverseDistance/#InverseDistance-Surrogate-Tutorial","page":"InverseDistance","title":"InverseDistance Surrogate Tutorial","text":"The Inverse Distance Surrogate is an interpolating method, and in this method, the unknown points are calculated with a weighted average of the sampling points. This model uses the inverse distance between the unknown and training points to predict the unknown point. We do not need to fit this model because the response of an unknown point x is computed with respect to the distance between x and the training points.\n\nLet's optimize the following function to use Inverse Distance Surrogate:\n\nf(x) = sin(x) + sin(x)^2 + sin(x)^3\n\n.\n\nFirst of all, we have to import these two packages: Surrogates and Plots.\n\nusing Surrogates\nusing Plots","category":"section"},{"location":"InverseDistance/#Sampling","page":"InverseDistance","title":"Sampling","text":"We choose to sample f in 1000 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy, this can be done by passing HaltonSample() to the sample function.\n\nf(x) = sin(x) + sin(x)^2 + sin(x)^3\n\nn_samples = 100\nlower_bound = 0.0\nupper_bound = 10.0\nx = sample(n_samples, lower_bound, upper_bound, HaltonSample())\ny = f.(x)\n\nscatter(x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"InverseDistance/#Building-a-Surrogate","page":"InverseDistance","title":"Building a Surrogate","text":"InverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\nprediction = InverseDistance(5.0)\n\nNow, we will simply plot InverseDistance:\n\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(InverseDistance, label = \"Surrogate function\",\n    xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"InverseDistance/#Optimizing","page":"InverseDistance","title":"Optimizing","text":"Having built a surrogate, we can now use it to search for minima in our original function f.\n\nTo optimize using our surrogate we call surrogate_optimize! method. We choose to use Stochastic RBF as the optimization technique and again Sobol sampling as the sampling technique.\n\nsurrogate_optimize!(\n    f, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample())\nscatter(x, y, label = \"Sampled points\", legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(InverseDistance, label = \"Surrogate function\",\n    xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"InverseDistance/#Inverse-Distance-Surrogate-Tutorial-(ND):","page":"InverseDistance","title":"Inverse Distance Surrogate Tutorial (ND):","text":"First of all we will define the Schaffer function we are going to build a surrogate for. Notice, how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.\n\nusing Plots\ndefault(c = :matter, legend = false, xlabel = \"x\", ylabel = \"y\")\nusing Surrogates\n\nfunction schaffer(x)\n    x1 = x[1]\n    x2 = x[2]\n    fact1 = (sin(x1^2 - x2^2))^2 - 0.5\n    fact2 = (1 + 0.001 * (x1^2 + x2^2))^2\n    y = 0.5 + fact1 / fact2\nend","category":"section"},{"location":"InverseDistance/#Sampling-2","page":"InverseDistance","title":"Sampling","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 100 samples of the space using Sobol Sequences. We then evaluate our function on all the sampling points.\n\nn_samples = 100\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);\n\nx, y = -5:10, 0:15\np1 = surface(x, y, (x1, x2) -> schaffer((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> schaffer((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")","category":"section"},{"location":"InverseDistance/#Building-a-surrogate","page":"InverseDistance","title":"Building a surrogate","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.\n\nInverseDistance = InverseDistanceSurrogate(xys, zs, lower_bound, upper_bound)\n\np1 = surface(x, y, (x, y) -> InverseDistance([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> InverseDistance([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Surrogate\")","category":"section"},{"location":"InverseDistance/#Optimizing-2","page":"InverseDistance","title":"Optimizing","text":"With our surrogate, we can now search for the minima of the function.\n\nNotice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.\n\nsize(xys)\n\nsurrogate_optimize!(schaffer, SRBF(), lower_bound, upper_bound,\n    InverseDistance, SobolSample(), maxiters = 10)\n\nsize(xys)\n\np1 = surface(x, y, (x, y) -> InverseDistance([x y]))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nzs = schaffer.(xys)\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> InverseDistance([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2)","category":"section"},{"location":"wendland/#Wendland-Surrogate-Tutorial","page":"Wendland","title":"Wendland Surrogate Tutorial","text":"The Wendland surrogate is a compact surrogate: it allocates much less memory than other surrogates. The coefficients are found using an iterative solver.\n\nf = x - exp(-x^2)\n\nusing Surrogates\nusing Plots\n\nn = 100\nlower_bound = 0.0\nupper_bound = 1.0\nf = x -> exp(-x^2)\nx = sample(n, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n\nWe choose to sample f in 100 points between 5 and 25 using sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"section"},{"location":"wendland/#Building-Surrogate","page":"Wendland","title":"Building Surrogate","text":"The choice of the right parameter is especially important here: a slight change in ϵ would produce a totally different fit. Try it yourself with this function!\n\nwend = Wendland(x, y, lower_bound, upper_bound, eps = 0.45)\n\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(wend, label = \"Surrogate function\", xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"ackley/#Ackley-Function","page":"Ackley function","title":"Ackley Function","text":"The Ackley function is defined as: f(x) = -a*exp(-bsqrtfrac1dsum_i=1^d x_i^2) - exp(frac1d sum_i=1^d cos(cx_i)) + a + exp(1) Usually the recommended values are: a = 20, b = 02 and c = 2pi\n\nLet's see the 1D case.\n\nusing Surrogates\nusing Plots\n\nNow, let's define the Ackley function:\n\nfunction ackley(x)\n    a, b, c = 20.0, 0.2, 2.0 * π\n    len_recip = inv(length(x))\n    sum_sqrs = zero(eltype(x))\n    sum_cos = sum_sqrs\n    for i in x\n        sum_cos += cos(c * i)\n        sum_sqrs += i^2\n    end\n    return (-a * exp(-b * sqrt(len_recip * sum_sqrs)) -\n            exp(len_recip * sum_cos) + a + 2.71)\nend\n\nn = 100\nlb = -32.768\nub = 32.768\nx = sample(n, lb, ub, SobolSample())\ny = ackley.(x)\nxs = lb:0.001:ub\nscatter(x, y, label = \"Sampled points\", xlims = (lb, ub), ylims = (0, 30), legend = :top)\nplot!(xs, ackley.(xs), label = \"True function\", legend = :top)\n\nmy_rad = RadialBasis(x, y, lb, ub)\nmy_loba = LobachevskySurrogate(x, y, lb, ub)\n\nscatter(x, y, label = \"Sampled points\", xlims = (lb, ub), ylims = (0, 30), legend = :top)\nplot!(xs, ackley.(xs), label = \"True function\", legend = :top)\nplot!(xs, my_rad.(xs), label = \"Polynomial expansion\", legend = :top)\nplot!(xs, my_loba.(xs), label = \"Lobachevsky\", legend = :top)\n\nThe fit looks good. Let's now see if we are able to find the minimum value using optimization methods:\n\nsurrogate_optimize!(ackley, DYCORS(), lb, ub, my_rad, RandomSample())\nscatter(x, y, label = \"Sampled points\", xlims = (lb, ub), ylims = (0, 30), legend = :top)\nplot!(xs, ackley.(xs), label = \"True function\", legend = :top)\nplot!(xs, my_rad.(xs), label = \"Radial basis optimized\", legend = :top)\n\nThe DYCORS method successfully finds the minimum.","category":"section"},{"location":"lobachevsky/#Lobachevsky-Surrogate-Tutorial","page":"Lobachevsky","title":"Lobachevsky Surrogate Tutorial","text":"Lobachevsky splines function is a function that is used for univariate and multivariate scattered interpolation. Introduced by Lobachevsky in 1842 to investigate errors in astronomical measurements.\n\nWe are going to use a Lobachevsky surrogate to optimize f(x)=sin(x)+sin(103 * x).\n\nFirst of all import Surrogates and Plots.\n\nusing Surrogates\nusing Plots","category":"section"},{"location":"lobachevsky/#Sampling","page":"Lobachevsky","title":"Sampling","text":"We choose to sample f in 100 points between 0 and 4 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.\n\nf(x) = sin(x) + sin(10 / 3 * x)\nn_samples = 100\nlower_bound = 1.0\nupper_bound = 4.0\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound))\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound))","category":"section"},{"location":"lobachevsky/#Building-a-surrogate","page":"Lobachevsky","title":"Building a surrogate","text":"With our sampled points, we can build the Lobachevsky surrogate using the LobachevskySurrogate function.\n\nlobachevsky_surrogate behaves like an ordinary function, which we can simply plot. Alpha is the shape parameter, and n specifies how close you want Lobachevsky function to be to the radial basis function.\n\nalpha = 2.0\nn = 6\nlobachevsky_surrogate = LobachevskySurrogate(\n    x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), legend = true)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound))\nplot!(\n    lobachevsky_surrogate, label = \"Surrogate function\", xlims = (lower_bound, upper_bound))","category":"section"},{"location":"lobachevsky/#Optimizing","page":"Lobachevsky","title":"Optimizing","text":"Having built a surrogate, we can now use it to search for minima in our original function f.\n\nTo optimize using our surrogate we call surrogate_optimize! method. We choose to use Stochastic RBF as the optimization technique and again Sobol sampling as the sampling technique.\n\nsurrogate_optimize!(\n    f, SRBF(), lower_bound, upper_bound, lobachevsky_surrogate, SobolSample())\nscatter(x, y, label = \"Sampled points\")\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound))\nplot!(\n    lobachevsky_surrogate, label = \"Surrogate function\", xlims = (lower_bound, upper_bound))\n\nIn the example below, it shows how to use lobachevsky_surrogate for higher dimension problems.","category":"section"},{"location":"lobachevsky/#Lobachevsky-Surrogate-Tutorial-(ND):","page":"Lobachevsky","title":"Lobachevsky Surrogate Tutorial (ND):","text":"First of all, we will define the Schaffer function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.\n\nusing Plots\ndefault(c = :matter, legend = false, xlabel = \"x\", ylabel = \"y\")\nusing Surrogates\n\nfunction schaffer(x)\n    x1 = x[1]\n    x2 = x[2]\n    fact1 = x1^2\n    fact2 = x2^2\n    y = fact1 + fact2\nend","category":"section"},{"location":"lobachevsky/#Sampling-2","page":"Lobachevsky","title":"Sampling","text":"Let's define our bounds, this time we are working in two dimensions. In particular, we want our first dimension x to have bounds 0, 8, and 0, 8 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.\n\nn_samples = 60\nlower_bound = [0.0, 0.0]\nupper_bound = [8.0, 8.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);\n\nx, y = 0:8, 0:8\np1 = surface(x, y, (x1, x2) -> schaffer((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> schaffer((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")","category":"section"},{"location":"lobachevsky/#Building-a-surrogate-2","page":"Lobachevsky","title":"Building a surrogate","text":"Using the sampled points, we build the surrogate, the steps are analogous to the 1-dimensional case.\n\nLobachevsky = LobachevskySurrogate(\n    xys, zs, lower_bound, upper_bound, alpha = [2.4, 2.4], n = 8)\n\np1 = surface(x, y, (x, y) -> Lobachevsky([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> Lobachevsky([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Surrogate\")","category":"section"},{"location":"lobachevsky/#Optimizing-2","page":"Lobachevsky","title":"Optimizing","text":"With our surrogate, we can now search for the minima of the function.\n\nNotice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.\n\nsize(Lobachevsky.x)\n\nsurrogate_optimize!(schaffer, SRBF(), lower_bound, upper_bound, Lobachevsky,\n    SobolSample(), maxiters = 1, num_new_samples = 10)\n\nsize(Lobachevsky.x)\n\np1 = surface(x, y, (x, y) -> Lobachevsky([x y]))\nxys = Lobachevsky.x\nxs = [i[1] for i in xys]\nys = [i[2] for i in xys]\nzs = schaffer.(xys)\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> Lobachevsky([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2)","category":"section"},{"location":"xgboost/#Random-Forests-Surrogate-Tutorial","page":"XGBoost","title":"Random Forests Surrogate Tutorial","text":"Random forests is a supervised learning algorithm that randomly creates and merges multiple decision trees into one forest.\n\nWe are going to use a xgboost surrogate to optimize f(x)=sin(x)+sin(103 * x).\n\nFirst of all import Surrogates, XGBoost and Plots.\n\nusing Surrogates\nusing XGBoost\nusing Plots","category":"section"},{"location":"xgboost/#Sampling","page":"XGBoost","title":"Sampling","text":"We choose to sample f in 100 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.\n\nf(x) = sin(x) + sin(10 / 3 * x)\nn_samples = 100\nlower_bound = 2.7\nupper_bound = 7.5\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound))\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"xgboost/#Building-a-surrogate","page":"XGBoost","title":"Building a surrogate","text":"With our sampled points, we can build the XGBoost surrogate using the XGBoostSurrogate function.\n\nxgboost_surrogate behaves like an ordinary function, which we can simply plot. Additionally, you can specify the number of trees created using the parameter num_round\n\nxgboost_surrogate = XGBoostSurrogate(\n    x, y, lower_bound, upper_bound, num_round = 10)\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(xgboost_surrogate, label = \"Surrogate function\",\n    xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"xgboost/#Optimizing","page":"XGBoost","title":"Optimizing","text":"Having built a surrogate, we can now use it to search for minima in our original function f.\n\nTo optimize using our surrogate, we call surrogate_optimize! method. We choose to use Stochastic RBF as the optimization technique and again Sobol sampling as the sampling technique.\n\nsurrogate_optimize!(\n    f, SRBF(), lower_bound, upper_bound, xgboost_surrogate, SobolSample())\nscatter(x, y, label = \"Sampled points\")\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(xgboost_surrogate, label = \"Surrogate function\",\n    xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"xgboost/#Random-Forest-ND","page":"XGBoost","title":"Random Forest ND","text":"First of all we will define the Bukin Function N. 6 function we are going to build a surrogate for.\n\nusing Plots\nusing Surrogates\nusing XGBoost\n\nfunction bukin6(x)\n    x1 = x[1]\n    x2 = x[2]\n    term1 = 100 * sqrt(abs(x2 - 0.01 * x1^2))\n    term2 = 0.01 * abs(x1 + 10)\n    y = term1 + term2\nend","category":"section"},{"location":"xgboost/#Sampling-2","page":"XGBoost","title":"Sampling","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 100 samples of the space using Sobol Sequences. We then evaluate our function on all the sampling points.\n\nn_samples = 100\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = bukin6.(xys)\n\nx, y = -5:10, 0:15\np1 = surface(x, y, (x1, x2) -> bukin6((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> bukin6((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")","category":"section"},{"location":"xgboost/#Building-a-surrogate-2","page":"XGBoost","title":"Building a surrogate","text":"Using the sampled points, we build the surrogate, the steps are analogous to the 1-dimensional case.\n\nXGBoost = XGBoostSurrogate(xys, zs, lower_bound, upper_bound)\n\np1 = surface(x, y, (x, y) -> XGBoost([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> XGBoost([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Surrogate\")","category":"section"},{"location":"xgboost/#Optimizing-2","page":"XGBoost","title":"Optimizing","text":"With our surrogate, we can now search for the minima of the function.\n\nNotice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.\n\nsize(xys)\n\nsurrogate_optimize!(\n    bukin6, SRBF(), lower_bound, upper_bound, XGBoost, SobolSample(), maxiters = 20)\n\nsize(xys)\n\np1 = surface(x, y, (x, y) -> XGBoost([x y]))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nzs = bukin6.(xys)\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> XGBoost([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2)","category":"section"},{"location":"tensor_prod/#Tensor-product-function","page":"Tensor product","title":"Tensor product function","text":"The tensor product function is defined as: f(x) = prod_i=1^d cos(api x_i)\n\nLet's import Surrogates and Plots:\n\nusing Surrogates\nusing Plots\n\nDefine the 1D objective function:\n\nfunction f(x)\n    a = 0.5\n    return cos(a * pi * x)\nend\n\nn = 100\nlb = -5.0\nub = 5.0\na = 0.5\nx = sample(n, lb, ub, SobolSample())\ny = f.(x)\nxs = lb:0.001:ub\nscatter(x, y, label = \"Sampled points\", xlims = (lb, ub), ylims = (-1, 1), legend = :top)\nplot!(xs, f.(xs), label = \"True function\", legend = :top)\n\nFitting and plotting different surrogates:\n\nloba_1 = LobachevskySurrogate(x, y, lb, ub)\nkrig = Kriging(x, y, lb, ub)\nscatter(\n    x, y, label = \"Sampled points\", xlims = (lb, ub), ylims = (-2.5, 2.5), legend = :bottom)\nplot!(xs, f.(xs), label = \"True function\", legend = :top)\nplot!(xs, loba_1.(xs), label = \"Lobachevsky\", legend = :top)\nplot!(xs, krig.(xs), label = \"Kriging\", legend = :top)","category":"section"},{"location":"welded_beam/#Welded-beam-function","page":"Welded beam function","title":"Welded beam function","text":"The welded beam function is defined as: f(hlt) = sqrtfraca^2 + b^2 + ablsqrt025(l^2+(h+t)^2) With: a = frac6000sqrt2hl b = frac6000(14 + 05l)*sqrt025(l^2+(h+t)^2)2*0707hl(fracl^212+025*(h+t)^2)\n\nIt has 3 dimension.\n\nusing Surrogates\nusing Plots\nusing LinearAlgebra\n\nDefine the objective function:\n\nfunction f(x)\n    h = x[1]\n    l = x[2]\n    t = x[3]\n    a = 6000 / (sqrt(2) * h * l)\n    b = (6000 * (14 + 0.5 * l) * sqrt(0.25 * (l^2 + (h + t)^2))) /\n        (2 * (0.707 * h * l * (l^2 / 12 + 0.25 * (h + t)^2)))\n    return (sqrt(a^2 + b^2 + l * a * b)) / (sqrt(0.25 * (l^2 + (h + t)^2)))\nend\n\nn = 300\nd = 3\nlb = [0.125, 5.0, 5.0]\nub = [1.0, 10.0, 10.0]\nx = sample(n, lb, ub, SobolSample())\ny = f.(x)\nn_test = 1000\nx_test = sample(n_test, lb, ub, GoldenSample())\ny_true = f.(x_test)\n\nmy_rad = RadialBasis(x, y, lb, ub)\ny_rad = my_rad.(x_test)\nmse_rad = norm(y_true - y_rad, 2) / n_test\nprintln(\"MSE Radial: $mse_rad\")\n\nmy_loba = LobachevskySurrogate(x, y, lb, ub)\ny_loba = my_loba.(x_test)\nmse_rad = norm(y_true - y_loba, 2) / n_test\nprintln(\"MSE Lobachevsky: $mse_rad\")","category":"section"},{"location":"parallel/#Parallel-Optimization","page":"Parallel Optimization","title":"Parallel Optimization","text":"There are some situations where it can be beneficial to run multiple optimizations in parallel. For example, if your objective function is very expensive to evaluate, you may want to run multiple evaluations in parallel.","category":"section"},{"location":"parallel/#Ask-Tell-Interface","page":"Parallel Optimization","title":"Ask-Tell Interface","text":"To enable parallel optimization, we make use of an Ask-Tell interface. The user will construct the initial surrogate model the same way as for non-parallel surrogate models, but instead of using surrogate_optimize!, the user will use potential_optimal_points. This will return the coordinates of points that the optimizer has determined are most useful to evaluate next. How the user evaluates these points is up to them. The Ask-Tell interface requires more manual control than surrogate_optimize!, but it allows for more flexibility. After the point has been evaluated, the user will tell the surrogate model the new points with the update! function.","category":"section"},{"location":"parallel/#Virtual-Points","page":"Parallel Optimization","title":"Virtual Points","text":"To ensure that points of interest returned by potential_optimal_points are sufficiently far from each other, the function makes use of virtual points. They are used as follows:\n\npotential_optimal_points is told to return n points.\nThe point with the highest merit function value is selected.\nThis point is now treated as a virtual point and is assigned a temporary value that changes the landscape of the merit function. How the temporary value is chosen depends on the strategy used. (see below)\nThe point with the new highest merit is selected.\nThe process is repeated until n points have been selected.\n\nThe following strategies are available for virtual point selection for all optimization algorithms:\n\n\"Minimum Constant Liar (MinimumConstantLiar)\":\nThe virtual point is assigned using the lowest known value of the merit function across all evaluated points.\n\"Mean Constant Liar (MeanConstantLiar)\":\nThe virtual point is assigned using the mean of the merit function across all evaluated points.\n\"Maximum Constant Liar (MaximumConstantLiar)\":\nThe virtual point is assigned using the greatest known value of the merit function across all evaluated points.\n\nFor Kriging surrogates, specifically, the above and following strategies are available:\n\n\"Kriging Believer (KrigingBeliever):\nThe virtual point is assigned using the mean of the Kriging surrogate at the virtual point.\n\"Kriging Believer Upper Bound (KrigingBelieverUpperBound)\":\nThe virtual point is assigned using 3sigma above the mean of the Kriging surrogate at the virtual point.\n\"Kriging Believer Lower Bound (KrigingBelieverLowerBound)\":\nThe virtual point is assigned using 3sigma below the mean of the Kriging surrogate at the virtual point.\n\nIn general, MinimumConstantLiar and KrigingBelieverLowerBound tend to favor exploitation, while MaximumConstantLiar and KrigingBelieverUpperBound tend to favor exploration. MeanConstantLiar and KrigingBeliever tend to be compromises between the two.","category":"section"},{"location":"parallel/#Examples","page":"Parallel Optimization","title":"Examples","text":"using Surrogates\n\nlb = 0.0\nub = 10.0\nf = x -> log(x) * exp(x)\nx = sample(5, lb, ub, SobolSample())\ny = f.(x)\n\nmy_k = Kriging(x, y, lb, ub)\n\nfor _ in 1:10\n    new_x,\n    eis = potential_optimal_points(\n        EI(), MeanConstantLiar(), lb, ub, my_k, SobolSample(), 3)\n    update!(my_k, new_x, f.(new_x))\nend","category":"section"},{"location":"polychaos/#Polynomial-Chaos-Surrogate-Tutorial","page":"Polynomial Chaos","title":"Polynomial Chaos Surrogate Tutorial","text":"We can create a surrogate using a polynomial expansion, with a different polynomial basis depending on the distribution of the data we are trying to fit. Under the hood, PolyChaos.jl has been used. It is possible to specify a type of polynomial for each dimension of the problem.","category":"section"},{"location":"polychaos/#Sampling","page":"Polynomial Chaos","title":"Sampling","text":"We choose to sample f in 100 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy. This can be done by passing HaltonSample() to the sample function.\n\nusing Surrogates\nusing PolyChaos\nusing Plots\n\nn = 100\nlower_bound = 1.0\nupper_bound = 6.0\nx = sample(n, lower_bound, upper_bound, HaltonSample())\nf = x -> log(x) * x + sin(x)\ny = f.(x)\nscatter(x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"polychaos/#Building-a-Surrogate","page":"Polynomial Chaos","title":"Building a Surrogate","text":"poly1 = PolynomialChaosSurrogate(x, y, lower_bound, upper_bound)\npoly2 = PolynomialChaosSurrogate(\n    x, y, lower_bound, upper_bound, orthopolys = GaussOrthoPoly(5))\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(poly1, label = \"First polynomial\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(poly2, label = \"Second polynomial\", xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"kriging/#Kriging-Surrogate-Tutorial-(1D)","page":"Kriging","title":"Kriging Surrogate Tutorial (1D)","text":"Kriging or Gaussian process regression, is a method of interpolation in which the interpolated values are modeled by a Gaussian process.\n\nWe are going to use a Kriging surrogate to optimize f(x)=(6x-2)^2sin(12x-4).\n\nFirst of all, import Surrogates and Plots.\n\nusing Surrogates\nusing Plots","category":"section"},{"location":"kriging/#Sampling","page":"Kriging","title":"Sampling","text":"We choose to sample f in 100 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence; This can be done by passing SobolSample() to the sample function.\n\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\n\nn_samples = 100\nlower_bound = 0.0\nupper_bound = 1.0\n\nxs = lower_bound:0.001:upper_bound\n\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n\nscatter(\n    x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound), ylims = (-7, 17))\nplot!(xs, f.(xs), label = \"True function\", legend = :top)","category":"section"},{"location":"kriging/#Building-a-surrogate","page":"Kriging","title":"Building a surrogate","text":"With our sampled points, we can build the Kriging surrogate using the Kriging function.\n\nkriging_surrogate behaves like an ordinary function, which we can simply plot. A nice statistical property of this surrogate is being able to calculate the error of the function at each point. We plot this as a confidence interval using the ribbon argument.\n\nkriging_surrogate = Kriging(x, y, lower_bound, upper_bound)\n\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), ylims = (-7, 17), legend = :top)\nplot!(xs, f.(xs), label = \"True function\", legend = :top)\nplot!(xs, kriging_surrogate.(xs), label = \"Surrogate function\",\n    ribbon = p -> std_error_at_point(kriging_surrogate, p), legend = :top)","category":"section"},{"location":"kriging/#Optimizing","page":"Kriging","title":"Optimizing","text":"Having built a surrogate, we can now use it to search for minima in our original function f.\n\nTo optimize using our surrogate, we call surrogate_optimize! method. We choose to use Stochastic RBF as the optimization technique and again Sobol sampling as the sampling technique.\n\nsurrogate_optimize!(\n    f, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample())\n\nscatter(x, y, label = \"Sampled points\", ylims = (-7, 7), legend = :top)\nplot!(xs, f.(xs), label = \"True function\", legend = :top)\nplot!(xs, kriging_surrogate.(xs), label = \"Surrogate function\",\n    ribbon = p -> std_error_at_point(kriging_surrogate, p), legend = :top)","category":"section"},{"location":"kriging/#Kriging-Surrogate-Tutorial-(ND)","page":"Kriging","title":"Kriging Surrogate Tutorial (ND)","text":"First of all, let's define the function we are going to build a surrogate for. Notice how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.\n\nusing Plots\ndefault(c = :matter, legend = false, xlabel = \"x\", ylabel = \"y\")\nusing Surrogates\n\nfunction branin(x)\n    x1 = x[1]\n    x2 = x[2]\n    a = 1\n    b = 5.1 / (4 * π^2)\n    c = 5 / π\n    r = 6\n    s = 10\n    t = 1 / (8π)\n    a * (x2 - b * x1 + c * x1 - r)^2 + s * (1 - t) * cos(x1) + s\nend","category":"section"},{"location":"kriging/#Sampling-2","page":"Kriging","title":"Sampling","text":"Let's define our bounds, this time we are working in two dimensions. In particular, we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol sequences. We then evaluate our function on all the sampling points.\n\nn_samples = 100\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, GoldenSample())\nzs = branin.(xys)\n\nx, y = -5:10, 0:15\np1 = surface(x, y, (x1, x2) -> branin((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> branin((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")","category":"section"},{"location":"kriging/#Building-a-surrogate-2","page":"Kriging","title":"Building a surrogate","text":"Using the sampled points, we build the surrogate, the steps are analogous to the 1-dimensional case.\n\nkriging_surrogate = Kriging(\n    xys, zs, lower_bound, upper_bound, p = [2.0, 2.0], theta = [0.03, 0.003])\n\np1 = surface(x, y, (x, y) -> kriging_surrogate([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Surrogate\")","category":"section"},{"location":"kriging/#Optimizing-2","page":"Kriging","title":"Optimizing","text":"With our surrogate, we can now search for the minima of the branin function.\n\nNotice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.\n\nsize(xys)\n\nsurrogate_optimize!(branin, SRBF(), lower_bound, upper_bound, kriging_surrogate,\n    SobolSample(); maxiters = 100, num_new_samples = 10)\n\nsize(xys)\n\np1 = surface(x, y, (x, y) -> kriging_surrogate([x y]))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nzs = branin.(xys)\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2)","category":"section"},{"location":"sphere_function/#Sphere-function","page":"Sphere function","title":"Sphere function","text":"The sphere function of dimension d is defined as: f(x) = sum_i=1^d x_i^2 with lower bound -10 and upper bound 10.\n\nLet's import Surrogates and Plots:\n\nusing Surrogates\nusing Plots\n\nDefine the objective function:\n\nfunction sphere_function(x)\n    return sum(x .^ 2)\nend\n\nThe 1D case is just a simple parabola, let's plot it:\n\nn = 20\nlb = -10\nub = 10\nx = sample(n, lb, ub, SobolSample())\ny = sphere_function.(x)\nxs = lb:0.001:ub\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lb, ub), ylims = (-2, 120), legend = :top)\nplot!(xs, sphere_function.(xs), label = \"True function\", legend = :top)\n\nFitting RadialSurrogate with different radial basis:\n\nrad_1d_linear = RadialBasis(x, y, lb, ub)\nrad_1d_cubic = RadialBasis(x, y, lb, ub, rad = cubicRadial())\nrad_1d_multiquadric = RadialBasis(x, y, lb, ub, rad = multiquadricRadial())\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lb, ub), ylims = (-2, 120), legend = :top)\nplot!(xs, sphere_function.(xs), label = \"True function\", legend = :top)\nplot!(xs, rad_1d_linear.(xs), label = \"Radial surrogate with linear\", legend = :top)\nplot!(xs, rad_1d_cubic.(xs), label = \"Radial surrogate with cubic\", legend = :top)\nplot!(xs, rad_1d_multiquadric.(xs),\n    label = \"Radial surrogate with multiquadric\", legend = :top)\n\nFitting Lobachevsky Surrogate with different values of hyperparameter alpha:\n\nloba_1 = LobachevskySurrogate(x, y, lb, ub)\nloba_2 = LobachevskySurrogate(x, y, lb, ub, alpha = 1.5, n = 6)\nloba_3 = LobachevskySurrogate(x, y, lb, ub, alpha = 0.3, n = 6)\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lb, ub), ylims = (-2, 120), legend = :top)\nplot!(xs, sphere_function.(xs), label = \"True function\", legend = :top)\nplot!(xs, loba_1.(xs), label = \"Lobachevsky surrogate 1\", legend = :top)\nplot!(xs, loba_2.(xs), label = \"Lobachevsky surrogate 2\", legend = :top)\nplot!(xs, loba_3.(xs), label = \"Lobachevsky surrogate 3\", legend = :top)","category":"section"},{"location":"variablefidelity/#Variable-Fidelity-Surrogate-Tutorial","page":"Variable Fidelity","title":"Variable Fidelity Surrogate Tutorial","text":"With the variable fidelity surrogate, we can specify two different surrogates: one for high-fidelity data and one for low-fidelity data. By default, the first half of the samples are considered high-fidelity and the second half low-fidelity.\n\nusing Surrogates\nusing Plots\n\nn = 100\nlower_bound = 1.0\nupper_bound = 6.0\nx = sample(n, lower_bound, upper_bound, SobolSample())\nf = x -> 1 / 3 * x\ny = f.(x)\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\n\nvarfid = VariableFidelitySurrogate(x, y, lower_bound, upper_bound)\n\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(\n    varfid, label = \"Surrogate function\", xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"cantilever/#Cantilever-Beam-Function","page":"Cantilever beam","title":"Cantilever Beam Function","text":"The Cantilever Beam function is defined as: f(wt) = frac4L^3Ewt*sqrt (fracYt^2)^2 + (fracXw^2)^2  With parameters L,E,X and Y given.\n\nLet's import Surrogates and Plots:\n\nusing Surrogates\nusing PolyChaos\nusing Plots\n\nDefine the objective function:\n\nfunction f(x)\n    t = x[1]\n    w = x[2]\n    L = 100.0\n    E = 2.770674127819261e7\n    X = 530.8038576066307\n    Y = 997.8714938733949\n    return (4 * L^3) / (E * w * t) * sqrt((Y / t^2)^2 + (X / w^2)^2)\nend\n\nLet's plot it:\n\nn = 100\nlb = [1.0, 1.0]\nub = [8.0, 8.0]\nxys = sample(n, lb, ub, SobolSample());\nzs = f.(xys);\nx, y = 0.0:8.0, 0.0:8.0\np1 = surface(x, y, (x1, x2) -> f((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1, x2) -> f((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")\n\nFitting different surrogates:\n\nmypoly = PolynomialChaosSurrogate(xys, zs, lb, ub)\nloba = LobachevskySurrogate(xys, zs, lb, ub)\nrad = RadialBasis(xys, zs, lb, ub)\n\nPlotting:\n\np1 = surface(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Polynomial expansion\")\n\np1 = surface(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Lobachevsky\")\n\np1 = surface(x, y, (x, y) -> rad([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> rad([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Inverse distance\")","category":"section"},{"location":"ImprovedBraninFunction/#Branin-Function","page":"Improved Branin function","title":"Branin Function","text":"The Branin Function is commonly used as a test function for metamodelling in computer experiments, especially in the context of optimization.","category":"section"},{"location":"ImprovedBraninFunction/#Modifications-for-Improved-Branin-Function:","page":"Improved Branin function","title":"Modifications for Improved Branin Function:","text":"To enhance the Branin function, changes were made to introduce irregularities, variability, and a dynamic aspect to its landscape. Here's an example:\n\nfunction improved_branin(x, time_step)\n    x1 = x[1]\n    x2 = x[2]\n    b = 5.1 / (4 * pi^2)\n    c = 5 / pi\n    r = 6\n    a = 1\n    s = 10\n    t = 1 / (8 * pi)\n\n    # Adding noise to the function's output\n    noise = randn() * time_step  # Simulating time-varying noise\n    term1 = a * (x2 - b * x1^2 + c * x1 - r)^2\n    term2 = s * (1 - t) * cos(x1 + noise)  # Introducing dynamic component\n    y = term1 + term2 + s\nend\n\nThis improved function now incorporates irregularities, variability, and a dynamic aspect. These changes aim to make the optimization landscape more challenging and realistic.","category":"section"},{"location":"ImprovedBraninFunction/#Using-the-Improved-Branin-Function:","page":"Improved Branin function","title":"Using the Improved Branin Function:","text":"After defining the improved Branin function, you can proceed to test different surrogates and visualize their performance using the updated function. Here's an example of using the improved function with the Radial Basis surrogate:\n\nusing Surrogates, Plots\n\nn_samples = 80\nlower_bound = [-5, 0]\nupper_bound = [10, 15]\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = [improved_branin(xy, 0.1) for xy in xys]\nradial_surrogate = RadialBasis(xys, zs, lower_bound, upper_bound)\nx, y = -5.00:10.00, 0.00:15.00\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\np1 = surface(x, y, (x, y) -> radial_surrogate([x, y]))\nscatter!(xs, ys, marker_z = zs)\np2 = contour(x, y, (x, y) -> radial_surrogate([x, y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Radial Surrogate\")","category":"section"},{"location":"LinearSurrogate/#Linear-Surrogate","page":"Linear","title":"Linear Surrogate","text":"Linear Surrogate is a linear approach to modeling the relationship between a scalar response or dependent variable and one or more explanatory variables. We will use Linear Surrogate to optimize following function:\n\nf(x) = sin(x) + log(x)\n\nFirst of all we have to import these two packages: Surrogates and Plots.\n\nusing Surrogates\nusing Plots","category":"section"},{"location":"LinearSurrogate/#Sampling","page":"Linear","title":"Sampling","text":"We choose to sample f in 100 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.\n\nf(x) = 2 * x + 10.0\nn_samples = 100\nlower_bound = 5.2\nupper_bound = 12.5\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound))\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound))","category":"section"},{"location":"LinearSurrogate/#Building-a-Surrogate","page":"Linear","title":"Building a Surrogate","text":"With our sampled points, we can build the Linear Surrogate using the LinearSurrogate function.\n\nWe can simply calculate linear_surrogate for any value.\n\nmy_linear_surr_1D = LinearSurrogate(x, y, lower_bound, upper_bound)\nval = my_linear_surr_1D(5.0)\n\nNow, we will simply plot linear_surrogate:\n\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound))\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound))\nplot!(my_linear_surr_1D, label = \"Surrogate function\", xlims = (lower_bound, upper_bound))","category":"section"},{"location":"LinearSurrogate/#Optimizing","page":"Linear","title":"Optimizing","text":"Having built a surrogate, we can now use it to search for minima in our original function f.\n\nTo optimize using our surrogate we call surrogate_optimize! method. We choose to use Stochastic RBF as the optimization technique and again Sobol sampling as the sampling technique.\n\nsurrogate_optimize!(\n    f, SRBF(), lower_bound, upper_bound, my_linear_surr_1D, SobolSample())\nscatter(x, y, label = \"Sampled points\")\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound))\nplot!(my_linear_surr_1D, label = \"Surrogate function\", xlims = (lower_bound, upper_bound))","category":"section"},{"location":"LinearSurrogate/#Linear-Surrogate-tutorial-(ND)","page":"Linear","title":"Linear Surrogate tutorial (ND)","text":"First of all we will define the Egg Holder function we are going to build a surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.\n\nusing Plots\ndefault(c = :matter, legend = false, xlabel = \"x\", ylabel = \"y\")\nusing Surrogates\n\nfunction egg(x)\n    x1 = x[1]\n    x2 = x[2]\n    term1 = -(x2 + 47) * sin(sqrt(abs(x2 + x1 / 2 + 47)))\n    term2 = -x1 * sin(sqrt(abs(x1 - (x2 + 47))))\n    y = term1 + term2\nend","category":"section"},{"location":"LinearSurrogate/#Sampling-2","page":"Linear","title":"Sampling","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -10, 5, and 0, 15 for the second dimension. We are taking 100 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.\n\nn_samples = 100\nlower_bound = [-10.0, 0.0]\nupper_bound = [5.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = egg.(xys)\n\nx, y = -10:5, 0:15\np1 = surface(x, y, (x1, x2) -> egg((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> egg((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")","category":"section"},{"location":"LinearSurrogate/#Building-a-surrogate","page":"Linear","title":"Building a surrogate","text":"Using the sampled points, we build the surrogate, the steps are analogous to the 1-dimensional case.\n\nmy_linear_ND = LinearSurrogate(xys, zs, lower_bound, upper_bound)\n\np1 = surface(x, y, (x, y) -> my_linear_ND([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> my_linear_ND([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Surrogate\")","category":"section"},{"location":"LinearSurrogate/#Optimizing-2","page":"Linear","title":"Optimizing","text":"With our surrogate, we can now search for the minima of the function.\n\nNotice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.\n\nsize(xys)\n\nsurrogate_optimize!(\n    egg, SRBF(), lower_bound, upper_bound, my_linear_ND, SobolSample(), maxiters = 10)\n\nsize(xys)\n\np1 = surface(x, y, (x, y) -> my_linear_ND([x y]))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nzs = egg.(xys)\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> my_linear_ND([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2)","category":"section"},{"location":"optimizations/#Optimization-techniques","page":"Optimization","title":"Optimization techniques","text":"SRBF\n\nLCBS\n\nEI\n\nDYCORS\n\nSOP","category":"section"},{"location":"optimizations/#Adding-another-optimization-method","page":"Optimization","title":"Adding another optimization method","text":"To add another optimization method, you just need to define a new SurrogateOptimizationAlgorithm and write its corresponding algorithm, overloading the following:\n\nsurrogate_optimize!(obj::Function,::NewOptimizationType,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"section"},{"location":"optimizations/#Surrogates.surrogate_optimize!-Tuple{Function, SRBF, Any, Any, Union{SurrogatesBase.AbstractDeterministicSurrogate, SurrogatesBase.AbstractStochasticSurrogate}, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize!","text":"The main idea is to pick the new evaluations from a set of candidate points, where each candidate point is generated as an N(0, sigma^2) distributed perturbation from the current best solution. The value of sigma is modified based on progress and follows the same logic as in many trust region methods: we increase sigma if we make a lot of progress (the surrogate is accurate) and decrease sigma when we aren’t able to make progress (the surrogate model is inaccurate). More details about how sigma is updated are given in the original papers.\n\nAfter generating the candidate points, we predict their objective function value and compute the minimum distance to the previously evaluated point. Let the candidate points be denoted by C and let the function value predictions be s(x_i) and the distance values be d(x_i), both rescaled through a linear transformation to the interval [0,1]. This is done to put the values on the same scale. The next point selected for evaluation is the candidate point x that minimizes the weighted-distance merit function:\n\nmerit(x) = ws(x) + (1-w)(1-d(x))\n\nwhere 0 leq w leq 1. That is, we want a small function value prediction and a large minimum distance from the previously evaluated points. The weight w is commonly cycled between a few values to achieve both exploitation and exploration. When w is close to zero, we do pure exploration, while w close to 1 corresponds to exploitation.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#Surrogates.surrogate_optimize!-Tuple{Function, LCBS, Any, Any, Any, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize!","text":"This is an implementation of Lower Confidence Bound (LCB), a popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to minimize:\n\nLCB(x) = Ex - k * sqrt(Vx)\n\ndefault value k = 2.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#Surrogates.surrogate_optimize!-Tuple{Function, EI, Any, Any, Any, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize!","text":"This is an implementation of Expected Improvement (EI), arguably the most popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to maximize expected improvement:\n\nEI(x) = Emax(f_best-f(x)0)\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#Surrogates.surrogate_optimize!-Tuple{Function, DYCORS, Any, Any, Union{SurrogatesBase.AbstractDeterministicSurrogate, SurrogatesBase.AbstractStochasticSurrogate}, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize!","text":"  surrogate_optimize!(obj::Function,::DYCORS,lb::Number,ub::Number,surr1::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)\n\nThis is an implementation of the DYCORS strategy by Regis and Shoemaker: Rommel G Regis and Christine A Shoemaker. Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization. Engineering Optimization, 45(5): 529–555, 2013. This is an extension of the SRBF strategy that changes how the candidate points are generated. The main idea is that many objective functions depend only on a few directions, so it may be advantageous to perturb only a few directions. In particular, we use a perturbation probability to perturb a given coordinate and decrease this probability after each function evaluation, so fewer coordinates are perturbed later in the optimization.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#Surrogates.surrogate_optimize!-Tuple{Function, SOP, Number, Number, Union{SurrogatesBase.AbstractDeterministicSurrogate, SurrogatesBase.AbstractStochasticSurrogate}, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize!","text":"surrogateoptimize!(obj::Function,::SOP,lb::Number,ub::Number,surr::AbstractSurrogate,sampletype::SamplingAlgorithm;maxiters=100,numnewsamples=100)\n\nSOP Surrogate optimization method, following closely the following papers:\n\n- SOP: parallel surrogate global optimization with Pareto center selection for computationally expensive single objective problems by Tipaluck Krityakierne\n- Multiobjective Optimization Using Evolutionary Algorithms by Kalyan Deb\n\n#Suggested number of new_samples = min(500*d,5000)\n\n\n\n\n\n","category":"method"},{"location":"rosenbrock/#Rosenbrock-function","page":"Rosenbrock","title":"Rosenbrock function","text":"The Rosenbrock function is defined as: f(x) = sum_i=1^d-1 (x_i+1-x_i)^2 + (x_i - 1)^2\n\nI will treat the 2D version, which is commonly defined as: f(xy) = (1-x)^2 + 100(y-x^2)^2 Let's import Surrogates and Plots:\n\nusing Surrogates\nusing PolyChaos\nusing Plots\n\nDefine the objective function:\n\nfunction f(x)\n    x1 = x[1]\n    x2 = x[2]\n    return (1 - x1)^2 + 100 * (x2 - x1^2)^2\nend\n\nLet's plot it:\n\nn = 100\nlb = [0.0, 0.0]\nub = [1.0, 1.0]\nxys = sample(n, lb, ub, SobolSample())\nzs = f.(xys);\nx, y = 0:1, 0:1\np1 = surface(x, y, (x1, x2) -> f((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> f((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")\n\nFitting different surrogates:\n\nmypoly = PolynomialChaosSurrogate(xys, zs, lb, ub)\nloba = LobachevskySurrogate(xys, zs, lb, ub)\ninver = InverseDistanceSurrogate(xys, zs, lb, ub)\n\nPlotting:\n\np1 = surface(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Polynomial expansion\")\n\np1 = surface(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Lobachevsky\")\n\np1 = surface(x, y, (x, y) -> inver([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> inver([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Inverse distance\")","category":"section"},{"location":"secondorderpoly/#Second-Order-Polynomial-Surrogate-Tutorial","page":"SecondOrderPolynomial","title":"Second Order Polynomial Surrogate Tutorial","text":"The square polynomial model can be expressed by:\n\ny = Xβ + ϵ\n\nWhere X is the matrix of the linear model augmented by adding 2d columns, containing pair by pair products of variables and variables squared.\n\nusing Surrogates\nusing Plots","category":"section"},{"location":"secondorderpoly/#Sampling","page":"SecondOrderPolynomial","title":"Sampling","text":"f = x -> 3 * sin(x) + 10 / x\nlb = 3.0\nub = 6.0\nn = 100\nx = sample(n, lb, ub, HaltonSample())\ny = f.(x)\nscatter(x, y, label = \"Sampled points\", xlims = (lb, ub))\nplot!(f, label = \"True function\", xlims = (lb, ub))","category":"section"},{"location":"secondorderpoly/#Building-the-surrogate","page":"SecondOrderPolynomial","title":"Building the surrogate","text":"sec = SecondOrderPolynomialSurrogate(x, y, lb, ub)\nplot(x, y, seriestype = :scatter, label = \"Sampled points\", xlims = (lb, ub))\nplot!(f, label = \"True function\", xlims = (lb, ub))\nplot!(sec, label = \"Surrogate function\", xlims = (lb, ub))","category":"section"},{"location":"secondorderpoly/#Optimizing","page":"SecondOrderPolynomial","title":"Optimizing","text":"surrogate_optimize!(f, SRBF(), lb, ub, sec, SobolSample())\nscatter(x, y, label = \"Sampled points\")\nplot!(f, label = \"True function\", xlims = (lb, ub))\nplot!(sec, label = \"Surrogate function\", xlims = (lb, ub))\n\nThe optimization method successfully found the minimum.","category":"section"},{"location":"gekpls/#GEKPLS-Surrogate-Tutorial","page":"GEKPLS","title":"GEKPLS Surrogate Tutorial","text":"Gradient Enhanced Kriging with Partial Least Squares Method (GEKPLS) is a surrogate modeling technique that brings down computation time and returns improved accuracy for high-dimensional problems. The Julia implementation of GEKPLS is adapted from the Python version by SMT which is based on this paper.\n\nThe following are the inputs when building a GEKPLS surrogate:\n\nx - The vector containing the training points\ny - The vector containing the training outputs associated with each of the training points\ngrads - The gradients at each of the input X training points\nn_comp - Number of components to retain for the partial least squares regression (PLS)\ndelta_x -  The step size to use for the first order Taylor approximation\nlb - The lower bound for the training points\nub - The upper bound for the training points\nextra_points - The number of additional points to use for the PLS\ntheta - The hyperparameter to use for the correlation model","category":"section"},{"location":"gekpls/#Basic-GEKPLS-Usage","page":"GEKPLS","title":"Basic GEKPLS Usage","text":"The following example illustrates how to use GEKPLS:\n\nusing Surrogates\nusing Zygote\n\nfunction water_flow(x)\n    r_w = x[1]\n    r = x[2]\n    T_u = x[3]\n    H_u = x[4]\n    T_l = x[5]\n    H_l = x[6]\n    L = x[7]\n    K_w = x[8]\n    log_val = log(r / r_w)\n    return (2 * pi * T_u * (H_u - H_l)) /\n           (log_val * (1 + (2 * L * T_u / (log_val * r_w^2 * K_w)) + T_u / T_l))\nend\n\nn = 1000\nlb = [0.05, 100, 63070, 990, 63.1, 700, 1120, 9855]\nub = [0.15, 50000, 115600, 1110, 116, 820, 1680, 12045]\nx = sample(n, lb, ub, SobolSample())\ngrads = gradient.(water_flow, x)\ny = water_flow.(x)\nn_test = 100\nx_test = sample(n_test, lb, ub, GoldenSample())\ny_true = water_flow.(x_test)\nn_comp = 2\ndelta_x = 0.0001\nextra_points = 2\ninitial_theta = [0.01 for i in 1:n_comp]\ng = GEKPLS(x, y, grads, n_comp, delta_x, lb, ub, extra_points, initial_theta)\ny_pred = g.(x_test)\nrmse = sqrt(sum(((y_pred - y_true) .^ 2) / n_test))","category":"section"},{"location":"gekpls/#Using-GEKPLS-With-Surrogate-Optimization","page":"GEKPLS","title":"Using GEKPLS With Surrogate Optimization","text":"GEKPLS can also be used to find the minimum of a function with the optimization function. This next example demonstrates how this can be accomplished.\n\nusing Surrogates\nusing Zygote\n\nfunction sphere_function(x)\n    return sum(x .^ 2)\nend\n\nlb = [-5.0, -5.0, -5.0]\nub = [5.0, 5.0, 5.0]\nn_comp = 2\ndelta_x = 0.0001\nextra_points = 2\ninitial_theta = [0.01 for i in 1:n_comp]\nn = 100\nx = sample(n, lb, ub, SobolSample())\ngrads = gradient.(sphere_function, x)\ny = sphere_function.(x)\ng = GEKPLS(x, y, grads, n_comp, delta_x, lb, ub, extra_points, initial_theta)\nx_point,\nminima = surrogate_optimize!(sphere_function, SRBF(), lb, ub, g,\n    RandomSample(); maxiters = 20,\n    num_new_samples = 20, needs_gradient = true)\nminima","category":"section"},{"location":"Salustowicz/#Salustowicz-Benchmark-Function","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark Function","text":"The true underlying function HyGP had to approximate is the 1D Salustowicz function. The function can be evaluated in the given domain: x in 0 10.\n\nThe Salustowicz benchmark function is as follows:\n\nf(x) = e^-x x^3 cos(x) sin(x) (cos(x) sin^2(x) - 1)\n\nLet's import these two packages  Surrogates and Plots:\n\nusing Surrogates\nusing Plots\n\nNow, let's define our objective function:\n\nfunction salustowicz(x)\n    term1 = 2.72^(-x) * x^3 * cos(x) * sin(x)\n    term2 = (cos(x) * sin(x) * sin(x) - 1)\n    y = term1 * term2\nend\n\nLet's sample f in 30 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol Sample, this can be done by passing SobolSample() to the sample function.\n\nn_samples = 30\nlower_bound = 0\nupper_bound = 10\nnum_round = 2\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = salustowicz.(x)\nxs = lower_bound:0.001:upper_bound\nscatter(x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(xs, salustowicz.(xs), label = \"True function\", legend = :top)\n\nNow, let's fit the Salustowicz function with different surrogates:\n\nInverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\nlobachevsky_surrogate = LobachevskySurrogate(\n    x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\nscatter(\n    x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound), legend = :topright)\nplot!(xs, salustowicz.(xs), label = \"True function\", legend = :topright)\nplot!(xs, InverseDistance.(xs), label = \"InverseDistanceSurrogate\", legend = :topright)\nplot!(xs, lobachevsky_surrogate.(xs), label = \"Lobachevsky\", legend = :topright)\n\nNot's let's see Kriging Surrogate with different hyper parameter:\n\nkriging_surrogate1 = Kriging(x, y, lower_bound, upper_bound, p = 0.9);\nkriging_surrogate2 = Kriging(x, y, lower_bound, upper_bound, p = 1.5);\nkriging_surrogate3 = Kriging(x, y, lower_bound, upper_bound, p = 1.9);\nscatter(\n    x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound), legend = :topright)\nplot!(xs, salustowicz.(xs), label = \"True function\", legend = :topright)\nplot!(xs, kriging_surrogate1.(xs), label = \"kriging_surrogate1\",\n    ribbon = p -> std_error_at_point(kriging_surrogate1, p), legend = :topright)\nplot!(xs, kriging_surrogate2.(xs), label = \"kriging_surrogate2\",\n    ribbon = p -> std_error_at_point(kriging_surrogate2, p), legend = :topright)\nplot!(xs, kriging_surrogate3.(xs), label = \"kriging_surrogate3\",\n    ribbon = p -> std_error_at_point(kriging_surrogate3, p), legend = :topright)","category":"section"},{"location":"abstractgps/#Gaussian-Process-Surrogate-Tutorial","page":"Gaussian Process","title":"Gaussian Process Surrogate Tutorial","text":"Gaussian Process regression in Surrogates.jl is implemented as a simple wrapper around the AbstractGPs.jl package. AbstractGPs comes with a variety of covariance functions (kernels). See KernelFunctions.jl for examples.\n\ntip: Tip\nThe examples below demonstrate the use of AbstractGPs with out-of-the-box settings without hyperparameter optimization (i.e. without changing parameters like lengthscale, signal variance, and noise variance). Beyond hyperparameter optimization, careful initialization of hyperparameters and priors on the parameters is required for this surrogate to work properly. For more details on how to fit GPs in practice, check out A Practical Guide to Gaussian Processes.Also see this example to understand hyperparameter optimization with AbstractGPs.","category":"section"},{"location":"abstractgps/#1D-Example","page":"Gaussian Process","title":"1D Example","text":"In the example below, the 'gp_surrogate' assignment code can be commented / uncommented to see how the different kernels influence the predictions.\n\nusing Surrogates\nusing Plots\nusing AbstractGPs\n\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\nn_samples = 100\nlower_bound = 0.0\nupper_bound = 1.0\nxs = lower_bound:0.001:upper_bound\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n\ngp_surrogate = AbstractGPSurrogate(\n    x, y, gp = GP(PolynomialKernel(; c = 2.0, degree = 15)), Σy = 0.25)\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), ylims = (-7, 17), legend = :top)\nplot!(xs, f.(xs), label = \"True function\", legend = :top)\nplot!(0:0.001:1, gp_surrogate.gp_posterior; label = \"Posterior\", ribbon_scale = 2)","category":"section"},{"location":"abstractgps/#Optimization-Example","page":"Gaussian Process","title":"Optimization Example","text":"This example shows the use of AbstractGP Surrogates to find the minima of a function:\n\nusing Surrogates\nusing Plots\nusing AbstractGPs\n\nf(x) = (x - 2)^2\nn_samples = 100\nlower_bound = 0.0\nupper_bound = 4.0\nxs = lower_bound:0.1:upper_bound\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\ngp_surrogate = AbstractGPSurrogate(x, y)\nsurrogate_optimize!(f, SRBF(), lower_bound, upper_bound, gp_surrogate, SobolSample())\n\nPlotting the function and the sampled points:\n\nscatter(gp_surrogate.x, gp_surrogate.y, label = \"Sampled points\",\n    ylims = (-1.0, 5.0), legend = :top)\nplot!(xs, gp_surrogate.(xs), label = \"Surrogate function\",\n    ribbon = p -> Surrogates.std_error_at_point(gp_surrogate, p), legend = :top)\nplot!(xs, f.(xs), label = \"True function\", legend = :top)","category":"section"},{"location":"abstractgps/#ND-Example","page":"Gaussian Process","title":"ND Example","text":"using Plots\ndefault(c = :matter, legend = false, xlabel = \"x\", ylabel = \"y\")\nusing Surrogates\nusing AbstractGPs\n\nhypot_func = z -> 3 * hypot(z...) + 1\nn_samples = 100\nlower_bound = [-1.0, -1.0]\nupper_bound = [1.0, 1.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = hypot_func.(xys)\n\nx, y = -2:2, -2:2\np1 = surface(x, y, (x1, x2) -> hypot_func((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> hypot_func((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")\n\nNow let's see how our surrogate performs:\n\ngp_surrogate = AbstractGPSurrogate(xys, zs)\np1 = surface(x, y, (x, y) -> gp_surrogate([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> gp_surrogate([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Surrogate\")\n\ngp_surrogate((0.2, 0.2))\n\nhypot_func((0.2, 0.2))\n\nAnd this is our log marginal posterior predictive probability:\n\nlogpdf_surrogate(gp_surrogate)","category":"section"},{"location":"surrogate/#Surrogate","page":"Surrogates","title":"Surrogate","text":"Every surrogate has a different definition depending on the parameters needed. It uses the interface defined in SurrogatesBase.jl. In a nutshell, they use:\n\nupdate!(::AbstractDeterministicSurrogate, x_new, y_new)\nAbstractDeterministicSurrogate(value)\n\nThe first function adds a sample point to the surrogate, thus changing the internal coefficients. The second one calculates the approximation at value.\n\nLinear surrogate\n\nRadial basis function surrogate\n\nKriging surrogate\n\nLobachevsky surrogate\n\nSupport vector machine surrogate, requires using LIBSVM.\n\nSVMSurrogate(x,y,lb::Number,ub::Number)\n\nRandom forest surrogate, requires using XGBoost.\n\nXGBoostSurrogate(x,y,lb,ub;num_round::Int = 1)\n\nNeural network surrogate, requires using Flux.\n\nNeuralSurrogate(x,y,lb,ub; model = Chain(Dense(length(x[1]),1), first), loss = (x,y) -> Flux.mse(model(x), y),opt = Descent(0.01),n_echos::Int = 1)","category":"section"},{"location":"surrogate/#Creating-another-surrogate","page":"Surrogates","title":"Creating another surrogate","text":"It's great that you want to add another surrogate to the library! You will need to:\n\nDefine a new mutable struct and a constructor function\nDefine update!(your_surrogate, x_new, y_new)\nDefine your_surrogate(value) for the approximation","category":"section"},{"location":"surrogate/#Example","page":"Surrogates","title":"Example","text":"mutable struct NewSurrogate{X, Y, L, U, C, A, B} <: AbstractDeterministicSurrogate\n    x::X\n    y::Y\n    lb::L\n    ub::U\n    coeff::C\n    alpha::A\n    beta::B\nend\n\nfunction NewSurrogate(x, y, lb, ub, parameters)\n    ...\n    return NewSurrogate(x, y, lb, ub, calculated \\ _coeff, alpha, beta)\nend\n\nfunction update!(NewSurrogate, x_new, y_new)\n    ...\nend\n\nfunction (s::NewSurrogate)(value)\n    return s.coeff * value + s.alpha\nend","category":"section"},{"location":"surrogate/#Surrogates.LinearSurrogate-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.LinearSurrogate","text":"LinearSurrogate(x,y,lb,ub)\n\nBuilds a linear surrogate using GLM.jl\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Surrogates.RadialBasis-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.RadialBasis","text":"RadialBasis(x,y,lb,ub,rad::RadialFunction, scale_factor::Float = 1.0, regularization::Real = 0.0)\n\nConstructor for RadialBasis surrogate, of the form\n\nf(x) = sum_i=1^N w_i phi(x - boldc_i) boldv^T + boldv^mathrmT  0 boldx \n\nwhere w_i are the weights of polyharmonic splines phi(x) and boldv are coefficients of a polynomial term.\n\nregularization is a regularization term added to the diagonal of the interpolation matrix to avoid SingularException.\n\nReferences: https://en.wikipedia.org/wiki/Polyharmonic_spline\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Surrogates.Kriging-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.Kriging","text":"Kriging(x,y,lb,ub;p=collect(one.(x[1])),theta=collect(one.(x[1])))\n\nConstructor for Kriging surrogate.\n\n(x,y): sampled points\np: array of values 0<=p<2 modeling the smoothness of the function being approximated in the i-th variable. low p -> rough, high p -> smooth\ntheta: array of values > 0 modeling how much the function is changing in the i-th variable.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Surrogates.LobachevskySurrogate-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.LobachevskySurrogate","text":"LobachevskySurrogate(x,y,alpha,n::Int,lb,ub,sparse = false)\n\nBuild the Lobachevsky surrogate with parameters alpha and n.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Surrogates.lobachevsky_integral-Tuple{LobachevskySurrogate, Any, Any}","page":"Surrogates","title":"Surrogates.lobachevsky_integral","text":"lobachevsky_integral(loba::LobachevskySurrogate,lb,ub)\n\nCalculates the integral of the Lobachevsky surrogate, which has a closed form.\n\n\n\n\n\n","category":"method"},{"location":"gramacylee/#Gramacy-and-Lee-Function","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"the Gramacy & Lee function is a continuous function. It is not convex. The function is defined on a 1-dimensional space. It is unimodal. The function can be defined on any input domain, but it is usually evaluated on x in -05 25.\n\nThe Gramacy & Lee is as follows: f(x) = fracsin(10pi x)2x + (x-1)^4.\n\nLet's import these two packages Surrogates and Plots:\n\nusing Surrogates\nusing PolyChaos\nusing Plots\n\nNow, let's define our objective function:\n\nfunction gramacylee(x)\n    term1 = sin(10 * pi * x) / (2 * x)\n    term2 = (x - 1)^4\n    y = term1 + term2\nend\n\nLet's sample f in 25 points between -0.5 and 2.5 using the sample function. The sampling points are chosen using a Sobol sample, this can be done by passing SobolSample() to the sample function.\n\nn = 25\nlower_bound = -0.5\nupper_bound = 2.5\nx = sample(n, lower_bound, upper_bound, SobolSample())\ny = gramacylee.(x)\nxs = lower_bound:0.001:upper_bound\nscatter(x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound),\n    ylims = (-5, 20), legend = :top)\nplot!(xs, gramacylee.(xs), label = \"True function\", legend = :top)\n\nNow, let's fit Gramacy & Lee function with different surrogates:\n\nmy_pol = PolynomialChaosSurrogate(x, y, lower_bound, upper_bound)\nloba_1 = LobachevskySurrogate(x, y, lower_bound, upper_bound)\nkrig = Kriging(x, y, lower_bound, upper_bound)\nscatter(x, y, label = \"Sampled points\", xlims = (lower_bound, upper_bound),\n    ylims = (-5, 20), legend = :top)\nplot!(xs, gramacylee.(xs), label = \"True function\", legend = :top)\nplot!(xs, my_pol.(xs), label = \"Polynomial expansion\", legend = :top)\nplot!(xs, loba_1.(xs), label = \"Lobachevsky\", legend = :top)\nplot!(xs, krig.(xs), label = \"Kriging\", legend = :top)","category":"section"},{"location":"water_flow/#Water-flow-function","page":"Water Flow function","title":"Water flow function","text":"The water flow function is defined as: f(r_wrT_uH_uT_lH_lLK_w) = frac2*pi*T_u(H_u - H_l)log(fracrr_w)*1 + frac2LT_ulog(fracrr_w)*r_w^2*K_w+ fracT_uT_l \n\nIt has 8 dimensions.\n\nusing Surrogates\nusing PolyChaos\nusing Plots\nusing LinearAlgebra\n\nDefine the objective function:\n\nfunction f(x)\n    r_w = x[1]\n    r = x[2]\n    T_u = x[3]\n    H_u = x[4]\n    T_l = x[5]\n    H_l = x[6]\n    L = x[7]\n    K_w = x[8]\n    log_val = log(r / r_w)\n    return (2 * pi * T_u * (H_u - H_l)) /\n           (log_val * (1 + (2 * L * T_u / (log_val * r_w^2 * K_w)) + T_u / T_l))\nend\n\nn = 180\nd = 8\nlb = [0.05, 100, 63070, 990, 63.1, 700, 1120, 9855]\nub = [0.15, 50000, 115600, 1110, 116, 820, 1680, 12045]\nx = sample(n, lb, ub, SobolSample())\ny = f.(x)\nn_test = 1000\nx_test = sample(n_test, lb, ub, GoldenSample())\ny_true = f.(x_test)\n\nmy_rad = RadialBasis(x, y, lb, ub)\ny_rad = my_rad.(x_test)\nmy_poly = PolynomialChaosSurrogate(x, y, lb, ub)\ny_poly = my_poly.(x_test)\nmse_rad = norm(y_true - y_rad, 2) / n_test\nmse_poly = norm(y_true - y_poly, 2) / n_test\nprintln(\"MSE Radial: $mse_rad\")\nprintln(\"MSE Radial: $mse_poly\")","category":"section"},{"location":"radials/#Radial-Basis-Surrogates-Tutorial-(1D)","page":"Radials","title":"Radial Basis Surrogates Tutorial (1D)","text":"The Radial Basis Surrogate model represents the interpolating function as a linear combination of basis functions, one for each training point. Let's say we are building a surrogate for:\n\nf(x) = log(x) cdot x^2+x^3\n\nLet's choose the Radial Basis Surrogate for 1D. First of all we have to import these two packages: Surrogates and Plots.\n\nusing Surrogates\nusing Plots\n\nWe choose to sample f in 100 points between 5 to 25 using sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.\n\nf(x) = log(x) * x^2 + x^3\nn_samples = 100\nlower_bound = 5.0\nupper_bound = 25.0\nx = sort(sample(n_samples, lower_bound, upper_bound, SobolSample()))\ny = f.(x)\nscatter(x, y, label = \"Sampled Points\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(x, y, label = \"True function\", legend = :top)","category":"section"},{"location":"radials/#Building-Surrogate","page":"Radials","title":"Building Surrogate","text":"With our sampled points we can build the Radial Surrogate using the RadialBasis function.\n\nWe can simply calculate radial_surrogate for any value.\n\nradial_surrogate = RadialBasis(x, y, lower_bound, upper_bound)\nval = radial_surrogate(5.4)\n\nWe can also use cubic radial basis functions.\n\nradial_surrogate = RadialBasis(x, y, lower_bound, upper_bound, rad = cubicRadial())\nval = radial_surrogate(5.4)\n\nCurrently, available radial basis functions are linearRadial (the default), cubicRadial, multiquadricRadial, and thinplateRadial.\n\nNow, we will simply plot radial_surrogate:\n\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(radial_surrogate, label = \"Surrogate function\",\n    xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"radials/#Optimizing","page":"Radials","title":"Optimizing","text":"Having built a surrogate, we can now use it to search for minima in our original function f.\n\nTo optimize using our surrogate, we call surrogate_optimize! method. We choose to use Stochastic RBF as the optimization technique and again Sobol sampling as the sampling technique.\n\nsurrogate_optimize!(\n    f, SRBF(), lower_bound, upper_bound, radial_surrogate, SobolSample())\nscatter(x, y, label = \"Sampled points\", legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(radial_surrogate, label = \"Surrogate function\",\n    xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"radials/#Radial-Basis-Surrogate-Tutorial-(ND)","page":"Radials","title":"Radial Basis Surrogate Tutorial (ND)","text":"First of all, we will define the Booth function we are going to build the surrogate for:\n\nf(x) = (x_1 + 2*x_2 - 7)^2 + (2*x_1 + x_2 - 5)^2\n\nNotice, how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.\n\nusing Plots\ndefault(c = :matter, legend = false, xlabel = \"x\", ylabel = \"y\")\nusing Surrogates\n\nfunction booth(x)\n    x1 = x[1]\n    x2 = x[2]\n    term1 = (x1 + 2 * x2 - 7)^2\n    term2 = (2 * x1 + x2 - 5)^2\n    y = term1 + term2\nend","category":"section"},{"location":"radials/#Sampling","page":"Radials","title":"Sampling","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 100 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.\n\nn_samples = 100\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = booth.(xys)\n\nx, y = -5.0:10.0, 0.0:15.0\np1 = surface(x, y, (x1, x2) -> booth((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> booth((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")","category":"section"},{"location":"radials/#Building-a-surrogate","page":"Radials","title":"Building a surrogate","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.\n\nradial_basis = RadialBasis(xys, zs, lower_bound, upper_bound)\n\np1 = surface(x, y, (x, y) -> radial_basis([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> radial_basis([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Surrogate\")","category":"section"},{"location":"radials/#Optimizing-2","page":"Radials","title":"Optimizing","text":"With our surrogate, we can now search for the minima of the function.\n\nNotice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.\n\nsize(xys)\n\nsurrogate_optimize!(\n    booth, SRBF(), lower_bound, upper_bound, radial_basis, RandomSample(), maxiters = 50)\n\nsize(xys)\n\np1 = surface(x, y, (x, y) -> radial_basis([x y]))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nzs = booth.(xys)\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> radial_basis([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2)","category":"section"},{"location":"multi_objective_opt/#Multi-objective-optimization","page":"Multi objective optimization","title":"Multi-objective optimization","text":"","category":"section"},{"location":"multi_objective_opt/#Case-1:-Non-colliding-objective-functions","page":"Multi objective optimization","title":"Case 1: Non-colliding objective functions","text":"using Surrogates\n\nm = 10\nf = x -> [x^i for i in 1:m]\nlb = 1.0\nub = 10.0\nx = sample(50, lb, ub, GoldenSample())\ny = f.(x)\nmy_radial_basis_ego = RadialBasis(x, y, lb, ub)\npareto_set,\npareto_front = surrogate_optimize!(\n    f, SMB(), lb, ub, my_radial_basis_ego, SobolSample(); maxiters = 10, n_new_look = 100)\n\nm = 5\nf = x -> [x^i for i in 1:m]\nlb = 1.0\nub = 10.0\nx = sample(50, lb, ub, SobolSample())\ny = f.(x)\nmy_radial_basis_rtea = RadialBasis(x, y, lb, ub)\nZ = 0.8\nK = 2\np_cross = 0.5\nn_c = 1.0\nsigma = 1.5\nsurrogate_optimize!(\n    f, RTEA(Z, K, p_cross, n_c, sigma), lb, ub, my_radial_basis_rtea, SobolSample())","category":"section"},{"location":"multi_objective_opt/#Case-2:-objective-functions-with-conflicting-minima","page":"Multi objective optimization","title":"Case 2: objective functions with conflicting minima","text":"\nf = x -> [sqrt((x[1] - 4)^2 + 25 * (x[2])^2),\n    sqrt((x[1] + 4)^2 + 25 * (x[2])^2),\n    sqrt((x[1] - 3)^2 + (x[2] - 1)^2)]\nlb = [2.5, -0.5]\nub = [3.5, 0.5]\nx = sample(50, lb, ub, SobolSample())\ny = f.(x)\nmy_radial_basis_ego = RadialBasis(x, y, lb, ub)\n#I can find my pareto set and pareto front by calling again the surrogate_optimize! function:\npareto_set,\npareto_front = surrogate_optimize!(\n    f, SMB(), lb, ub, my_radial_basis_ego, SobolSample(); maxiters = 10, n_new_look = 100);","category":"section"},{"location":"BraninFunction/#Branin-Function","page":"Branin function","title":"Branin Function","text":"The Branin function is commonly used as a test function for metamodelling in computer experiments, especially in the context of optimization.\n\nThe expression of the Branin Function is given as: f(x) = (x_2 - frac514pi^2x_1^2 + frac5pix_1 - 6)^2 + 10(1-frac18pi)cos(x_1) + 10\n\nwhere x = (x_1 x_2) with -5leq x_1 leq 10 0 leq x_2 leq 15\n\nFirst of all, we will import these two packages: Surrogates and Plots.\n\nusing Surrogates\nusing Plots\n\nNow, let's define our objective function:\n\nfunction branin(x)\n    x1 = x[1]\n    x2 = x[2]\n    b = 5.1 / (4 * pi^2)\n    c = 5 / pi\n    r = 6\n    a = 1\n    s = 10\n    t = 1 / (8 * pi)\n    term1 = a * (x2 - b * x1^2 + c * x1 - r)^2\n    term2 = s * (1 - t) * cos(x1)\n    y = term1 + term2 + s\nend\n\nNow, let's plot it:\n\nn_samples = 80\nlower_bound = [-5, 0]\nupper_bound = [10, 15]\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = branin.(xys)\nx, y = -5.00:10.00, 0.00:15.00\np1 = surface(x, y, (x1, x2) -> branin((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> branin((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")\n\nNow it's time to try fitting different surrogates, and then we will plot them. We will have a look at the radial basis surrogate Radial Basis Surrogate. :\n\nradial_surrogate = RadialBasis(xys, zs, lower_bound, upper_bound)\n\np1 = surface(x, y, (x, y) -> radial_surrogate([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> radial_surrogate([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Radial Surrogate\")\n\nNow, we will have a look at Inverse Distance Surrogate:\n\nInverseDistance = InverseDistanceSurrogate(xys, zs, lower_bound, upper_bound)\n\np1 = surface(x, y, (x, y) -> InverseDistance([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> InverseDistance([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Inverse Distance Surrogate\")\n\nNow, let's talk about Lobachevsky Surrogate:\n\nLobachevsky = LobachevskySurrogate(\n    xys, zs, lower_bound, upper_bound, alpha = [2.8, 2.8], n = 8)\n\np1 = surface(x, y, (x, y) -> Lobachevsky([x y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> Lobachevsky([x y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Lobachevsky Surrogate\")","category":"section"},{"location":"gek/#Gradient-Enhanced-Kriging-Surrogate-Tutorial","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging Surrogate Tutorial","text":"Gradient-enhanced Kriging is an extension of kriging which supports gradient information. GEK is usually more accurate than kriging. However, it is not computationally efficient when the number of inputs, the number of sampling points, or both, are high. This is mainly due to the size of the corresponding correlation matrix, which increases proportionally with both the number of inputs and the number of sampling points.\n\nLet's have a look at the following function to use Gradient Enhanced Surrogate: f(x) = x^3 - 6x^2 + 4x + 12\n\nFirst of all, we will import Surrogates and Plots packages:\n\nusing Surrogates\nusing Plots","category":"section"},{"location":"gek/#Sampling","page":"Gradient Enhanced Kriging","title":"Sampling","text":"We choose to sample f in 100 points between 2 and 10 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.\n\nn_samples = 100\nlower_bound = 2\nupper_bound = 10\nxs = lower_bound:0.001:upper_bound\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\nf(x) = x^3 - 6x^2 + 4x + 12\ny1 = f.(x)\nder = x -> 3 * x^2 - 12 * x + 4\ny2 = der.(x)\ny = vcat(y1, y2)\nscatter(x, y1, label = \"Sampled points\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"gek/#Building-a-surrogate","page":"Gradient Enhanced Kriging","title":"Building a surrogate","text":"With our sampled points, we can build the Gradient Enhanced Kriging surrogate using the GEK function.\n\nmy_gek = GEK(x, y, lower_bound, upper_bound, p = 0.03, theta = 0.3)\n\nscatter(x, y1, label = \"Sampled points\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(f, label = \"True function\", xlims = (lower_bound, upper_bound), legend = :top)\nplot!(my_gek, label = \"Surrogate function\", ribbon = p -> std_error_at_point(my_gek, p),\n    xlims = (lower_bound, upper_bound), legend = :top)","category":"section"},{"location":"gek/#Gradient-Enhanced-Kriging-Surrogate-Tutorial-(ND)","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging Surrogate Tutorial (ND)","text":"First of all, let's define the function we are going to build a surrogate for.\n\nusing Plots\nusing Surrogates\n\nNow, let's define the function:\n\nfunction leon(x)\n    x1 = x[1]\n    x2 = x[2]\n    term1 = (x2 - x1^3)^2\n    term2 = (1 - x1)^2\n    y = term1 + term2\nend","category":"section"},{"location":"gek/#Sampling-2","page":"Gradient Enhanced Kriging","title":"Sampling","text":"Let's define our bounds, this time we are working in two dimensions. In particular, we want our first dimension x to have bounds 0, 1, and 0, 1 for the second dimension. We are taking 100 samples of the space using Sobol Sequences. We then evaluate our function on all the sampling points.\n\nn_samples = 100\nlower_bound = [0, 0]\nupper_bound = [1, 1]\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny1 = leon.(xys)\n\nx, y = 0:1, 0:1\np1 = surface(x, y, (x1, x2) -> leon((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, y1)\np2 = contour(x, y, (x1, x2) -> leon((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")","category":"section"},{"location":"gek/#Building-a-surrogate-2","page":"Gradient Enhanced Kriging","title":"Building a surrogate","text":"Using the sampled points, we build the surrogate, the steps are analogous to the 1-dimensional case.\n\ngrad1 = x -> 2 * (x[2] - x[1]^3) * (-3x[1]^2) - 2 * (1 - x[1])\ngrad2 = x -> 2 * (x[2] - x[1]^3)\nd = 2\nn = 100\nfunction create_grads(n, d, grad1, grad2, y1)\n    c = 0\n    y2 = zeros(eltype(y1[1]), n * d)\n    for i in 1:n\n        y2[i + c] = grad1(xys[i])\n        y2[i + c + 1] = grad2(xys[i])\n        c = c + 1\n    end\n    return y2\nend\ny2 = create_grads(n, d, grad1, grad2, y1)\ny = vcat(y1, y2)\n\nmy_GEK = GEK(xys, y, lower_bound, upper_bound)\n\np1 = surface(x, y, (x, y) -> my_GEK([x y]))\nscatter!(xs, ys, y1, marker_z = y1)\np2 = contour(x, y, (x, y) -> my_GEK([x y]))\nscatter!(xs, ys, marker_z = y1)\nplot(p1, p2, title = \"Surrogate\")","category":"section"},{"location":"lp/#Lp-norm-function","page":"Lp norm","title":"Lp norm function","text":"The Lp norm function is defined as: f(x) = sqrtp sum_i=1^d vert x_i vert ^p\n\nLet's import Surrogates and Plots:\n\nusing Surrogates\nusing PolyChaos\nusing Plots\nusing LinearAlgebra\n\nDefine the objective function:\n\nfunction f(x, p)\n    return norm(x, p)\nend\n\nLet's see a simple 1D case:\n\nn = 30\nlb = -5.0\nub = 5.0\np = 1.3\nx = sample(n, lb, ub, SobolSample())\ny = f.(x, p)\nxs = lb:0.001:ub\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lb, ub), ylims = (0, 5), legend = :top)\nplot!(xs, f.(xs, p), label = \"True function\", legend = :top)\n\nFitting different surrogates:\n\nmy_pol = PolynomialChaosSurrogate(x, y, lb, ub)\nloba_1 = LobachevskySurrogate(x, y, lb, ub)\nkrig = Kriging(x, y, lb, ub)\nplot(x, y, seriestype = :scatter, label = \"Sampled points\",\n    xlims = (lb, ub), ylims = (0, 5), legend = :top)\nplot!(xs, f.(xs, p), label = \"True function\", legend = :top)\nplot!(xs, my_pol.(xs), label = \"Polynomial expansion\", legend = :top)\nplot!(xs, loba_1.(xs), label = \"Lobachevsky\", legend = :top)\nplot!(xs, krig.(xs), label = \"Kriging\", legend = :top)","category":"section"},{"location":"#Surrogates.jl:-Surrogate-models-and-optimization-for-scientific-machine-learning","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"A surrogate model is an approximation method that mimics the behavior of a computationally expensive simulation. In more mathematical terms: suppose we are attempting to optimize a function  f(p), but each calculation of  f is very expensive. It may be the case that we need to solve a PDE for each point or use advanced numerical linear algebra machinery, which is usually costly. The idea is then to develop a surrogate model  g which approximates  f by training on previous data collected from evaluations of  f. The construction of a surrogate model can be seen as a three-step process:\n\nSample selection\nConstruction of the surrogate model\nSurrogate optimization\n\nThe sampling methods are super important for the behavior of the surrogate. Sampling can be done through QuasiMonteCarlo.jl, all the functions available there can be used in Surrogates.jl.\n\nThe available surrogates are:\n\nLinear\nRadial Basis\nKriging\nCustom Kriging provided with Stheno\nNeural Network\nSupport Vector Machine\nRandom Forest\nSecond Order Polynomial\nInverse Distance\n\nAfter the surrogate is built, we need to optimize it with respect to some objective function. That is, simultaneously looking for a minimum and sampling the most unknown region. The available optimization methods are:\n\nStochastic RBF (SRBF)\nLower confidence-bound strategy (LCBS)\nExpected improvement (EI)\nDynamic coordinate search (DYCORS)","category":"section"},{"location":"#Multi-output-Surrogates","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Multi-output Surrogates","text":"In certain situations, the function being modeled may have a multi-dimensional output space. In such a case, the surrogate models can take advantage of correlations between the observed output variables to obtain more accurate predictions.\n\nWhen constructing the original surrogate, each element of the passed y vector should itself be a vector. For example, the following y are all valid.\n\nusing Surrogates\nusing StaticArrays\n\nx = sample(5, [0.0; 0.0], [1.0; 1.0], SobolSample())\nf_static = (x) -> StaticVector(x[1], log(x[2]*x[1]))\nf = (x) -> [x, log(x)/2]\n\ny = f_static.(x)\ny = f.(x)\n\nCurrently, the following are implemented as multi-output surrogates:\n\nRadial Basis\nNeural Network (via Flux)\nSecond Order Polynomial\nInverse Distance\nCustom Kriging (via Stheno)","category":"section"},{"location":"#Gradients","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Gradients","text":"The surrogates implemented here are all automatically differentiable via Zygote. Because of this property, surrogates are useful models for processes which aren't explicitly differentiable, and can be used as layers in, for instance, Flux models.","category":"section"},{"location":"#Installation","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Installation","text":"Surrogates is registered in the Julia General Registry. In the REPL:\n\nusing Pkg\nPkg.add(\"Surrogates\")","category":"section"},{"location":"#Contributing","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Contributing","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"section"},{"location":"#Quick-example","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Quick example","text":"using Surrogates\nnum_samples = 10\nlb = 0.0\nub = 10.0\n\n#Sampling\nx = sample(num_samples, lb, ub, SobolSample())\nf = x -> log(x) * x^2 + x^3\ny = f.(x)\n\n#Creating surrogate\nalpha = 2.0\nn = 6\nmy_lobachevsky = LobachevskySurrogate(x, y, lb, ub, alpha = alpha, n = n)\n\n#Approximating value at 5.0\nvalue = my_lobachevsky(5.0)\n\n#Adding more data points\nsurrogate_optimize!(f, SRBF(), lb, ub, my_lobachevsky, RandomSample())\n\n#New approximation\nvalue = my_lobachevsky(5.0)","category":"section"},{"location":"#Reproducibility","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Reproducibility","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>\n\nusing Pkg # hide\nPkg.status() # hide\n\n</details>\n\n<details><summary>and using this machine and Julia version.</summary>\n\nusing InteractiveUtils # hide\nversioninfo() # hide\n\n</details>\n\n<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>\n\nusing Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide\n\n</details>\n\nusing TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"section"},{"location":"moe/#Mixture-of-Experts-(MOE)-Surrogate-Tutorial","page":"MOE","title":"Mixture of Experts (MOE) Surrogate Tutorial","text":"The Mixture of Experts (MOE) Surrogate model represents the interpolating function as a combination of other surrogate models. SurrogatesMOE is a Julia implementation of the Python version from SMT.\n\nMOE is most useful when we have a discontinuous function. For example, let's say we want to build a surrogate for the following function:","category":"section"},{"location":"moe/#1D-Example","page":"MOE","title":"1D Example","text":"function discont_1D(x)\n    if x < 0.0\n        return -5.0\n    elseif x >= 0.0\n        return 5.0\n    end\nend\n\nLet's choose the MOE Surrogate for 1D. Note that we have to import the GaussianMixtures in addition to Surrogates and Plots.\n\nusing Surrogates\nusing GaussianMixtures\nusing Plots\n\nlb = -1.0\nub = 1.0\nx = sample(50, lb, ub, SobolSample())\ny = discont_1D.(x)\nscatter(\n    x, y, label = \"Sampled Points\", xlims = (lb, ub), ylims = (-6.0, 7.0), legend = :top)\n\nHow does a regular surrogate perform on such a dataset?\n\nRAD_1D = RadialBasis(x, y, lb, ub, rad = linearRadial(), scale_factor = 1.0, sparse = false)\nRAD_at0 = RAD_1D(0.0) #true value should be 5.0\n\nAs we can see, the prediction is far from the ground truth. Now, how does the MOE perform?\n\nexpert_types = [\n    RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n        sparse = false),\n    RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n        sparse = false)\n]\n\nMOE_1D_RAD_RAD = MOE(x, y, expert_types)\nMOE_at0 = MOE_1D_RAD_RAD(0.0)\n\nAs we can see, the accuracy is significantly better.","category":"section"},{"location":"moe/#Under-the-Hood-How-SurrogatesMOE-Works","page":"MOE","title":"Under the Hood - How SurrogatesMOE Works","text":"First, we create Gaussian Mixture Models for the number of expert types provided using the x and y values. For example, in the above example, we create two clusters. Then, using a small test dataset kept aside from the input data, we choose the best surrogate model for each of the clusters. At prediction time, we use the appropriate surrogate model based on the cluster to which the new point belongs.","category":"section"},{"location":"moe/#N-Dimensional-Example","page":"MOE","title":"N-Dimensional Example","text":"using Surrogates\nusing GaussianMixtures\n\n# helper to test accuracy of predictors\nfunction rmse(a, b)\n    a = vec(a)\n    b = vec(b)\n    if (size(a) != size(b))\n        println(\"error in inputs\")\n        return\n    end\n    n = size(a, 1)\n    return sqrt(sum((a - b) .^ 2) / n)\nend\n\n# multidimensional input function\nfunction discont_NDIM(x)\n    if (x[1] >= 0.0 && x[2] >= 0.0)\n        return sum(x .^ 2) + 5\n    else\n        return sum(x .^ 2) - 5\n    end\nend\nlb = [-1.0, -1.0]\nub = [1.0, 1.0]\nn = 150\nx = sample(n, lb, ub, RandomSample())\ny = discont_NDIM.(x)\nx_test = sample(10, lb, ub, GoldenSample())\n\nexpert_types = [\n    RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n        sparse = false),\n    RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n        sparse = false)\n]\nmoe_nd_rad_rad = MOE(x, y, expert_types, ndim = 2)\nmoe_pred_vals = moe_nd_rad_rad.(x_test)\ntrue_vals = discont_NDIM.(x_test)\nmoe_rmse = rmse(true_vals, moe_pred_vals)\nrbf = RadialBasis(x, y, lb, ub)\nrbf_pred_vals = rbf.(x_test)\nrbf_rmse = rmse(true_vals, rbf_pred_vals)\n@show rbf_rmse, moe_rmse","category":"section"},{"location":"moe/#Usage-Notes-Example-With-Other-Surrogates","page":"MOE","title":"Usage Notes - Example With Other Surrogates","text":"From the above example, simply change or add to the expert types:\n\nusing Surrogates\n#To use Inverse Distance and Radial Basis Surrogates\nexpert_types = [\n    KrigingStructure(p = [1.0, 1.0], theta = [1.0, 1.0]),\n    InverseDistanceStructure(p = 1.0)\n]\n\n#With 3 Surrogates\nexpert_types = [\n    RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n        sparse = false),\n    LinearStructure(),\n    InverseDistanceStructure(p = 1.0)\n]","category":"section"},{"location":"neural/#Neural-Network-Surrogate-Tutorial","page":"NeuralSurrogate","title":"Neural Network Surrogate Tutorial","text":"It's possible to define a neural network as a surrogate, using Flux. This is useful because we can call optimization methods on it.\n\nFirst of all we will define the Schaffer function we are going to build a surrogate for.\n\nusing Plots\ndefault(c = :matter, legend = false, xlabel = \"x\", ylabel = \"y\")\nusing Surrogates\nusing Flux\n\nfunction schaffer(x)\n    x1 = x[1]\n    x2 = x[2]\n    fact1 = x1^2\n    fact2 = x2^2\n    y = fact1 + fact2\nend","category":"section"},{"location":"neural/#Sampling","page":"NeuralSurrogate","title":"Sampling","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 8, and 0, 8 for the second dimension. We are taking 100 samples of the space using Sobol Sequences. We then evaluate our function on all the sampling points.\n\nn_samples = 100\nlower_bound = [0.0, 0.0]\nupper_bound = [8.0, 8.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys)\n\nx, y = 0:8, 0:8\np1 = surface(x, y, (x1, x2) -> schaffer((x1, x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1, x2) -> schaffer((x1, x2)))\nscatter!(xs, ys)\nplot(p1, p2, title = \"True function\")","category":"section"},{"location":"neural/#Building-a-surrogate","page":"NeuralSurrogate","title":"Building a surrogate","text":"You can specify your own model, optimization function, loss functions, and epochs. As always, getting the model right is the hardest thing.\n\nmodel1 = Chain(\n    Dense(2, 32, relu),\n    Dense(32, 32, relu),\n    Dense(32, 1),\n    first\n)\nneural = NeuralSurrogate(xys, zs, lower_bound, upper_bound, model = model1, n_epochs = 1000)\n\np1 = surface(x, y, (x, y) -> neural([x, y]))\nscatter!(xs, ys, zs, marker_z = zs)\np2 = contour(x, y, (x, y) -> neural([x, y]))\nscatter!(xs, ys, marker_z = zs)\nplot(p1, p2, title = \"Surrogate\")","category":"section"},{"location":"neural/#Optimization","page":"NeuralSurrogate","title":"Optimization","text":"We can now call an optimization function on the neural network:\n\nsurrogate_optimize!(schaffer, SRBF(), lower_bound, upper_bound, neural,\n    SobolSample(), maxiters = 20, num_new_samples = 10)","category":"section"}]
}
