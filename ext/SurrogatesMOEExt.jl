module SurrogatesMOEExt

import Surrogates: linearRadial, cubicRadial, multiquadricRadial,
                   thinplateRadial, RadialBasisStructure, RadialBasis,
                   InverseDistanceSurrogate, Kriging, LobachevskyStructure,
                   LobachevskySurrogate, NeuralStructure, PolyChaosStructure,
                   LinearSurrogate, MOE, 
                   NeuralSurrogate, XGBoostSurrogate, PolynomialChaosSurrogate
using SurrogatesBase
using GaussianMixtures
using Random
using Distributions
using LinearAlgebra

"""
    MOE(x, y, expert_types;  ndim=1, n_clusters=2)

constructor for MOE; takes in x, y and expert types and returns an MOE struct
"""
function MOE(x, y, expert_types; ndim = 1, n_clusters = 2, quantile = 10)
    if (ndim > 1)
        #x = _vector_of_tuples_to_matrix(x)
        X = _vector_of_tuples_to_matrix(x)
        values = hcat(X, y)
    else
        values = hcat(x, y)
    end

    x_and_y_test, x_and_y_train = _extract_part(values, quantile)
    # We get posdef error without jitter; And if values repeat we get NaN vals 
    # https://github.com/davidavdav/GaussianMixtures.jl/issues/21 
    jitter_vals = ((rand(eltype(x_and_y_train), size(x_and_y_train))) ./ 10000)
    gm_cluster = GMM(n_clusters, x_and_y_train + jitter_vals, kind = :full, nInit = 50,
        nIter = 20)
    mvn_distributions = _create_clusters_distributions(gm_cluster, ndim, n_clusters)
    cluster_classifier_train = _cluster_predict(gm_cluster, x_and_y_train)
    clusters_train = _cluster_values(x_and_y_train, cluster_classifier_train, n_clusters)
    cluster_classifier_test = _cluster_predict(gm_cluster, x_and_y_test)
    clusters_test = _cluster_values(x_and_y_test, cluster_classifier_test, n_clusters)
    best_models = []
    for i in 1:n_clusters
        best_model = _find_best_model(clusters_train[i], clusters_test[i], ndim,
            expert_types)
        push!(best_models, best_model)
    end
    # X = values[:, 1:ndim]
    # y = values[:, 2]

    #return MOE(X, y, gm_cluster, mvn_distributions, best_models)
    return MOE(x, y, gm_cluster, mvn_distributions, best_models, expert_types, ndim,
        n_clusters)
end

"""
    (moe::MOE)(val::Number)

predictor for 1D inputs
"""
function (moe::MOE)(val::Number)
    val = [val]
    weights = GaussianMixtures.weights(moe.c)
    rvs = [Distributions.pdf(moe.d[k], val) for k in 1:length(weights)]
    probs = weights .* rvs
    rad = sum(probs)
    if rad > 0
        probs = probs / rad
    end
    max_index = argmax(probs)
    prediction = moe.m[max_index](val[1])
    return prediction
end

"""
    (moe::MOE)(val)

predictor for ndimensional inputs
"""
function (moe::MOE)(val)
    val = collect(val) #to handle inputs that may sometimes be tuples
    weights = GaussianMixtures.weights(moe.c)
    rvs = [Distributions.pdf(moe.d[k], val) for k in 1:length(weights)]
    probs = weights .* rvs
    rad = sum(probs)

    if rad > 0
        probs = probs ./ rad
    end

    max_index = argmax(probs)
    prediction = moe.m[max_index](val)
    return prediction
end

"""
    _cluster_predict(gmm:GMM, X::Matrix)

gmm - a trained Gaussian Mixture Model
X - a matrix of points with dimensions equal to the inputs used for the
training of the model

Return - Clusters to which each of the points belong to (starts at int 1)

Example:
X = [1.0 2; 1 4; 1 0; 10 2; 10 4; 10 0] + rand(Float64, (6, 2))
gm = GMM(2, X)
_cluster_predict(gm,  [0.0 0.0; 12.0 3.0]) #returns [1,2]
"""
function _cluster_predict(gmm::GMM, X::Matrix)
    llpg_X = llpg(gmm, X) #log likelihood probability of X belonging to each of the clusters in the gaussian mixture
    return map(argmax, eachrow(llpg_X))
end

"""
    _extract_part(values, quantile)
    values - a matrix containing all the input values (n test points by d dimensions)
    quantiles - the interval between rows 
    returns a test values matrix and a training values matrix
    Ex: 
    values = [1.0 2.0; 3.0 4.0; 5.0 6.0; 7.0 8.0; 9.0 10]
    quantile = 4
    test, train = _extract_part(values, quantile)
    test # [1.0   2.0; 9.0  10.0]
    train #  [3.0  4.0; 5.0  6.0; 7.0  8.0]
"""
function _extract_part(values, quantile)
    num = size(values, 1)
    indices = collect(1:quantile:num)
    mask = falses(num)
    mask[indices] .= true
    #mask
    return values[mask, :], values[.~mask, :]
end

"""
    _cluster_values(values, cluster_classifier, num_clusters)

values - a concatenation of input and output values
cluster_classifier - a vector of integers representing which cluster each data point belongs to
num_clusters - number of clusters

output
clusters - values grouped by clusters

## Ex:

vals = [1.0 2.0; 3.0 4.0; 5.0 6.0; 7.0 8.0; 9.0 10.0]
cluster_classifier = [1, 2, 2, 2, 1]
num_clusters = 2
clusters = _cluster_values(vals, cluster_classifier, num_clusters)
@show clusters #prints values below

    [[1.0, 2.0], [9.0, 10.0]]
    [[3.0, 4.0], [5.0, 6.0], [7.0, 8.0]]
"""
function _cluster_values(values, cluster_classifier, num_clusters)
    num = length(cluster_classifier)
    if (size(values, 1) != num)
        error("Number of values don't match number of cluster_classifier points")
    end
    clusters = [[] for n in 1:num_clusters]
    for i in 1:num
        push!(clusters[cluster_classifier[i]], (values[i, :]))
    end
    return clusters
end

"""
_create_clusters_distributions(gmm::GMM, ndim, n_clusters)
gmm - a gaussian mixture model with concatenated X and y values that have been clustered
ndim - number of dimensions in X
n_clusters - number of clusters

output
distribs - a vector containing frozen multivariate normal distributions for each cluster
"""
function _create_clusters_distributions(gmm::GMM, ndim, n_clusters)
    means = gmm.Î¼
    cov = covars(gmm)
    distribs = []

    for k in 1:n_clusters
        meansk = means[k, 1:ndim]
        covk = cov[k][1:ndim, 1:ndim]
        mvn = MvNormal(meansk, covk) # todo - check if we need allow_singular=True and implement
        push!(distribs, mvn)
    end
    return distribs
end

"""
_find_upper_lower_bounds(m::Matrix)
returns upper and lower bounds in vector form
"""
function _find_upper_lower_bounds(X::Matrix)
    ub = []
    lb = []
    for col in eachcol(X)
        push!(ub, findmax(col)[1])
        push!(lb, findmin(col)[1])
    end
    if (size(X, 2) == 1)
        return lb[1][1], ub[1][1]
    else
        return lb, ub
    end
end

"""
_find_best_model(clustered_values, clustered_test_values)
finds best model for each set of clustered values by validating against the clustered_test_values
"""
function _find_best_model(clustered_train_values, clustered_test_values, dim,
        enabled_expert_types)
    # find upper and lower bounds for clustered_train and test values concatenated

    x_vec = [a[1:dim] for a in clustered_train_values]
    y_vec = [last(a) for a in clustered_train_values]

    x_test_vec = [a[1:dim] for a in clustered_test_values]
    y_test_vec = [last(a) for a in clustered_test_values]

    if (dim == 1)
        xtrain_mat = reshape(x_vec, (size(clustered_train_values, 1), dim))
        xtest_mat = reshape(x_test_vec, (size(clustered_test_values, 1), dim))
    else
        xtrain_mat = _vector_of_tuples_to_matrix(x_vec)
        xtest_mat = _vector_of_tuples_to_matrix(x_test_vec)
    end

    X = !isnothing(xtest_mat) ? vcat(xtrain_mat, xtest_mat) : xtrain_mat
    x_test_vec = !isnothing(xtest_mat) ? x_test_vec : x_vec
    y_test_vec = !isnothing(xtest_mat) ? y_test_vec : y_vec
    lb, ub = _find_upper_lower_bounds(X)

    # call on _surrogate_builder with clustered_train_vals, enabled expert types, lb, ub 

    surr_vec = _surrogate_builder(
        enabled_expert_types, length(enabled_expert_types), x_vec,
        y_vec, lb, ub)

    # use the models to find best model after validating against test data and return best model 
    best_rmse = Inf
    best_model = surr_vec[1] #initial assignment can be any model
    for surr_model in surr_vec
        pred = surr_model.(x_test_vec)
        rmse = norm(pred - y_test_vec, 2)
        if (rmse < best_rmse)
            best_rmse = rmse
            best_model = surr_model
        end
    end
    return best_model
end

"""
    _surrogate_builder(local_kind, k, x, y, lb, ub)

takes in an array of surrogate types, and number of cluster, builds the surrogates and returns
an array of surrogate objects
"""
function _surrogate_builder(local_kind, k, x, y, lb, ub)
    local_surr = []
    for i in 1:k
        if local_kind[i][1] == "RadialBasis"
            #fit and append to local_surr
            my_local_i = RadialBasis(x, y, lb, ub,
                rad = local_kind[i].radial_function,
                scale_factor = local_kind[i].scale_factor,
                sparse = local_kind[i].sparse)
            push!(local_surr, my_local_i)

        elseif local_kind[i][1] == "Kriging"
            #because Kriging takes abs of two vectors
            if (length(lb) == 1)
                x = [a[1] for a in x]
            end

            my_local_i = Kriging(x, y, lb, ub, p = local_kind[i].p,
                theta = local_kind[i].theta)
            push!(local_surr, my_local_i)

        elseif local_kind[i][1] == "GEK"
            my_local_i = GEK(x, y, lb, ub, p = local_kind[i].p,
                theta = local_kind[i].theta)
            push!(local_surr, my_local_i)

        elseif local_kind[i] == "LinearSurrogate"
            my_local_i = LinearSurrogate(x, y, lb, ub)
            push!(local_surr, my_local_i)

        elseif local_kind[i][1] == "InverseDistanceSurrogate"
            my_local_i = InverseDistanceSurrogate(x, y, lb, ub, local_kind[i].p)
            push!(local_surr, my_local_i)

        elseif local_kind[i][1] == "LobachevskySurrogate"
            my_local_i = LobachevskyStructure(x, y, lb, ub,
                alpha = local_kind[i].alpha,
                n = local_kind[i].n,
                sparse = local_kind[i].sparse)
            push!(local_surr, my_local_i)

        elseif local_kind[i][1] == "NeuralSurrogate"
            my_local_i = NeuralSurrogate(x, y, lb, ub,
                model = local_kind[i].model,
                loss = local_kind[i].loss, opt = local_kind[i].opt,
                n_epochs = local_kind[i].n_epochs)
            push!(local_surr, my_local_i)

        elseif local_kind[i][1] == "XGBoostSurrogate"
            my_local_i = XGBoostSurrogate(x, y, lb, ub,
                num_round = local_kind[i].num_round)
            push!(local_surr, my_local_i)

        elseif local_kind[i] == "SecondOrderPolynomialSurrogate"
            my_local_i = SecondOrderPolynomialSurrogate(x, y, lb, ub)
            push!(local_surr, my_local_i)

        elseif local_kind[i][1] == "Wendland"
            my_local_i = Wendand(x, y, lb, ub, eps = local_kind[i].eps,
                maxiters = local_kind[i].maxiters, tol = local_kind[i].tol)
            push!(local_surr, my_local_i)

        elseif local_kind[i][1] == "PolynomialChaosSurrogate"
            my_local_i = PolynomialChaosSurrogate(x, y, lb, ub, op = local_kind[i].op)
            push!(local_surr, my_local_i)
        else
            throw("A surrogate with name provided does not exist or is not currently supported with MOE.")
        end
    end
    return local_surr
end

"""
    update!(m::MOE, new_x, new_y)

add a new point to the dataset.
"""
function SurrogatesBase.update!(m::MOE, x, y)
    #function update!(m) #this works
    push!(m.x, x)
    push!(m.y, y)

    quantile = 10

    if (m.nd > 1) #numbef of dimensions
        X = _vector_of_tuples_to_matrix(m.x)
        values = hcat(X, m.y)
    else
        values = hcat(m.x, m.y)
    end
    x_and_y_test, x_and_y_train = _extract_part(values, quantile)
    # We get posdef error without jitter; And if values repeat we get NaN vals 
    # https://github.com/davidavdav/GaussianMixtures.jl/issues/21 
    jitter_vals = ((rand(eltype(x_and_y_train), size(x_and_y_train))) ./ 10000)
    gm_cluster = GMM(m.nc, x_and_y_train + jitter_vals, kind = :full, nInit = 50,
        nIter = 20)
    mvn_distributions = _create_clusters_distributions(gm_cluster, m.nd, m.nc)
    cluster_classifier_train = _cluster_predict(gm_cluster, x_and_y_train)
    clusters_train = _cluster_values(x_and_y_train, cluster_classifier_train, m.nc)
    cluster_classifier_test = _cluster_predict(gm_cluster, x_and_y_test)
    clusters_test = _cluster_values(x_and_y_test, cluster_classifier_test, m.nc)
    best_models = []
    for i in 1:(m.nc)
        best_model = _find_best_model(clusters_train[i], clusters_test[i], m.nd,
            m.e)
        push!(best_models, best_model)
    end
    m.c = gm_cluster
    m.d = mvn_distributions
    m.m = best_models
end

"""
    _vector_of_tuples_to_matrix(v)

takes in a vector of tuples or vector of vectors and converts it into a matrix
"""
function _vector_of_tuples_to_matrix(v)
    if !isempty(v)
        num_rows = length(v)
        num_cols = length(first(v))
        K = zeros(num_rows, num_cols)
        for row in 1:num_rows
            for col in 1:num_cols
                K[row, col] = v[row][col]
            end
        end
        return K
    end
    return nothing
end

end #module
