var documenterSearchIndex = {"docs":
[{"location":"tutorials/#Surrogates-101-1","page":"Basics","title":"Surrogates 101","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's start with something easy to get our hands dirty. I want to build a surrogate for f(x) = log(x)*x^2+x^3. Let's choose the radial basis surrogate.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> log(x)*x^2+x^3\nlb = 1.0\nub = 10.0\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at 5.4\napprox = my_radial_basis(5.4)","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now see an example in 2D.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing LinearAlgebra\nf = x -> x[1]*x[2]\nlb = [1.0,2.0]\nub = [10.0,8.5]\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at (1.0,1.4)\napprox = my_radial_basis((1.0,1.4))","category":"page"},{"location":"tutorials/#Kriging-standard-error-1","page":"Basics","title":"Kriging standard error","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now use the Kriging surrogate, which is a single-output Gaussian process. This surrogate has a nice feature: not only does it approximate the solution at a point, it also calculates the standard error at such point. Let's see an example:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> exp(x)*x^2+x^3\nlb = 0.0\nub = 10.0\nx = sample(100,lb,ub,UniformSample())\ny = f.(x)\np = 1.9\nmy_krig = Kriging(x,y,lb,ub,p=p)\n\n#I want an approximation at 5.4\napprox = my_krig(5.4)\n\n#I want to find the standard error at 5.4\nstd_err = std_error_at_point(my_krig,5.4)","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now optimize the Kriging surrogate using Lower confidence bound method, this is just a one-liner:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"surrogate_optimize(f,LCBS(),lb,ub,my_krig,UniformSample())","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Surrogate optimization methods have two purposes: they both sample the space in unknown regions and look for the minima at the same time.","category":"page"},{"location":"tutorials/#Lobachevsky-integral-1","page":"Basics","title":"Lobachevsky integral","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"The Lobachevsky surrogate has the nice feature of having a closed formula for its integral, which is something that other surrogates are missing. Let's compare it with QuadGK:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing QuadGK\nobj = x -> 3*x + log(x)\na = 1.0\nb = 4.0\nx = sample(2000,a,b,SobolSample())\ny = obj.(x)\nalpha = 2.0\nn = 6\nmy_loba = LobachevskySurrogate(x,y,a,b,alpha=alpha,n=n)\n\n#1D integral\nint_1D = lobachevsky_integral(my_loba,a,b)\nint = quadgk(obj,a,b)\nint_val_true = int[1]-int[2]\n@assert int_1D ≈ int_val_true","category":"page"},{"location":"tutorials/#Example-of-NeuralSurrogate-1","page":"Basics","title":"Example of NeuralSurrogate","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Basic example of fitting a neural network on a simple function of two variables.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing Flux\nusing Statistics\n\nf = x -> x[1]^2 + x[2]^2\nbounds = Float32[-1.0, -1.0], Float32[1.0, 1.0]\n# Flux models are in single precision by default.\n# Thus, single precision will also be used here for our training samples.\n\nx_train = sample(100, bounds..., SobolSample())\ny_train = f.(x_train)\n\n# Perceptron with one hidden layer of 20 neurons.\nmodel = Chain(Dense(2, 20, relu), Dense(20, 1))\nloss(x, y) = Flux.mse(model(x), y)\n\n# Training of the neural network\nlearning_rate = 0.1\noptimizer = Descent(learning_rate)  # Simple gradient descent. See Flux documentation for other options.\nn_epochs = 50\nsgt = NeuralSurrogate(x_train, y_train, bounds..., model=model, loss=loss, opt=optimizer, n_echos=n_epochs)\n\n# Testing the new model\nx_test = sample(30, bounds..., SobolSample())\ntest_error = mean(abs2, sgt(x)[1] - f(x) for x in x_test)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"The Inverse Distance Surrogate is an interpolating method and in this method the unknown points are calculated with a weighted average of the sampling points. This model uses the inverse distance between the unknown and training points to predict the unknown point. We do not need to fit this model because the response of an unknown point x is computed with respect to the distance between x and the training points.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Let's optimize following function to use Inverse Distance Surrogate:","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"f(x) = sin(x) + sin(x)^2 + sin(x)^3","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":".","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"First of all, we have to import these two packages: Surrogates and Plots.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"InverseDistance/#Sampling-1","page":"InverseDistance","title":"Sampling","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"We choose to sample f in 25 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy, this can be done by passing LowDiscrepancySample() to the sample function.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"f(x) = sin(x) + sin(x)^2 + sin(x)^3\r\n\r\nn_samples = 25\r\nlower_bound = 0.0\r\nupper_bound = 10.0\r\nx = sample(n_samples, lower_bound, upper_bound, LowDiscrepancySample(2))\r\ny = f.(x)\r\n\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Building-a-Surrogate-1","page":"InverseDistance","title":"Building a Surrogate","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"InverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\r\nadd_point!(InverseDistance, 5.0, f(5.0))\r\nadd_point!(InverseDistance, [5.1,5.2], [f(5.1),f(5.2)])\r\nprediction = InverseDistance(5.0)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Now, we will simply plot InverseDistance:","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(InverseDistance, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Optimizing-1","page":"InverseDistance","title":"Optimizing","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample())\r\nscatter(x, y, label=\"Sampled points\", legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(InverseDistance, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Inverse-Distance-Surrogate-Tutorial-(ND):-1","page":"InverseDistance","title":"Inverse Distance Surrogate Tutorial (ND):","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"First of all we will define the Schaffer function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide\r\n\r\nfunction schaffer(x)\r\n    x1=x[1]\r\n    x2=x[2]\r\n    fact1 = (sin(x1^2-x2^2))^2 - 0.5;\r\n    fact2 = (1 + 0.001*(x1^2+x2^2))^2;\r\n    y = 0.5 + fact1/fact2;\r\nend","category":"page"},{"location":"InverseDistance/#Sampling-2","page":"InverseDistance","title":"Sampling","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"n_samples = 60\r\nlower_bound = [-5.0, 0.0]\r\nupper_bound = [10.0, 15.0]\r\n\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = schaffer.(xys);","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"x, y = -5:10, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"InverseDistance/#Building-a-surrogate-1","page":"InverseDistance","title":"Building a surrogate","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"InverseDistance = InverseDistanceSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"InverseDistance/#Optimizing-2","page":"InverseDistance","title":"Optimizing","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"With our surrogate we can now search for the minimas of the function.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"size(xys)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample(), maxiters=10)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"size(xys)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nzs = schaffer.(xys) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2) # hide","category":"page"},{"location":"contributing/#Contributions-1","page":"Contributing","title":"Contributions","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Contributions are very welcome! There are many ways do help:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Opening/solving issues\nMaking the code more efficient\nOpening a new PR with a new Sampling technique, Surrogate or optimization method\nWriting more tutorials with your own unique use case of the library\nYour own idea!","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"You can also contact me on the Julia slack channel at @ludoro.","category":"page"},{"location":"contributing/#List-of-contributors-1","page":"Contributing","title":"List of contributors","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Ludovico Bessi (@ludoro)\nChris Rackauckas (@ChrisRackauckas)\nRohit Singh Rathaur (@RohitRathore1)\nAndrea Cognolato (@mrandri19)\nKanav Gupta (@kanav99)","category":"page"},{"location":"ackley/#Ackley-function-1","page":"Ackley function","title":"Ackley function","text":"","category":"section"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"The Ackley function is defined as: f(x) = -a*exp(-bsqrtfrac1dsum_i=1^d x_i^2) - exp(frac1d sum_i=1^d cos(cx_i)) + a + exp(1) Usually the recommended values are: a =  20, b = 02 and c =  2pi","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"Let's see the 1D case.","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"Now, let's define the Ackley function:","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"function ackley(x)\n    a, b, c = 20.0, -0.2, 2.0*π\n    len_recip = inv(length(x))\n    sum_sqrs = zero(eltype(x))\n    sum_cos = sum_sqrs\n    for i in x\n        sum_cos += cos(c*i)\n        sum_sqrs += i^2\n    end\n    return (-a * exp(b * sqrt(len_recip*sum_sqrs)) -\n            exp(len_recip*sum_cos) + a + 2.71)\nend","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"n = 100\nlb = -32.768\nub = 32.768\nx = sample(n, lb, ub, SobolSample())\ny = ackley.(x)\nxs = lb:0.001:ub\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0,30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"my_rad = RadialBasis(x, y, lb, ub)\nmy_krig = Kriging(x, y, lb, ub)\nmy_loba = LobachevskySurrogate(x, y, lb, ub)","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"scatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_rad.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, my_krig.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, my_loba.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"The fit looks good. Let's now see if we are able to find the minimum value using optimization methods:","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"surrogate_optimize(ackley,DYCORS(),lb,ub,my_rad,UniformSample())\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_rad.(xs), label=\"Radial basis optimized\", legend=:top)","category":"page"},{"location":"ackley/#","page":"Ackley function","title":"Ackley function","text":"The DYCORS methods successfully finds the minimum.","category":"page"},{"location":"wendland/#Wendland-Surrogate-1","page":"Wendland","title":"Wendland Surrogate","text":"","category":"section"},{"location":"wendland/#","page":"Wendland","title":"Wendland","text":"The Wendland surrogate is a compact surrogate: it allocates much less memory then other surrogates. The coefficients are found using an iterative solver.","category":"page"},{"location":"wendland/#","page":"Wendland","title":"Wendland","text":"f = x - exp(-x^2)","category":"page"},{"location":"wendland/#","page":"Wendland","title":"Wendland","text":"using Surrogates\nusing Plots","category":"page"},{"location":"wendland/#","page":"Wendland","title":"Wendland","text":"n = 40\nlower_bound = 0.0\nupper_bound = 1.0\nf = x -> exp(-x^2)\nx = sample(n,lower_bound,upper_bound,SobolSample())\ny = f.(x)","category":"page"},{"location":"wendland/#","page":"Wendland","title":"Wendland","text":"We choose to sample f in 30 points between 5 to 25 using sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"wendland/#Building-Surrogate-1","page":"Wendland","title":"Building Surrogate","text":"","category":"section"},{"location":"wendland/#","page":"Wendland","title":"Wendland","text":"The choice of the right parameter is especially important here: a slight change in ϵ would produce a totally different fit. Try it yourself with this function!","category":"page"},{"location":"wendland/#","page":"Wendland","title":"Wendland","text":"my_eps = 0.5\nwend = Wendland(x,y,lower_bound,upper_bound,eps=my_eps)","category":"page"},{"location":"wendland/#","page":"Wendland","title":"Wendland","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(wend, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"lobachevsky/#Lobachevsky-surrogate-tutorial-1","page":"Lobachevsky","title":"Lobachevsky surrogate tutorial","text":"","category":"section"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"Lobachevsky splines function is a function that used for univariate and multivariate scattered interpolation. Introduced by Lobachevsky in 1842 to investigate errors in astronomical measurements.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"We are going to use a Lobachevsky surrogate to optimize f(x)=sin(x)+sin(103 * x).","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"lobachevsky/#Sampling-1","page":"Lobachevsky","title":"Sampling","text":"","category":"section"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"We choose to sample f in 4 points between 0 and 4 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"f(x) = sin(x) + sin(10/3 * x)\nn_samples = 5\nlower_bound = 1.0\nupper_bound = 4.0\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"lobachevsky/#Building-a-surrogate-1","page":"Lobachevsky","title":"Building a surrogate","text":"","category":"section"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"With our sampled points we can build the Lobachevsky surrogate using the LobachevskySurrogate function.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"lobachevsky_surrogate behaves like an ordinary function which we can simply plot. Alpha is the shape parameters and n specify how close you want lobachevsky function to radial basis function.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"alpha = 2.0\nn = 6\nlobachevsky_surrogate = LobachevskySurrogate(x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(lobachevsky_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"lobachevsky/#Optimizing-1","page":"Lobachevsky","title":"Optimizing","text":"","category":"section"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, lobachevsky_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(lobachevsky_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"In the example below, it shows how to use lobachevsky_surrogate for higher dimension problems.","category":"page"},{"location":"lobachevsky/#Lobachevsky-Surrogate-Tutorial-(ND):-1","page":"Lobachevsky","title":"Lobachevsky Surrogate Tutorial (ND):","text":"","category":"section"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"First of all we will define the Schaffer function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction schaffer(x)\n    x1=x[1]\n    x2=x[2]\n    fact1 = x1 ^2;\n    fact2 = x2 ^2;\n    y = fact1 + fact2;\nend","category":"page"},{"location":"lobachevsky/#Sampling-2","page":"Lobachevsky","title":"Sampling","text":"","category":"section"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 8, and 0, 8 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"n_samples = 60\nlower_bound = [0.0, 0.0]\nupper_bound = [8.0, 8.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"x, y = 0:8, 0:8 # hide\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"lobachevsky/#Building-a-surrogate-2","page":"Lobachevsky","title":"Building a surrogate","text":"","category":"section"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"Lobachevsky = LobachevskySurrogate(xys, zs,  lower_bound, upper_bound, alpha = [2.4,2.4], n=8)","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"lobachevsky/#Optimizing-2","page":"Lobachevsky","title":"Optimizing","text":"","category":"section"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"With our surrogate we can now search for the minimas of the function.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"size(Lobachevsky.x)","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, Lobachevsky, SobolSample(), maxiters=1, num_new_samples=10)","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"size(Lobachevsky.x)","category":"page"},{"location":"lobachevsky/#","page":"Lobachevsky","title":"Lobachevsky","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\nxys = Lobachevsky.x # hide\nxs = [i[1] for i in xys] # hide\nys = [i[2] for i in xys] # hide\nzs = schaffer.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"randomforest/#Random-forests-surrogate-tutorial-1","page":"RandomForest","title":"Random forests surrogate tutorial","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"Random forests is a supervised learning algorithm that randomly creates and merges multiple decision trees into one forest.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"We are going to use a Random forests surrogate to optimize f(x)=sin(x)+sin(103 * x).","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"randomforest/#Sampling-1","page":"RandomForest","title":"Sampling","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"f(x) = sin(x) + sin(10/3 * x)\nn_samples = 5\nlower_bound = 2.7\nupper_bound = 7.5\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"randomforest/#Building-a-surrogate-1","page":"RandomForest","title":"Building a surrogate","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"With our sampled points we can build the Random forests surrogate using the RandomForestSurrogate function.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"randomforest_surrogate behaves like an ordinary function which we can simply plot. Addtionally you can specify the number of trees created using the parameter num_round","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"num_round = 2\nrandomforest_surrogate = RandomForestSurrogate(x ,y ,lower_bound, upper_bound, num_round = 2)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(randomforest_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"randomforest/#Optimizing-1","page":"RandomForest","title":"Optimizing","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, randomforest_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(randomforest_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"randomforest/#Random-Forest-ND-1","page":"RandomForest","title":"Random Forest ND","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"First of all we will define the Bukin Function N. 6 function we are going to build surrogate for.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction bukin6(x)\n    x1=x[1]\n    x2=x[2]\n    term1 = 100 * sqrt(abs(x2 - 0.01*x1^2));\n    term2 = 0.01 * abs(x1+10);\n    y = term1 + term2;\nend","category":"page"},{"location":"randomforest/#Sampling-2","page":"RandomForest","title":"Sampling","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"n_samples = 50\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = bukin6.(xys);","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"x, y = -5:10, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> bukin6((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> bukin6((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"randomforest/#Building-a-surrogate-2","page":"RandomForest","title":"Building a surrogate","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"RandomForest = RandomForestSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"p1 = surface(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"randomforest/#Optimizing-2","page":"RandomForest","title":"Optimizing","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"With our surrogate we can now search for the minimas of the function.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"size(xys)","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"surrogate_optimize(bukin6, SRBF(), lower_bound, upper_bound, RandomForest, SobolSample(), maxiters=20)","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"size(xys)","category":"page"},{"location":"randomforest/#","page":"RandomForest","title":"RandomForest","text":"p1 = surface(x, y, (x, y) -> RandomForest([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = bukin6.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"tensor_prod/#Tensor-product-function-1","page":"Tensor product","title":"Tensor product function","text":"","category":"section"},{"location":"tensor_prod/#","page":"Tensor product","title":"Tensor product","text":"The tensor product function is defined as: f(x) = prod_i=1^d cos(api x_i)","category":"page"},{"location":"tensor_prod/#","page":"Tensor product","title":"Tensor product","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"tensor_prod/#","page":"Tensor product","title":"Tensor product","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"tensor_prod/#","page":"Tensor product","title":"Tensor product","text":"Define the 1D objective function:","category":"page"},{"location":"tensor_prod/#","page":"Tensor product","title":"Tensor product","text":"function f(x)\n    a = 0.5;\n    return cos(a*pi*x)\nend","category":"page"},{"location":"tensor_prod/#","page":"Tensor product","title":"Tensor product","text":"n = 30\nlb = -5.0\nub = 5.0\na = 0.5\nx = sample(n, lb, ub, SobolSample())\ny = f.(x)\nxs = lb:0.001:ub\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(-1, 1), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"tensor_prod/#","page":"Tensor product","title":"Tensor product","text":"Fitting and plotting different surrogates:","category":"page"},{"location":"tensor_prod/#","page":"Tensor product","title":"Tensor product","text":"loba_1 = LobachevskySurrogate(x, y, lb, ub)\nkrig = Kriging(x, y, lb, ub)\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2.5, 2.5), legend=:bottom)\nplot!(xs,f.(xs), label=\"True function\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"welded_beam/#","page":"Welded beam function","title":"Welded beam function","text":"#Welded beam function","category":"page"},{"location":"welded_beam/#","page":"Welded beam function","title":"Welded beam function","text":"The welded beam function is defined as: f(hlt) = sqrtfraca^2 + b^2 + ablsqrt025(l^2+(h+t)^2) With: a = frac6000sqrt2hl b = frac6000(14 + 05l)*sqrt025(l^2+(h+t)^2)2*0707hl(fracl^212+025*(h+t)^2)","category":"page"},{"location":"welded_beam/#","page":"Welded beam function","title":"Welded beam function","text":"It has 3 dimension.","category":"page"},{"location":"welded_beam/#","page":"Welded beam function","title":"Welded beam function","text":"using Surrogates\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"welded_beam/#","page":"Welded beam function","title":"Welded beam function","text":"Define the objective function:","category":"page"},{"location":"welded_beam/#","page":"Welded beam function","title":"Welded beam function","text":"function f(x)\n    h = x[1]\n    l = x[2]\n    t = x[3]\n    a = 6000/(sqrt(2)*h*l)\n    b = (6000*(14+0.5*l)*sqrt(0.25*(l^2+(h+t)^2)))/(2*(0.707*h*l*(l^2/12 + 0.25*(h+t)^2)))\n    return (sqrt(a^2+b^2 + l*a*b))/(sqrt(0.25*(l^2+(h+t)^2)))\nend","category":"page"},{"location":"welded_beam/#","page":"Welded beam function","title":"Welded beam function","text":"n = 300\nd = 3\nlb = [0.125,5.0,5.0]\nub = [1.,10.,10.]\nx = sample(n,lb,ub,SobolSample())\ny = f.(x)\nn_test = 1000\nx_test = sample(n_test,lb,ub,GoldenSample());\ny_true = f.(x_test);","category":"page"},{"location":"welded_beam/#","page":"Welded beam function","title":"Welded beam function","text":"my_rad = RadialBasis(x,y,lb,ub)\ny_rad = my_rad.(x_test)\nmse_rad = norm(y_true - y_rad,2)/n_test\nprint(\"MSE Radial: $mse_rad\")\n\nmy_krig = Kriging(x,y,lb,ub)\ny_krig = my_krig.(x_test)\nmse_krig = norm(y_true - y_krig,2)/n_test\nprint(\"MSE Kriging: $mse_krig\")\n\nmy_loba = LobachevskySurrogate(x,y,lb,ub)\ny_loba = my_loba.(x_test)\nmse_rad = norm(y_true - y_loba,2)/n_test\nprint(\"MSE Lobachevsky: $mse_rad\")","category":"page"},{"location":"polychaos/#Polynomial-chaos-surrogate-1","page":"Polynomial Chaos","title":"Polynomial chaos surrogate","text":"","category":"section"},{"location":"polychaos/#","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"We can create a surrogate using a polynomial expansion, with a different polynomial basis depending on the distribution of the data we are trying to fit. Under the hood, PolyChaos.jl has been used. It is possible to specify a type of polynomial for each dimension of the problem.","category":"page"},{"location":"polychaos/#","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"polychaos/#Sampling-1","page":"Polynomial Chaos","title":"Sampling","text":"","category":"section"},{"location":"polychaos/#","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"We choose to sample f in 25 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy, this can be done by passing LowDiscrepancySample() to the sample function.","category":"page"},{"location":"polychaos/#","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"using Surrogates\nusing Plots\nn = 20\nlower_bound = 1.0\nupper_bound = 6.0\nx = sample(n,lower_bound,upper_bound,LowDiscrepancySample(2))\nf = x -> log(x)*x + sin(x)\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"polychaos/#Building-a-Surrogate-1","page":"Polynomial Chaos","title":"Building a Surrogate","text":"","category":"section"},{"location":"polychaos/#","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"poly1 = PolynomialChaosSurrogate(x,y,lower_bound,upper_bound)\npoly2 = PolynomialChaosSurrogate(x,y,lower_bound,upper_bound, op = GaussOrthoPoly(5))\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(poly1, label=\"First polynomial\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(poly2, label=\"Second polynomial\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"sphere_function/#Sphere-function-1","page":"Sphere function","title":"Sphere function","text":"","category":"section"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"The sphere function of dimension d is defined as: f(x) = sum_i=1^d x_i with lower bound -10 and upper bound 10.","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"Define the objective function:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"function sphere_function(x)\n    return sum(x.^2)\nend","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"The 1D case is just a simple parabola, let's plot it:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"n = 20\nlb = -10\nub = 10\nx = sample(n,lb,ub,SobolSample())\ny = sphere_function.(x)\nxs = lb:0.001:ub\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"Fitting RadialSurrogate with different radial basis:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"rad_1d_linear = RadialBasis(x,y,lb,ub)\nrad_1d_cubic = RadialBasis(x,y,lb,ub,rad = cubicRadial)\nrad_1d_multiquadric = RadialBasis(x,y,lb,ub, rad = multiquadricRadial)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)\nplot!(xs, rad_1d_linear.(xs), label=\"Radial surrogate with linear\", legend=:top)\nplot!(xs, rad_1d_cubic.(xs), label=\"Radial surrogate with cubic\", legend=:top)\nplot!(xs, rad_1d_multiquadric.(xs), label=\"Radial surrogate with multiquadric\", legend=:top)","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"Fitting Lobachevsky Surrogate with different values of hyperparameters alpha:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"loba_1 = LobachevskySurrogate(x,y,lb,ub)\nloba_2 = LobachevskySurrogate(x,y,lb,ub,alpha = 1.5, n = 6)\nloba_3 = LobachevskySurrogate(x,y,lb,ub,alpha = 0.3, n = 6)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky surrogate 1\", legend=:top)\nplot!(xs, loba_2.(xs), label=\"Lobachevsky surrogate 2\", legend=:top)\nplot!(xs, loba_3.(xs), label=\"Lobachevsky surrogate 3\", legend=:top)","category":"page"},{"location":"kriging/#Kriging-surrogate-tutorial-(1D)-1","page":"Kriging","title":"Kriging surrogate tutorial (1D)","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"We are going to use a Kriging surrogate to optimize f(x)=(6x-2)^2sin(12x-4). (function from Forrester et al. (2008)).","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"kriging/#Sampling-1","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"# https://www.sfu.ca/~ssurjano/forretal08.html\n# Forrester et al. (2008) Function\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\n\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 1.0\n\nxs = lower_bound:0.001:upper_bound\n\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17))\nplot!(xs, f.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"kriging/#Building-a-surrogate-1","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"With our sampled points we can build the Kriging surrogate using the Kriging function.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"kriging_surrogate behaves like an ordinary function which we can simply plot. A nice statistical property of this surrogate is being able to calculate the error of the function at each point, we plot this as a confidence interval using the ribbon argument.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(x, y, lower_bound, upper_bound, p=1.9);\n\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)\nplot!(xs, kriging_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p), legend=:top)","category":"page"},{"location":"kriging/#Optimizing-1","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample())\n\nscatter(x, y, label=\"Sampled points\", ylims=(-7, 7), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)\nplot!(xs, kriging_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p), legend=:top)","category":"page"},{"location":"kriging/#Kriging-surrogate-tutorial-(ND)-1","page":"Kriging","title":"Kriging surrogate tutorial (ND)","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"First of all let's define the function we are going to build a surrogate for. Notice how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction branin(x)\n    x1=x[1]\n    x2=x[2]\n    a=1;\n    b=5.1/(4*π^2);\n    c=5/π;\n    r=6;\n    s=10;\n    t=1/(8π);\n    a*(x2-b*x1+c*x1-r)^2+s*(1-t)*cos(x1)+s\nend","category":"page"},{"location":"kriging/#Sampling-2","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"n_samples = 50\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = branin.(xys);","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"x, y = -5:10, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> branin((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> branin((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"kriging/#Building-a-surrogate-2","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(xys, zs, lower_bound, upper_bound, p=[1.9, 1.9])","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"kriging/#Optimizing-2","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"With our surrogate we can now search for the minimas of the branin function.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"size(xys)","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"surrogate_optimize(branin, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample(), maxiters=10)","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"size(xys)","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = branin.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"variablefidelity/#Variable-fidelity-Surrogates-1","page":"Variable Fidelity","title":"Variable fidelity Surrogates","text":"","category":"section"},{"location":"variablefidelity/#","page":"Variable Fidelity","title":"Variable Fidelity","text":"With the variable fidelity surrogate, we can specify two different surrogates: one for high fidelity data and one for low fidelity data. By default, the first half samples are considered high fidelity and the second half low fidelity.","category":"page"},{"location":"variablefidelity/#","page":"Variable Fidelity","title":"Variable Fidelity","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"variablefidelity/#","page":"Variable Fidelity","title":"Variable Fidelity","text":"n = 20\nlower_bound = 1.0\nupper_bound = 6.0\nx = sample(n,lower_bound,upper_bound,SobolSample())\nf = x -> 1/3*x\ny = f.(x)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"variablefidelity/#","page":"Variable Fidelity","title":"Variable Fidelity","text":"varfid = VariableFidelitySurrogate(x,y,lower_bound,upper_bound)","category":"page"},{"location":"variablefidelity/#","page":"Variable Fidelity","title":"Variable Fidelity","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(varfid, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"cantilever/#Cantilever-beam-function-1","page":"Cantilever beam","title":"Cantilever beam function","text":"","category":"section"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"The Cantilever Beam function is defined as: f(wt) = frac4L^3Ewt*sqrt (fracYt^2)^2 + (fracXw^2)^2  With parameters L,E,X and Y given.","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"Define the objective function:","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"function f(x)\n    t = x[1]\n    w = x[2]\n    L = 100.0\n    E = 2.770674127819261e7\n    X = 530.8038576066307\n    Y = 997.8714938733949\n    return (4*L^3)/(E*w*t)*sqrt( (Y/t^2)^2 + (X/w^2)^2)\nend","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"Let's plot it:","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"n = 100\nlb = [1.0,1.0]\nub = [8.0,8.0]\nxys = sample(n,lb,ub,SobolSample());\nzs = f.(xys);\nx, y = 0:8, 0:8\np1 = surface(x, y, (x1,x2) -> f((x1,x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> f((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"Fitting different Surrogates:","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"mypoly = PolynomialChaosSurrogate(xys, zs,  lb, ub)\nloba = PolynomialChaosSurrogate(xys, zs,  lb, ub)\nrad = RadialBasis(xys,zs,lb,ub)","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"Plotting:","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Polynomial expansion\")","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Lobachevsky\")","category":"page"},{"location":"cantilever/#","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> rad([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> rad([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Inverse distance surrogate\")","category":"page"},{"location":"LinearSurrogate/#Linear-Surrogate-1","page":"Linear","title":"Linear Surrogate","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"Linear Surrogate is a linear approach to modeling the relationship between a scalar response or dependent variable and one or more explanatory variables. We will use Linear Surrogate to optimize following function:","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"f(x) = sin(x) + log(x)","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":".","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"First of all we have to import these two packages: Surrogates and Plots.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"LinearSurrogate/#Sampling-1","page":"Linear","title":"Sampling","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"We choose to sample f in 20 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"f(x) = sin(x) + log(x)\r\nn_samples = 20\r\nlower_bound = 5.2\r\nupper_bound = 12.5\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny = f.(x)\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\r\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Building-a-Surrogate-1","page":"Linear","title":"Building a Surrogate","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"With our sampled points we can build the Linear Surrogate using the LinearSurrogate function.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"We can simply calculate linear_surrogate for any value.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"my_linear_surr_1D = LinearSurrogate(x, y, lower_bound, upper_bound)\r\nadd_point!(my_linear_surr_1D,4.0,7.2)\r\nadd_point!(my_linear_surr_1D,[5.0,6.0],[8.3,9.7])\r\nval = my_linear_surr_1D(5.0)","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"Now, we will simply plot linear_surrogate:","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\r\nplot!(my_linear_surr_1D, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Optimizing-1","page":"Linear","title":"Optimizing","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, my_linear_surr_1D, SobolSample())\r\nscatter(x, y, label=\"Sampled points\")\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\r\nplot!(my_linear_surr_1D, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Linear-Surrogate-tutorial-(ND)-1","page":"Linear","title":"Linear Surrogate tutorial (ND)","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"First of all we will define the Egg Holder function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide\r\n\r\nfunction egg(x)\r\n    x1=x[1]\r\n    x2=x[2]\r\n    term1 = -(x2+47) * sin(sqrt(abs(x2+x1/2+47)));\r\n    term2 = -x1 * sin(sqrt(abs(x1-(x2+47))));\r\n    y = term1 + term2;\r\nend","category":"page"},{"location":"LinearSurrogate/#Sampling-2","page":"Linear","title":"Sampling","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -10, 5, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"n_samples = 50\r\nlower_bound = [-10.0, 0.0]\r\nupper_bound = [5.0, 15.0]\r\n\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = egg.(xys);","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"x, y = -10:5, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> egg((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> egg((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"LinearSurrogate/#Building-a-surrogate-1","page":"Linear","title":"Building a surrogate","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"my_linear_ND = LinearSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"p1 = surface(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"LinearSurrogate/#Optimizing-2","page":"Linear","title":"Optimizing","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"With our surrogate we can now search for the minimas of the function.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"size(xys)","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"surrogate_optimize(egg, SRBF(), lower_bound, upper_bound, my_linear_ND, SobolSample(), maxiters=10)","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"size(xys)","category":"page"},{"location":"LinearSurrogate/#","page":"Linear","title":"Linear","text":"p1 = surface(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nzs = egg.(xys) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2) # hide","category":"page"},{"location":"optimizations/#Optimization-techniques-1","page":"Optimization","title":"Optimization techniques","text":"","category":"section"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"SRBF","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::SRBF,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,SRBF,Any,Any,AbstractSurrogate,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"The main idea is to pick the new evaluations from a set of candidate points where each candidate point is generated as an N(0, sigma^2) distributed perturbation from the current best solution. The value of sigma is modified based on progress and follows the same logic as in many trust region methods: we increase sigma if we make a lot of progress (the surrogate is accurate) and decrease sigma when we aren’t able to make progress (the surrogate model is inaccurate). More details about how sigma is updated is given in the original papers.\n\nAfter generating the candidate points, we predict their objective function value and compute the minimum distance to the previously evaluated point. Let the candidate points be denoted by C and let the function value predictions be s(x_i) and the distance values be d(x_i), both rescaled through a linear transformation to the interval [0,1]. This is done to put the values on the same scale. The next point selected for evaluation is the candidate point x that minimizes the weighted-distance merit function:\n\nmerit(x) = ws(x) + (1-w)(1-d(x))\n\nwhere 0 leq w leq 1. That is, we want a small function value prediction and a large minimum distance from the previously evaluated points. The weight w is commonly cycled between a few values to achieve both exploitation and exploration. When w is close to zero, we do pure exploration, while w close to 1 corresponds to exploitation.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"LCBS","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::LCBS,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,LCBS,Any,Any,Any,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Lower Confidence Bound (LCB), a popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to minimize:\n\nLCB(x) = Ex - k * sqrt(Vx)\n\ndefault value k = 2.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"EI","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::EI,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,EI,Any,Any,Any,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Expected Improvement (EI), arguably the most popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to maximize expected improvement:\n\nEI(x) = Emax(f_best-f(x)0)\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"DYCORS","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::DYCORS,lb,ub,surrn::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,DYCORS,Any,Any,AbstractSurrogate,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"  surrogate_optimize(obj::Function,::DYCORS,lb::Number,ub::Number,surr1::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)\n\nThis is an implementation of the DYCORS strategy by Regis and Shoemaker: Rommel G Regis and Christine A Shoemaker. Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization. Engineering Optimization, 45(5): 529–555, 2013. This is an extension of the SRBF strategy that changes how the candidate points are generated. The main idea is that many objective functions depend only on a few directions so it may be advantageous to perturb only a few directions. In particular, we use a perturbation probability to perturb a given coordinate and decrease this probability after each function evaluation so fewer coordinates are perturbed later in the optimization.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"SOP","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,sop1::SOP,lb::Number,ub::Number,surrSOP::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=min(500*1,5000))","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,SOP,Number,Number,AbstractSurrogate,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"surrogateoptimize(obj::Function,::SOP,lb::Number,ub::Number,surr::AbstractSurrogate,sampletype::SamplingAlgorithm;maxiters=100,numnewsamples=100)\n\nSOP Surrogate optimization method, following closely the following papers:\n\n- SOP: parallel surrogate global optimization with Pareto center selection for computationally expensive single objective problems by Tipaluck Krityakierne\n- Multiobjective Optimization Using Evolutionary Algorithms by Kalyan Deb\n\n#Suggested number of new_samples = min(500*d,5000)\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#Adding-another-optimization-method-1","page":"Optimization","title":"Adding another optimization method","text":"","category":"section"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"To add another optimization method, you just need to define a new SurrogateOptimizationAlgorithm and write its corresponding algorithm, overloading the following:","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::NewOptimizatonType,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"rosenbrock/#Rosenbrock-function-1","page":"Rosenbrock","title":"Rosenbrock function","text":"","category":"section"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"The Rosenbrock function is defined as: f(x) = sum_i=1^d-1 (x_i+1-x_i)^2 + (x_i - 1)^2","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"I will treat the 2D version, which is commonly defined as: f(xy) = (1-x)^2 + 100(y-x^2)^2 Let's import Surrogates and Plots:","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"Define the objective function:","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"function f(x)\n    x1 = x[1]\n    x2 = x[2]\n    return (1-x1)^2 + 100*(x2-x1^2)^2\nend","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"Let's plot it:","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"n = 100\nlb = [0.0,0.0]\nub = [8.0,8.0]\nxys = sample(n,lb,ub,SobolSample());\nzs = f.(xys);\nx, y = 0:8, 0:8\np1 = surface(x, y, (x1,x2) -> f((x1,x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> f((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"Fitting different Surrogates:","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"mypoly = PolynomialChaosSurrogate(xys, zs,  lb, ub)\nloba = PolynomialChaosSurrogate(xys, zs,  lb, ub)\ninver = InverseDistanceSurrogate(xys, zs,  lb, ub)","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"Plotting:","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Polynomial expansion\")","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Lobachevsky\")","category":"page"},{"location":"rosenbrock/#","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> inver([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> inver([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Inverse distance surrogate\")","category":"page"},{"location":"secondorderpoly/#Second-order-polynomial-tutorial-1","page":"SecondOrderPolynomial","title":"Second order polynomial tutorial","text":"","category":"section"},{"location":"secondorderpoly/#","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"The square polynomial model can be expressed by: y = Xβ + ϵ Where X is the matrix of the linear model augmented by adding 2d columns, containing pair by pair product of variables and variables squared.","category":"page"},{"location":"secondorderpoly/#","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"secondorderpoly/#Sampling-1","page":"SecondOrderPolynomial","title":"Sampling","text":"","category":"section"},{"location":"secondorderpoly/#","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"f = x -> 3*sin(x) + 10/x\nlb = 3.0\nub = 6.0\nn = 10\nx = sample(n,lb,ub,LowDiscrepancySample(2))\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"secondorderpoly/#Building-the-surrogate-1","page":"SecondOrderPolynomial","title":"Building the surrogate","text":"","category":"section"},{"location":"secondorderpoly/#","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"sec = SecondOrderPolynomialSurrogate(x, y, lb, ub)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub))\nplot!(f, label=\"True function\",  xlims=(lb, ub))\nplot!(sec, label=\"Surrogate function\",  xlims=(lb, ub))","category":"page"},{"location":"secondorderpoly/#Optimizing-1","page":"SecondOrderPolynomial","title":"Optimizing","text":"","category":"section"},{"location":"secondorderpoly/#","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"@show surrogate_optimize(f, SRBF(), lb, ub, sec, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lb, ub))\nplot!(sec, label=\"Surrogate function\",  xlims=(lb, ub))","category":"page"},{"location":"secondorderpoly/#","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"The optimization method successfully found the minima.","category":"page"},{"location":"Salustowicz/#Salustowicz-Benchmark-Function-1","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark Function","text":"","category":"section"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"The true underlying function HyGP had to approximate is the 1D Salustowicz function. The function can be evaluated in the given domain: x in 0 10.","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"The Salustowicz benchmark function is as follows:","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"f(x) = e^(-x) x^3 cos(x) sin(x) (cos(x) sin^2(x) - 1)","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Let's import these two packages  Surrogates and Plots:","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Now, let's define our objective function:","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"function salustowicz(x)\r\n    term1 = 2.72^(-x) * x^3 * cos(x) * sin(x);\r\n    term2 = (cos(x) * sin(x)*sin(x) - 1);\r\n    y = term1 * term2;\r\nend","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Let's sample f in 30 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol Sample, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"n_samples = 30\r\nlower_bound = 0\r\nupper_bound = 10\r\nnum_round = 2\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny = salustowicz.(x)\r\nxs = lower_bound:0.001:upper_bound\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Now, let's fit Salustowicz Function with different Surrogates:","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"InverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\r\nlobachevsky_surrogate = LobachevskySurrogate(x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:topright)\r\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:topright)\r\nplot!(xs, InverseDistance.(xs), label=\"InverseDistanceSurrogate\", legend=:topright)\r\nplot!(xs, lobachevsky_surrogate.(xs), label=\"Lobachevsky\", legend=:topright)","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Not's let's see Kriging Surrogate with different hyper parameter:","category":"page"},{"location":"Salustowicz/#","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"kriging_surrogate1 = Kriging(x, y, lower_bound, upper_bound, p=0.9);\r\nkriging_surrogate2 = Kriging(x, y, lower_bound, upper_bound, p=1.5);\r\nkriging_surrogate3 = Kriging(x, y, lower_bound, upper_bound, p=1.9);\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:topright)\r\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:topright)\r\nplot!(xs, kriging_surrogate1.(xs), label=\"kriging_surrogate1\", ribbon=p->std_error_at_point(kriging_surrogate1, p), legend=:topright)\r\nplot!(xs, kriging_surrogate2.(xs), label=\"kriging_surrogate2\", ribbon=p->std_error_at_point(kriging_surrogate2, p), legend=:topright)\r\nplot!(xs, kriging_surrogate3.(xs), label=\"kriging_surrogate3\", ribbon=p->std_error_at_point(kriging_surrogate3, p), legend=:topright)","category":"page"},{"location":"surrogate/#Surrogate-1","page":"Surrogates","title":"Surrogate","text":"","category":"section"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Every surrogate has a different definition depending on the parameters needed. However, they have in common:","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"add_point!(::AbstractSurrogate,x_new,y_new)\nAbstractSurrogate(value)","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"The first function adds a sample point to the surrogate, thus changing the internal coefficients. The second one calculates the approximation at value.","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Linear surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"LinearSurrogate(x,y,lb,ub)","category":"page"},{"location":"surrogate/#Surrogates.LinearSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.LinearSurrogate","text":"LinearSurrogate(x,y,lb,ub)\n\nBuilds a linear surrogate using GLM.jl\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Radial basis function surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"RadialBasis(x, y, lb, ub; rad::RadialFunction = linearRadial, scale_factor::Real=1.0, sparse = false)","category":"page"},{"location":"surrogate/#Surrogates.RadialBasis-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.RadialBasis","text":"RadialBasis(x,y,lb,ub,rad::RadialFunction, scale_factor::Float = 1.0)\n\nConstructor for RadialBasis surrogate\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Kriging surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Kriging(x,y,p,theta)","category":"page"},{"location":"surrogate/#Surrogates.Kriging-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.Kriging","text":"Kriging(x,y,lb,ub;p=collect(one.(x[1])),theta=collect(one.(x[1])))\n\nConstructor for Kriging surrogate.\n\n(x,y): sampled points\np: array of values 0<=p<2 modeling the    smoothness of the function being approximated in the i-th variable.    low p -> rough, high p -> smooth\ntheta: array of values > 0 modeling how much the function is         changing in the i-th variable.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Lobachevsky surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"LobachevskySurrogate(x,y,lb,ub; alpha = collect(one.(x[1])),n::Int = 4, sparse = false)\nlobachevsky_integral(loba::LobachevskySurrogate,lb,ub)","category":"page"},{"location":"surrogate/#Surrogates.LobachevskySurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.LobachevskySurrogate","text":"LobachevskySurrogate(x,y,alpha,n::Int,lb,ub,sparse = false)\n\nBuild the Lobachevsky surrogate with parameters alpha and n.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Surrogates.lobachevsky_integral-Tuple{LobachevskySurrogate,Any,Any}","page":"Surrogates","title":"Surrogates.lobachevsky_integral","text":"lobachevsky_integral(loba::LobachevskySurrogate,lb,ub)\n\nCalculates the integral of the Lobachevsky surrogate, which has a closed form.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Support vector machine surrogate, requires using LIBSVM","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"SVMSurrogate(x,y,lb::Number,ub::Number)","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Random forest surrogate, requires using XGBoost","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"RandomForestSurrogate(x,y,lb,ub;num_round::Int = 1)","category":"page"},{"location":"surrogate/#Surrogates.RandomForestSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.RandomForestSurrogate","text":"RandomForestSurrogate(x,y,lb,ub,num_round)\n\nBuild Random forest surrogate. num_round is the number of trees.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Neural network surrogate, requires using Flux","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"NeuralSurrogate(x,y,lb,ub; model = Chain(Dense(length(x[1]),1), first), loss = (x,y) -> Flux.mse(model(x), y),opt = Descent(0.01),n_echos::Int = 1)","category":"page"},{"location":"surrogate/#Surrogates.NeuralSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.NeuralSurrogate","text":"NeuralSurrogate(x,y,lb,ub,model,loss,opt,n_echos)\n\nmodel: Flux layers\nloss: loss function\nopt: optimization function\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Creating-another-surrogate-1","page":"Surrogates","title":"Creating another surrogate","text":"","category":"section"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"It's great that you want to add another surrogate to the library! You will need to:","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Define a new mutable struct and a constructor function\nDefine add_point!(your_surrogate::AbstactSurrogate,x_new,y_new)\nDefine your_surrogate(value) for the approximation","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Example","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"mutable struct NewSurrogate{X,Y,L,U,C,A,B} <: AbstractSurrogate\n  x::X\n  y::Y\n  lb::L\n  ub::U\n  coeff::C\n  alpha::A\n  beta::B\nend\n\nfunction NewSurrogate(x,y,lb,ub,parameters)\n    ...\n    return NewSurrogate(x,y,lb,ub,calculated\\_coeff,alpha,beta)\nend\n\nfunction add_point!(NewSurrogate,x\\_new,y\\_new)\n\n  nothing\nend\n\nfunction (s::NewSurrogate)(value)\n  return s.coeff*value + s.alpha\nend","category":"page"},{"location":"gramacylee/#Gramacy-and-Lee-Function-1","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"","category":"section"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Gramacy & Lee Function is a continues function. It is not convex. The function is defined on 1-dimensional space. It is an unimodal. The function can be defined on any input domain but it is usually evaluated on x in -05 25.","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"The Gramacy & Lee is as follows: f(x) = fracsin(10pi x)2x + (x-1)^4.","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Let's import these two packages Surrogates and Plots:","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Now, let's define our objective function:","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"function gramacylee(x)\n    term1 = sin(10*pi*x) / 2*x;\n    term2 = (x - 1)^4;\n    y = term1 + term2;\nend","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Let's sample f in 25 points between -0.5 and 2.5 using the sample function. The sampling points are chosen using a Sobol Sample, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"n = 25\nlower_bound = -0.5\nupper_bound = 2.5\nx = sample(n, lower_bound, upper_bound, SobolSample())\ny = gramacylee.(x)\nxs = lower_bound:0.001:upper_bound\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-5, 20), legend=:top)\nplot!(xs, gramacylee.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Now, let's fit Gramacy & Lee Function with different Surrogates:","category":"page"},{"location":"gramacylee/#","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"my_pol = PolynomialChaosSurrogate(x, y, lower_bound, upper_bound)\nloba_1 = LobachevskySurrogate(x, y, lower_bound, upper_bound)\nkrig = Kriging(x, y, lower_bound, upper_bound)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-5, 20), legend=:top)\nplot!(xs, gramacylee.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_pol.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"water_flow/#Water-flow-function-1","page":"Water Flow function","title":"Water flow function","text":"","category":"section"},{"location":"water_flow/#","page":"Water Flow function","title":"Water Flow function","text":"The water flow function is defined as: f(r_wrT_uH_uT_lH_lLK_w) = frac2*pi*T_u(H_u - H_l)log(fracrr_w)*1 + frac2LT_ulog(fracrr_w)*r_w^2*K_w+ fracT_uT_l ","category":"page"},{"location":"water_flow/#","page":"Water Flow function","title":"Water Flow function","text":"It has 8 dimension.","category":"page"},{"location":"water_flow/#","page":"Water Flow function","title":"Water Flow function","text":"using Surrogates\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"water_flow/#","page":"Water Flow function","title":"Water Flow function","text":"Define the objective function:","category":"page"},{"location":"water_flow/#","page":"Water Flow function","title":"Water Flow function","text":"function f(x)\n    r_w = x[1]\n    r = x[2]\n    T_u = x[3]\n    H_u = x[4]\n    T_l = x[5]\n    H_l = x[6]\n    L = x[7]\n    K_w = x[8]\n    log_val = log(r/r_w)\n    return (2*pi*T_u*(H_u - H_l))/ ( log_val*(1 + (2*L*T_u/(log_val*r_w^2*K_w)) + T_u/T_l))\nend","category":"page"},{"location":"water_flow/#","page":"Water Flow function","title":"Water Flow function","text":"n = 180\nd = 8\nlb = [0.05,100,63070,990,63.1,700,1120,9855]\nub = [0.15,50000,115600,1110,116,820,1680,12045]\nx = sample(n,lb,ub,SobolSample())\ny = f.(x)\nn_test = 1000\nx_test = sample(n_test,lb,ub,GoldenSample());\ny_true = f.(x_test);","category":"page"},{"location":"water_flow/#","page":"Water Flow function","title":"Water Flow function","text":"my_rad = RadialBasis(x,y,lb,ub)\ny_rad = my_rad.(x_test)\nmy_poly = PolynomialChaosSurrogate(x,y,lb,ub)\ny_poli = my_poli.(x_test)\nmse_rad = norm(y_true - y_rad,2)/n_test\nmse_poli = norm(y_true - y_poli,2)/n_test\nprint(\"MSE Radial: $mse_rad\")\nprint(\"MSE Radial: $mse_poli\")","category":"page"},{"location":"radials/#Radial-Surrogates-1","page":"Radials","title":"Radial Surrogates","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"The Radial Basis Surrogate model represents the interpolating function as a linear combination of basis functions, one for each training point. Let's start with something easy to get our hands dirty. I want to build a surrogate for:","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"f(x) = log(x)*x^2+x^3`","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Let's choose the Radial Basis Surrogate for 1D. First of all we have to import these two packages: Surrogates and Plots,","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"We choose to sample f in 30 points between 5 to 25 using sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"f(x) = log(x)*x^2 + x^3\r\nn_samples = 30\r\nlower_bound = 5\r\nupper_bound = 25\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny = f.(x)\r\nscatter(x, y, label=\"Sampled Points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\", scatter(x, y, label=\"Sampled Points\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"radials/#Building-Surrogate-1","page":"Radials","title":"Building Surrogate","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"With our sampled points we can build the Radial Surrogate using the RadialBasis function.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"We can simply calculate radial_surrogate for any value.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"radial_surrogate = RadialBasis(x, y, lower_bound, upper_bound)\r\nval = radial_surrogate(5.4)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Now, we will simply plot radial_surrogate:","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(radial_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"radials/#Optimizing-1","page":"Radials","title":"Optimizing","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, radial_surrogate, SobolSample())\r\nscatter(x, y, label=\"Sampled points\", legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(radial_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"radials/#Radial-Basis-Surrogate-tutorial-(ND)-1","page":"Radials","title":"Radial Basis Surrogate tutorial (ND)","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"First of all we will define the Booth function we are going to build surrogate for:","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"f(x) = (x_1 + 2*x_2 - 7)^2 + (2*x_1 + x_2 - 5)^2","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide\r\n\r\nfunction booth(x)\r\n    x1=x[1]\r\n    x2=x[2]\r\n    term1 = (x1 + 2*x2 - 7)^2;\r\n    term2 = (2*x1 + x2 - 5)^2;\r\n    y = term1 + term2;\r\nend","category":"page"},{"location":"radials/#Sampling-1","page":"Radials","title":"Sampling","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 80 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"n_samples = 80\r\nlower_bound = [-5.0, 0.0]\r\nupper_bound = [10.0, 15.0]\r\n\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = booth.(xys);","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"x, y = -5:10, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> booth((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> booth((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"radials/#Building-a-surrogate-1","page":"Radials","title":"Building a surrogate","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"radial_basis = RadialBasis(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"p1 = surface(x, y, (x, y) -> radial_basis([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> radial_basis([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"radials/#Optimizing-2","page":"Radials","title":"Optimizing","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"With our surrogate we can now search for the minimas of the function.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"size(xys)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"surrogate_optimize(booth, SRBF(), lower_bound, upper_bound, radial_basis, UniformSample(), maxiters=50)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"size(xys)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"p1 = surface(x, y, (x, y) -> radial_basis([x y])) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nzs = booth.(xys) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> radial_basis([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2) # hide","category":"page"},{"location":"multi_objective_opt/#Multi-objective-optimization-benchmarks-1","page":"Multi objective optimization","title":"Multi objective optimization benchmarks","text":"","category":"section"},{"location":"multi_objective_opt/#Case-1:-Non-colliding-objective-functions-1","page":"Multi objective optimization","title":"Case 1: Non colliding objective functions","text":"","category":"section"},{"location":"multi_objective_opt/#","page":"Multi objective optimization","title":"Multi objective optimization","text":"using Surrogates\n#EGO\nm = 10\nf  = x -> [x^i for i = 1:m]\nlb = 1.0\nub = 10.0\nx  = sample(100, lb, ub, SobolSample())\ny  = f.(x)\nmy_radial_basis_ego = RadialBasis(x, y, lb, ub, rad = linearRadial)\npareto_set, pareto_front = surrogate_optimize(f,EGO(),lb,ub,my_radial_basis_ego,SobolSample())\n\nm = 5\nf  = x -> [x^i for i =1:m]\nlb = 1.0\nub = 10.0\nx  = sample(100, lb, ub, SobolSample())\ny  = f.(x)\nmy_radial_basis_rtea = RadialBasis(x, y, lb, ub, rad = linearRadial)\nZ = 0.8\nK = 2\np_cross = 0.5\nn_c = 1.0\nsigma = 1.5\nsurrogate_optimize(f,RTEA(Z,K,p_cross,n_c,sigma),lb,ub,my_radial_basis_rtea,SobolSample())","category":"page"},{"location":"multi_objective_opt/#Case-2:-objective-functions-with-conflicting-minima-1","page":"Multi objective optimization","title":"Case 2: objective functions with conflicting minima","text":"","category":"section"},{"location":"multi_objective_opt/#","page":"Multi objective optimization","title":"Multi objective optimization","text":"#EGO\nf  = x -> [sqrt((x[1] - 4)^2 + 25*(x[2])^2),\n           sqrt((x[1]+4)^2 + 25*(x[2])^2),\n           sqrt((x[1]-3)^2 + (x[2]-1)^2)]\nlb = [2.5,-0.5]\nub = [3.5,0.5]\nx  = sample(100, lb, ub, SobolSample())\ny  = f.(x)\nmy_radial_basis_ego = RadialBasis(x, y, lb, ub, rad = linearRadial)\n#I can find my pareto set and pareto front by calling again the surrogate_optimize function:\npareto_set, pareto_front = surrogate_optimize(f,EGO(),lb,ub,my_radial_basis_ego,SobolSample(),maxiters=30);","category":"page"},{"location":"BraninFunction/#Branin-Function-1","page":"Branin function","title":"Branin Function","text":"","category":"section"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"The Branin Function is commonly used as a test function for metamodelling in computer experiments, especially in the context of optimization.","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"The expression of the Branin Function is given as: f(x) = (x_2 - frac514pi^2x_1^2 + frac5pix_1 - 6)^2 + 10(1-frac18pi)cos(x_1) + 10","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"where x = (x_1 x_2) with -5leq x_1 leq 10 0 leq x_2 leq 15","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"First of all we will import these two packages Surrogates and Plots.","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"Now, let's define our objective function:","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"function branin(x)\r\n      x1 = x[1]\r\n      x2 = x[2]\r\n      b = 5.1 / (4*pi^2);\r\n      c = 5/pi;\r\n      r = 6;\r\n      a = 1;\r\n      s = 10;\r\n      t = 1 / (8*pi);\r\n      term1 = a * (x2 - b*x1^2 + c*x1 - r)^2;\r\n      term2 = s*(1-t)*cos(x1);\r\n      y = term1 + term2 + s;\r\nend","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"Now, let's plot it:","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"n_samples = 80\r\nlower_bound = [-5, 0]\r\nupper_bound = [10,15]\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = branin.(xys);\r\nx, y = -5:10, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> branin((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> branin((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"Now it's time to fitting different surrogates and then we will plot them. We will have a look on Kriging Surrogate:","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"kriging_surrogate = Kriging(xys, zs, lower_bound, upper_bound, p=[1.9, 1.9])","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Kriging Surrogate\") # hide","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"Now, we will have a look on Inverse Distance Surrogate:","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"InverseDistance = InverseDistanceSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Inverse Distance Surrogate\") # hide","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"Now, let's talk about Lobachevsky Surrogate:","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"Lobachevsky = LobachevskySurrogate(xys, zs,  lower_bound, upper_bound, alpha = [2.8,2.8], n=8)","category":"page"},{"location":"BraninFunction/#","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Lobachevsky Surrogate\") # hide","category":"page"},{"location":"gek/#Gradient-Enhanced-Kriging-1","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"","category":"section"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Gradient-enhanced Kriging is an extension of kriging which supports gradient information. GEK is usually more accurate than kriging, however, it is not computationally efficient when the number of inputs, the number of sampling points, or both, are high. This is mainly due to the size of the corresponding correlation matrix that increases proportionally with both the number of inputs and the number of sampling points.","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Let's have a look to the following function to use Gradient Enhanced Surrogate: f(x) = sin(x) + 2*x^2","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"First of all, we will import Surrogates and Plots packages:","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"gek/#Sampling-1","page":"Gradient Enhanced Kriging","title":"Sampling","text":"","category":"section"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"We choose to sample f in 8 points between 0 to 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"n_samples = 10\r\nlower_bound = 2\r\nupper_bound = 10\r\nxs = lower_bound:0.001:upper_bound\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nf(x) = x^3 - 6x^2 + 4x + 12\r\ny1 = f.(x)\r\nder = x -> 3*x^2 - 12*x + 4\r\ny2 = der.(x)\r\ny = vcat(y1,y2)\r\nscatter(x, y1, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"gek/#Building-a-surrogate-1","page":"Gradient Enhanced Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"With our sampled points we can build the Gradient Enhanced Kriging surrogate using the GEK function.","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"my_gek = GEK(x, y, lower_bound, upper_bound, p = 1.4);","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"scatter(x, y1, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(my_gek, label=\"Surrogate function\", ribbon=p->std_error_at_point(my_gek, p), xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"gek/#Gradient-Enhanced-Kriging-Surrogate-Tutorial-(ND)-1","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging Surrogate Tutorial (ND)","text":"","category":"section"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"First of all let's define the function we are going to build a surrogate for.","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Now, let's define the function:","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"function leon(x)\r\n      x1 = x[1]\r\n      x2 = x[2]\r\n      term1 = 100*(x2 - x1^3)^2\r\n      term2 = (1 - x1)^2\r\n      y = term1 + term2\r\nend","category":"page"},{"location":"gek/#Sampling-2","page":"Gradient Enhanced Kriging","title":"Sampling","text":"","category":"section"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 10, and 0, 10 for the second dimension. We are taking 80 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"n_samples = 80\r\nlower_bound = [0, 0]\r\nupper_bound = [10, 10]\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny1 = leon.(xys);","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"x, y = 0:10, 0:10 # hide\r\np1 = surface(x, y, (x1,x2) -> leon((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, y1) # hide\r\np2 = contour(x, y, (x1,x2) -> leon((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"gek/#Building-a-surrogate-2","page":"Gradient Enhanced Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"grad1 = x1 -> 2*(300*(x[1])^5 - 300*(x[1])^2*x[2] + x[1] -1)\r\ngrad2 = x2 -> 200*(x[2] - (x[1])^3)\r\nd = 2\r\nn = 10\r\nfunction create_grads(n, d, grad1, grad2, y)\r\n      c = 0\r\n      y2 = zeros(eltype(y[1]),n*d)\r\n      for i in 1:n\r\n            y2[i + c] = grad1(x[i])\r\n            y2[i + c + 1] = grad2(x[i])\r\n            c = c + 1\r\n      end\r\n      return y2\r\nend\r\ny2 = create_grads(n, d, grad2, grad2, y)\r\ny = vcat(y1,y2)","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"my_GEK = GEK(xys, y, lower_bound, upper_bound, p=[1.9, 1.9])","category":"page"},{"location":"gek/#","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"p1 = surface(x, y, (x, y) -> my_GEK([x y])) # hide\r\nscatter!(xs, ys, y1, marker_z=y1) # hide\r\np2 = contour(x, y, (x, y) -> my_GEK([x y])) # hide\r\nscatter!(xs, ys, marker_z=y1) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"lp/#Lp-norm-function-1","page":"Lp norm","title":"Lp norm function","text":"","category":"section"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"The Lp norm function is defined as: f(x) = sqrtp sum_i=1^d vert x_i vert ^p","category":"page"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"using Surrogates\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"Define the objective function:","category":"page"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"function f(x,p)\n    return norm(x,p)\nend","category":"page"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"Let's see a simple 1D case:","category":"page"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"n = 30\nlb = -5.0\nub = 5.0\np = 1.3\nx = sample(n,lb,ub,SobolSample())\ny = f.(x,p)\nxs = lb:0.001:ub\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 5), legend=:top)\nplot!(xs,f.(xs,p), label=\"True function\", legend=:top)","category":"page"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"Fitting different Surrogates:","category":"page"},{"location":"lp/#","page":"Lp norm","title":"Lp norm","text":"my_pol = PolynomialChaosSurrogate(x,y,lb,ub)\nloba_1 = LobachevskySurrogate(x,y,lb,ub)\nkrig = Kriging(x,y,lb,ub)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 5), legend=:top)\nplot!(xs,f.(xs,p), label=\"True function\", legend=:top)\nplot!(xs, my_pol.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"(Image: SurrogatesLogo)","category":"page"},{"location":"#Overview-1","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"A surrogate model is an approximation method that mimics the behavior of a computationally expensive simulation. In more mathematical terms: suppose we are attempting to optimize a function  f(p), but each calculation of  f is very expensive. It may be the case that we need to solve a PDE for each point or use advanced numerical linear algebra machinery, which is usually costly. The idea is then to develop a surrogate model  g which approximates  f by training on previous data collected from evaluations of  f. The construction of a surrogate model can be seen as a three-step process:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Sample selection\nConstruction of the surrogate model\nSurrogate optimization","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The sampling methods are super important for the behavior of the Surrogate. At the moment they are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Grid sample\nUniform sample\nSobol sample\nLatin Hypercube sample\nLow discrepancy sample","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The available surrogates are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Linear\nRadial Basis\nKriging\nCustom Kriging provided with Stheno\nNeural Network\nSupport Vector Machine\nRandom Forest\nSecond Order Polynomial\nInverse Distance","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"After the surrogate is built, we need to optimize it with respect to some objective function. That is, simultaneously looking for a minimum and sampling the most unknown region. The available optimization methods are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Stochastic RBF (SRBF)\nLower confidence bound strategy (LCBS)\nExpected improvement (EI)\nDynamic coordinate search (DYCORS)","category":"page"},{"location":"#Multi-output-Surrogates-1","page":"Overview","title":"Multi-output Surrogates","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"In certain situations, the function being modeled may have a multi-dimensional output space. In such a case, the surrogate models can take advantage of correlations between the observed output variables to obtain more accurate predictions.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"When constructing the original surrogate, each element of the passed y vector should itself be a vector. For example, the following y are all valid.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using Surrogates\nusing StaticArrays\n\nx = sample(5, [0.0; 0.0], [1.0; 1.0], SobolSample())\nf_static = (x) -> StaticVector(x[1], log(x[2]*x[1]))\nf = (x) -> [x, log(x)/2]\n\ny = f_static.(x)\ny = f.(x)","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Currently, the following are implemented as multi-output surrogates:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Radial Basis\nNeural Network (via Flux)\nSecond Order Polynomial\nInverse Distance\nCustom Kriging (via Stheno)","category":"page"},{"location":"#Gradients-1","page":"Overview","title":"Gradients","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"The surrogates implemented here are all automatically differentiable via Zygote. Because of this property, surrogates are useful models for processes which aren't explicitly differentiable, and can be used as layers in, for instance, Flux models.","category":"page"},{"location":"#Installation-1","page":"Overview","title":"Installation","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"Surrogates is registered in the Julia General Registry. In the REPL:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"]add Surrogates","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"You can obtain the current master with:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"]add https://github.com/JuliaDiffEq/Surrogates.jl#master","category":"page"},{"location":"#Quick-example-1","page":"Overview","title":"Quick example","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"using Surrogates\nnum_samples = 10\nlb = 0.0\nub = 10.0\n\n#Sampling\nx = sample(num_samples,lb,ub,SobolSample())\nf = x-> log(x)*x^2+x^3\ny = f.(x)\n\n#Creating surrogate\nalpha = 2.0\nn = 6\nmy_lobachevsky = LobachevskySurrogate(x,y,lb,ub,alpha=alpha,n=n)\n\n#Approximating value at 5.0\nvalue = my_lobachevsky(5.0)\n\n#Adding more data points\nsurrogate_optimize(f,SRBF(),lb,ub,my_lobachevsky,UniformSample())\n\n#New approximation\nvalue = my_lobachevsky(5.0)","category":"page"},{"location":"neural/#Neural-network-tutorial-1","page":"NeuralSurrogate","title":"Neural network tutorial","text":"","category":"section"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"It's possible to define a neural network as a surrogate, using Flux. This is useful because we can call optimization methods on it.","category":"page"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"First of all we will define the Schaffer function we are going to build surrogate for.","category":"page"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"using Plots\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates\nusing Flux\n\nfunction schaffer(x)\n    x1=x[1]\n    x2=x[2]\n    fact1 = x1 ^2;\n    fact2 = x2 ^2;\n    y = fact1 + fact2;\nend","category":"page"},{"location":"neural/#Sampling-1","page":"NeuralSurrogate","title":"Sampling","text":"","category":"section"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 8, and 0, 8 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"n_samples = 60\nlower_bound = [0.0, 0.0]\nupper_bound = [8.0, 8.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);","category":"page"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"x, y = 0:8, 0:8 # hide\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"neural/#Building-a-surrogate-1","page":"NeuralSurrogate","title":"Building a surrogate","text":"","category":"section"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"You can specify your own model, optimization function, loss functions and epochs. As always, getting the model right is hardest thing.","category":"page"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"model1 = Chain(\n  Dense(1, 5, σ),\n  Dense(5,2,σ),\n  Dense(2, 1)\n)\nneural = NeuralSurrogate(x, y, lb, ub, model = model1, n_echos = 10)","category":"page"},{"location":"neural/#Optimization-1","page":"NeuralSurrogate","title":"Optimization","text":"","category":"section"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"We can now call an optimization function on the neural network:","category":"page"},{"location":"neural/#","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, neura, SobolSample(), maxiters=20, num_new_samples=10)","category":"page"},{"location":"samples/#Samples-1","page":"Samples","title":"Samples","text":"","category":"section"},{"location":"samples/#","page":"Samples","title":"Samples","text":"The syntax for sampling in an interval or region is the following:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,S::SamplingAlgorithm)","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"where lb and ub are, respectively, the lower and upper bounds. There are many sampling algorithms to choose from:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Grid sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"GridSample{T}\nsample(n,lb,ub,S::GridSample)","category":"page"},{"location":"samples/#Surrogates.GridSample","page":"Samples","title":"Surrogates.GridSample","text":"GridSample{T}\n\nT is the step dx for lb:dx:ub\n\n\n\n\n\n","category":"type"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,GridSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,S::GridSample)\n\nReturns a tuple containing numbers in a grid.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Uniform sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::UniformSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,UniformSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::UniformRandom)\n\nReturns a Tuple containing uniform random numbers.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Sobol sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::SobolSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,SobolSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::SobolSampling)\n\nReturns a Tuple containing Sobol sequences.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Latin Hypercube sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::LatinHypercubeSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,LatinHypercubeSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::LatinHypercube)\n\nReturns a Tuple containing LatinHypercube sequences.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Low Discrepancy sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"LowDiscrepancySample{T}\nsample(n,lb,ub,S::LowDiscrepancySample)","category":"page"},{"location":"samples/#Surrogates.LowDiscrepancySample","page":"Samples","title":"Surrogates.LowDiscrepancySample","text":"LowDiscrepancySample{T}\n\nT is the base for the sequence\n\n\n\n\n\n","category":"type"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,LowDiscrepancySample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,S::LowDiscrepancySample)\n\nLow discrepancy sample:\n\nDimension 1: Van der Corput sequence\nDimension > 1: Halton sequence\n\nIf dimension d > 1, all bases must be coprime with each other.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Sample on section","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"SectionSample\nsample(n,lb,ub,S::SectionSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,SectionSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,d,K::SectionSample)\n\nReturns Tuples constrained to a section.\n\nIn surrogate-based identification and control, optimization can alternate between unconstrained sampling in the full-dimensional parameter space, and sampling constrained on specific sections (e.g. a planes in a 3D volume),\n\nA SectionSampler allows sampling and optimizing on a subset of 'free' dimensions while keeping 'fixed' ones constrained. The sampler is defined as in e.g. \n\nsectionsampleryis10 = SectionSample([NaN64, NaN64, 10.0, 10.0], Surrogates.UniformSample())\n\nwhere the first argument is a Vector{T} in which numbers are fixed coordinates and NaNs correspond to free dimensions, and the second argument is a SamplingAlgorithm which is used to sample in the free dimensions.\n\n\n\n\n\n","category":"method"},{"location":"samples/#Adding-a-new-sampling-method-1","page":"Samples","title":"Adding a new sampling method","text":"","category":"section"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Adding a new sampling method is a two- step process:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Adding a new SamplingAlgorithm type\nOverloading the sample function with the new type.","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Example","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"struct NewAmazingSamplingAlgorithm{OPTIONAL} <: SamplingAlgorithm end\n\nfunction sample(n,lb,ub,::NewAmazingSamplingAlgorithm)\n    if lb is  Number\n        ...\n        return x\n    else\n        ...\n        return Tuple.(x)\n    end\nend","category":"page"}]
}
