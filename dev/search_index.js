var documenterSearchIndex = {"docs":
[{"location":"samples/#Sampling","page":"Samples","title":"Sampling","text":"","category":"section"},{"location":"samples/","page":"Samples","title":"Samples","text":"Sampling methods are provided by the QuasiMonteCarlo package.","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"The syntax for sampling in an interval or region is the following:","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,S::SamplingAlgorithm)","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"where lb and ub are, respectively, the lower and upper bounds. There are many sampling algorithms to choose from:","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"Grid sample","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"GridSample{T}\nsample(n,lb,ub,S::GridSample)","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"Uniform sample","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::RandomSample)","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"Sobol sample","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::SobolSample)","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"Latin Hypercube sample","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::LatinHypercubeSample)","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"Low Discrepancy sample","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,S::HaltonSample)","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"Sample on section","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"SectionSample\nsample(n,lb,ub,S::SectionSample)","category":"page"},{"location":"samples/#Adding-a-new-sampling-method","page":"Samples","title":"Adding a new sampling method","text":"","category":"section"},{"location":"samples/","page":"Samples","title":"Samples","text":"Adding a new sampling method is a two- step process:","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"Adding a new SamplingAlgorithm type\nOverloading the sample function with the new type.","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"Example","category":"page"},{"location":"samples/","page":"Samples","title":"Samples","text":"struct NewAmazingSamplingAlgorithm{OPTIONAL} <: QuasiMonteCarlo.SamplingAlgorithm end\n\nfunction sample(n,lb,ub,::NewAmazingSamplingAlgorithm)\n    if lb is  Number\n        ...\n        return x\n    else\n        ...\n        return Tuple.(x)\n    end\nend","category":"page"},{"location":"tutorials/#Surrogates-101","page":"Basics","title":"Surrogates 101","text":"","category":"section"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"Let's start with something easy to get our hands dirty. I want to build a surrogate for f(x) = log(x) cdot x^2+x^3. Let's choose the radial basis surrogate.","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> log(x)*x^2+x^3\nlb = 1.0\nub = 10.0\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at 5.4\napprox = my_radial_basis(5.4)","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"Let's now see an example in 2D.","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nusing LinearAlgebra\nf = x -> x[1]*x[2]\nlb = [1.0,2.0]\nub = [10.0,8.5]\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at (1.0,1.4)\napprox = my_radial_basis((1.0,1.4))","category":"page"},{"location":"tutorials/#Kriging-standard-error","page":"Basics","title":"Kriging standard error","text":"","category":"section"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"Let's now use the Kriging surrogate, which is a single-output Gaussian process. This surrogate has a nice feature: not only does it approximate the solution at a point, it also calculates the standard error at such point. Let's see an example:","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> exp(x)*x^2+x^3\nlb = 0.0\nub = 10.0\nx = sample(50,lb,ub,RandomSample())\ny = f.(x)\np = 1.9\nmy_krig = Kriging(x,y,lb,ub,p=p)\n\n#I want an approximation at 5.4\napprox = my_krig(5.4)\n\n#I want to find the standard error at 5.4\nstd_err = std_error_at_point(my_krig,5.4)","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"Let's now optimize the Kriging surrogate using Lower confidence bound method, this is just a one-liner:","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"surrogate_optimize(f,LCBS(),lb,ub,my_krig,RandomSample(); maxiters = 10, num_new_samples = 10)","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"Surrogate optimization methods have two purposes: they both sample the space in unknown regions and look for the minima at the same time.","category":"page"},{"location":"tutorials/#Lobachevsky-integral","page":"Basics","title":"Lobachevsky integral","text":"","category":"section"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"The Lobachevsky surrogate has the nice feature of having a closed formula for its integral, which is something that other surrogates are missing. Let's compare it with QuadGK:","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nusing QuadGK\nobj = x -> 3*x + log(x)\na = 1.0\nb = 4.0\nx = sample(2000,a,b,SobolSample())\ny = obj.(x)\nalpha = 2.0\nn = 6\nmy_loba = LobachevskySurrogate(x,y,a,b,alpha=alpha,n=n)\n\n#1D integral\nint_1D = lobachevsky_integral(my_loba,a,b)\nint = quadgk(obj,a,b)\nint_val_true = int[1]-int[2]\nprintln(int_1D)\nprintln(int_val_true)","category":"page"},{"location":"tutorials/#Example-of-NeuralSurrogate","page":"Basics","title":"Example of NeuralSurrogate","text":"","category":"section"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"Basic example of fitting a neural network on a simple function of two variables.","category":"page"},{"location":"tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nusing Flux\nusing Statistics\nusing SurrogatesFlux\n\nf = x -> x[1]^2 + x[2]^2\nbounds = Float32[-1.0, -1.0], Float32[1.0, 1.0]\n# Flux models are in single precision by default.\n# Thus, single precision will also be used here for our training samples.\n\nx_train = sample(100, bounds..., SobolSample())\ny_train = f.(x_train)\n\n# Perceptron with one hidden layer of 20 neurons.\nmodel = Chain(Dense(2, 20, relu), Dense(20, 1))\nloss(x, y) = Flux.mse(model(x), y)\n\n# Training of the neural network\nlearning_rate = 0.1\noptimizer = Descent(learning_rate)  # Simple gradient descent. See Flux documentation for other options.\nn_epochs = 50\nsgt = NeuralSurrogate(x_train, y_train, bounds..., model=model, loss=loss, opt=optimizer, n_echos=n_epochs)\n\n# Testing the new model\nx_test = sample(30, bounds..., SobolSample())\ntest_error = mean(abs2, sgt(x)[1] - f(x) for x in x_test)","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"The Inverse Distance Surrogate is an interpolating method and in this method the unknown points are calculated with a weighted average of the sampling points. This model uses the inverse distance between the unknown and training points to predict the unknown point. We do not need to fit this model because the response of an unknown point x is computed with respect to the distance between x and the training points.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Let's optimize the following function to use Inverse Distance Surrogate:","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"f(x) = sin(x) + sin(x)^2 + sin(x)^3","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":".","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"First of all, we have to import these two packages: Surrogates and Plots.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"InverseDistance/#Sampling","page":"InverseDistance","title":"Sampling","text":"","category":"section"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"We choose to sample f in 25 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy, this can be done by passing HaltonSample() to the sample function.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"f(x) = sin(x) + sin(x)^2 + sin(x)^3\n\nn_samples = 25\nlower_bound = 0.0\nupper_bound = 10.0\nx = sample(n_samples, lower_bound, upper_bound, HaltonSample())\ny = f.(x)\n\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Building-a-Surrogate","page":"InverseDistance","title":"Building a Surrogate","text":"","category":"section"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"InverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\nadd_point!(InverseDistance, 5.0, f(5.0))\nadd_point!(InverseDistance, [5.1,5.2], [f(5.1),f(5.2)])\nprediction = InverseDistance(5.0)","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Now, we will simply plot InverseDistance:","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(InverseDistance, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Optimizing","page":"InverseDistance","title":"Optimizing","text":"","category":"section"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample())\nscatter(x, y, label=\"Sampled points\", legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(InverseDistance, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Inverse-Distance-Surrogate-Tutorial-(ND):","page":"InverseDistance","title":"Inverse Distance Surrogate Tutorial (ND):","text":"","category":"section"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"First of all we will define the Schaffer function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction schaffer(x)\n    x1=x[1]\n    x2=x[2]\n    fact1 = (sin(x1^2-x2^2))^2 - 0.5;\n    fact2 = (1 + 0.001*(x1^2+x2^2))^2;\n    y = 0.5 + fact1/fact2;\nend","category":"page"},{"location":"InverseDistance/#Sampling-2","page":"InverseDistance","title":"Sampling","text":"","category":"section"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"n_samples = 60\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"x, y = -5:10, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"InverseDistance/#Building-a-surrogate","page":"InverseDistance","title":"Building a surrogate","text":"","category":"section"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"InverseDistance = InverseDistanceSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"InverseDistance/#Optimizing-2","page":"InverseDistance","title":"Optimizing","text":"","category":"section"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"size(xys)","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample(), maxiters=10)","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"size(xys)","category":"page"},{"location":"InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = schaffer.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"wendland/#Wendland-Surrogate","page":"Wendland","title":"Wendland Surrogate","text":"","category":"section"},{"location":"wendland/","page":"Wendland","title":"Wendland","text":"The Wendland surrogate is a compact surrogate: it allocates much less memory than other surrogates. The coefficients are found using an iterative solver.","category":"page"},{"location":"wendland/","page":"Wendland","title":"Wendland","text":"f = x - exp(-x^2)","category":"page"},{"location":"wendland/","page":"Wendland","title":"Wendland","text":"using Surrogates\nusing Plots","category":"page"},{"location":"wendland/","page":"Wendland","title":"Wendland","text":"n = 40\nlower_bound = 0.0\nupper_bound = 1.0\nf = x -> exp(-x^2)\nx = sample(n,lower_bound,upper_bound,SobolSample())\ny = f.(x)","category":"page"},{"location":"wendland/","page":"Wendland","title":"Wendland","text":"We choose to sample f in 30 points between 5 to 25 using sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"wendland/#Building-Surrogate","page":"Wendland","title":"Building Surrogate","text":"","category":"section"},{"location":"wendland/","page":"Wendland","title":"Wendland","text":"The choice of the right parameter is especially important here: a slight change in ϵ would produce a totally different fit. Try it yourself with this function!","category":"page"},{"location":"wendland/","page":"Wendland","title":"Wendland","text":"my_eps = 0.5\nwend = Wendland(x,y,lower_bound,upper_bound,eps=my_eps)","category":"page"},{"location":"wendland/","page":"Wendland","title":"Wendland","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(wend, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"ackley/#Ackley-Function","page":"Ackley function","title":"Ackley Function","text":"","category":"section"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"The Ackley function is defined as: f(x) = -a*exp(-bsqrtfrac1dsum_i=1^d x_i^2) - exp(frac1d sum_i=1^d cos(cx_i)) + a + exp(1) Usually the recommended values are: a =  20, b = 02 and c =  2pi","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"Let's see the 1D case.","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"Now, let's define the Ackley function:","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"function ackley(x)\n    a, b, c = 20.0, 0.2, 2.0*π\n    len_recip = inv(length(x))\n    sum_sqrs = zero(eltype(x))\n    sum_cos = sum_sqrs\n    for i in x\n        sum_cos += cos(c*i)\n        sum_sqrs += i^2\n    end\n    return (-a * exp(-b * sqrt(len_recip*sum_sqrs)) -\n            exp(len_recip*sum_cos) + a + 2.71)\nend","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"n = 100\nlb = -32.768\nub = 32.768\nx = sample(n, lb, ub, SobolSample())\ny = ackley.(x)\nxs = lb:0.001:ub\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0,30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"my_rad = RadialBasis(x, y, lb, ub)\nmy_loba = LobachevskySurrogate(x, y, lb, ub)","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"scatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_rad.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, my_loba.(xs), label=\"Lobachevsky\", legend=:top)\n","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"The fit looks good. Let's now see if we are able to find the minimum value using optimization methods:","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"surrogate_optimize(ackley,DYCORS(),lb,ub,my_rad,RandomSample())\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_rad.(xs), label=\"Radial basis optimized\", legend=:top)","category":"page"},{"location":"ackley/","page":"Ackley function","title":"Ackley function","text":"The DYCORS methods successfully finds the minimum.","category":"page"},{"location":"lobachevsky/#Lobachevsky-surrogate-tutorial","page":"Lobachevsky","title":"Lobachevsky surrogate tutorial","text":"","category":"section"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Lobachevsky splines function is a function that is used for univariate and multivariate scattered interpolation. Introduced by Lobachevsky in 1842 to investigate errors in astronomical measurements.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"We are going to use a Lobachevsky surrogate to optimize f(x)=sin(x)+sin(103 * x).","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"lobachevsky/#Sampling","page":"Lobachevsky","title":"Sampling","text":"","category":"section"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"We choose to sample f in 4 points between 0 and 4 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"f(x) = sin(x) + sin(10/3 * x)\nn_samples = 5\nlower_bound = 1.0\nupper_bound = 4.0\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"lobachevsky/#Building-a-surrogate","page":"Lobachevsky","title":"Building a surrogate","text":"","category":"section"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"With our sampled points we can build the Lobachevsky surrogate using the LobachevskySurrogate function.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"lobachevsky_surrogate behaves like an ordinary function which we can simply plot. Alpha is the shape parameters and n specify how close you want lobachevsky function to radial basis function.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"alpha = 2.0\nn = 6\nlobachevsky_surrogate = LobachevskySurrogate(x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(lobachevsky_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"lobachevsky/#Optimizing","page":"Lobachevsky","title":"Optimizing","text":"","category":"section"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, lobachevsky_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(lobachevsky_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"In the example below, it shows how to use lobachevsky_surrogate for higher dimension problems.","category":"page"},{"location":"lobachevsky/#Lobachevsky-Surrogate-Tutorial-(ND):","page":"Lobachevsky","title":"Lobachevsky Surrogate Tutorial (ND):","text":"","category":"section"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"First of all we will define the Schaffer function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction schaffer(x)\n    x1=x[1]\n    x2=x[2]\n    fact1 = x1 ^2;\n    fact2 = x2 ^2;\n    y = fact1 + fact2;\nend","category":"page"},{"location":"lobachevsky/#Sampling-2","page":"Lobachevsky","title":"Sampling","text":"","category":"section"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 8, and 0, 8 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"n_samples = 60\nlower_bound = [0.0, 0.0]\nupper_bound = [8.0, 8.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"x, y = 0:8, 0:8 # hide\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"lobachevsky/#Building-a-surrogate-2","page":"Lobachevsky","title":"Building a surrogate","text":"","category":"section"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Lobachevsky = LobachevskySurrogate(xys, zs,  lower_bound, upper_bound, alpha = [2.4,2.4], n=8)","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"lobachevsky/#Optimizing-2","page":"Lobachevsky","title":"Optimizing","text":"","category":"section"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"size(Lobachevsky.x)","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, Lobachevsky, SobolSample(), maxiters=1, num_new_samples=10)","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"size(Lobachevsky.x)","category":"page"},{"location":"lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\nxys = Lobachevsky.x # hide\nxs = [i[1] for i in xys] # hide\nys = [i[2] for i in xys] # hide\nzs = schaffer.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"randomforest/#Random-forests-surrogate-tutorial","page":"RandomForest","title":"Random forests surrogate tutorial","text":"","category":"section"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"note: Note\nThis surrogate requires the 'SurrogatesRandomForest' module which can be added by inputting \"]add SurrogatesRandomForest\" from the Julia command line. ","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"Random forests is a supervised learning algorithm that randomly creates and merges multiple decision trees into one forest.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"We are going to use a Random forests surrogate to optimize f(x)=sin(x)+sin(103 * x).","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"using Surrogates\nusing SurrogatesRandomForest\nusing Plots\ndefault()","category":"page"},{"location":"randomforest/#Sampling","page":"RandomForest","title":"Sampling","text":"","category":"section"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"f(x) = sin(x) + sin(10/3 * x)\nn_samples = 5\nlower_bound = 2.7\nupper_bound = 7.5\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"randomforest/#Building-a-surrogate","page":"RandomForest","title":"Building a surrogate","text":"","category":"section"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"With our sampled points we can build the Random forests surrogate using the RandomForestSurrogate function.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"randomforest_surrogate behaves like an ordinary function which we can simply plot. Additionally you can specify the number of trees created using the parameter num_round","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"num_round = 2\nrandomforest_surrogate = RandomForestSurrogate(x ,y ,lower_bound, upper_bound, num_round = 2)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(randomforest_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"randomforest/#Optimizing","page":"RandomForest","title":"Optimizing","text":"","category":"section"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, randomforest_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(randomforest_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"randomforest/#Random-Forest-ND","page":"RandomForest","title":"Random Forest ND","text":"","category":"section"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"First of all we will define the Bukin Function N. 6 function we are going to build surrogate for.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction bukin6(x)\n    x1=x[1]\n    x2=x[2]\n    term1 = 100 * sqrt(abs(x2 - 0.01*x1^2));\n    term2 = 0.01 * abs(x1+10);\n    y = term1 + term2;\nend","category":"page"},{"location":"randomforest/#Sampling-2","page":"RandomForest","title":"Sampling","text":"","category":"section"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"n_samples = 50\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = bukin6.(xys);","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"x, y = -5:10, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> bukin6((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> bukin6((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"randomforest/#Building-a-surrogate-2","page":"RandomForest","title":"Building a surrogate","text":"","category":"section"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"using SurrogatesRandomForest\nRandomForest = RandomForestSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"p1 = surface(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"randomforest/#Optimizing-2","page":"RandomForest","title":"Optimizing","text":"","category":"section"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"size(xys)","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"surrogate_optimize(bukin6, SRBF(), lower_bound, upper_bound, RandomForest, SobolSample(), maxiters=20)","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"size(xys)","category":"page"},{"location":"randomforest/","page":"RandomForest","title":"RandomForest","text":"p1 = surface(x, y, (x, y) -> RandomForest([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = bukin6.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"tensor_prod/#Tensor-product-function","page":"Tensor product","title":"Tensor product function","text":"","category":"section"},{"location":"tensor_prod/","page":"Tensor product","title":"Tensor product","text":"The tensor product function is defined as: f(x) = prod_i=1^d cos(api x_i)","category":"page"},{"location":"tensor_prod/","page":"Tensor product","title":"Tensor product","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"tensor_prod/","page":"Tensor product","title":"Tensor product","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"tensor_prod/","page":"Tensor product","title":"Tensor product","text":"Define the 1D objective function:","category":"page"},{"location":"tensor_prod/","page":"Tensor product","title":"Tensor product","text":"function f(x)\n    a = 0.5;\n    return cos(a*pi*x)\nend","category":"page"},{"location":"tensor_prod/","page":"Tensor product","title":"Tensor product","text":"n = 30\nlb = -5.0\nub = 5.0\na = 0.5\nx = sample(n, lb, ub, SobolSample())\ny = f.(x)\nxs = lb:0.001:ub\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(-1, 1), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"tensor_prod/","page":"Tensor product","title":"Tensor product","text":"Fitting and plotting different surrogates:","category":"page"},{"location":"tensor_prod/","page":"Tensor product","title":"Tensor product","text":"loba_1 = LobachevskySurrogate(x, y, lb, ub)\nkrig = Kriging(x, y, lb, ub)\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2.5, 2.5), legend=:bottom)\nplot!(xs,f.(xs), label=\"True function\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"#Welded beam function","category":"page"},{"location":"welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"The welded beam function is defined as: f(hlt) = sqrtfraca^2 + b^2 + ablsqrt025(l^2+(h+t)^2) With: a = frac6000sqrt2hl b = frac6000(14 + 05l)*sqrt025(l^2+(h+t)^2)2*0707hl(fracl^212+025*(h+t)^2)","category":"page"},{"location":"welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"It has 3 dimension.","category":"page"},{"location":"welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"using Surrogates\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"Define the objective function:","category":"page"},{"location":"welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"function f(x)\n    h = x[1]\n    l = x[2]\n    t = x[3]\n    a = 6000/(sqrt(2)*h*l)\n    b = (6000*(14+0.5*l)*sqrt(0.25*(l^2+(h+t)^2)))/(2*(0.707*h*l*(l^2/12 + 0.25*(h+t)^2)))\n    return (sqrt(a^2+b^2 + l*a*b))/(sqrt(0.25*(l^2+(h+t)^2)))\nend","category":"page"},{"location":"welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"n = 300\nd = 3\nlb = [0.125,5.0,5.0]\nub = [1.,10.,10.]\nx = sample(n,lb,ub,SobolSample())\ny = f.(x)\nn_test = 1000\nx_test = sample(n_test,lb,ub,GoldenSample());\ny_true = f.(x_test);","category":"page"},{"location":"welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"my_rad = RadialBasis(x,y,lb,ub)\ny_rad = my_rad.(x_test)\nmse_rad = norm(y_true - y_rad,2)/n_test\nprintln(\"MSE Radial: $mse_rad\")\n\nmy_loba = LobachevskySurrogate(x,y,lb,ub)\ny_loba = my_loba.(x_test)\nmse_rad = norm(y_true - y_loba,2)/n_test\nprintln(\"MSE Lobachevsky: $mse_rad\")","category":"page"},{"location":"parallel/#Parallel-Optimization","page":"Parallel Optimization","title":"Parallel Optimization","text":"","category":"section"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"There are some situations where it can be beneficial to run multiple optimizations in parallel. For example, if your objective function is very expensive to evaluate, you may want to run multiple evaluations in parallel. ","category":"page"},{"location":"parallel/#Ask-Tell-Interface","page":"Parallel Optimization","title":"Ask-Tell Interface","text":"","category":"section"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"To enable parallel optimization, we make use of an Ask-Tell interface. The user will construct the initial surrogate model the same way as for non-parallel surrogate models, but instead of using surrogate_optimize, the user will use potential_optimal_points. This will return the coordinates of points that the optimizer has determined are most useful to evaluate next. How the user evaluates these points is up to them. The Ask-Tell interface requires more manual control than surrogate_optimize, but it allows for more flexibility. After the point has been evaluated, the user will tell the surrogate model the new points with the add_point! function.","category":"page"},{"location":"parallel/#Virtual-Points","page":"Parallel Optimization","title":"Virtual Points","text":"","category":"section"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"To ensure that points of interest returned by potential_optimal_points are sufficiently far from each other, the function makes use of virtual points. They are used as follows:","category":"page"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"potential_optimal_points is told to return n points.\nThe point with the highest merit function value is selected.\nThis point is now treated as a virtual point and is assigned a temporary value that changes the landscape of the merit function. How the the temporary value is chosen depends on the strategy used. (see below)\nThe point with the new highest merit is selected.\nThe process is repeated until n points have been selected.","category":"page"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"The following strategies are available for virtual point selection for all optimization algorithms:","category":"page"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"\"Minimum Constant Liar (MinimumConstantLiar)\":\nThe virtual point is assigned using the lowest known value of the merit function across all evaluated points.\n\"Mean Constant Liar (MeanConstantLiar)\":\nThe virtual point is assigned using the mean of the merit function across all evaluated points.\n\"Maximum Constant Liar (MaximumConstantLiar)\":\nThe virtual point is assigned using the great known value of the merit function across all evaluated points.","category":"page"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"For Kriging surrogates, specifically, the above and follow strategies are available:  ","category":"page"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"\"Kriging Believer (KrigingBeliever):\nThe virtual point is assigned using the mean of the Kriging surrogate at the virtual point.\n\"Kriging Believer Upper Bound (KrigingBelieverUpperBound)\":\nThe virtual point is assigned using 3sigma above the mean of the Kriging surrogate at the virtual point.\n\"Kriging Believer Lower Bound (KrigingBelieverLowerBound)\":\nThe virtual point is assigned using 3sigma below the mean of the Kriging surrogate at the virtual point.","category":"page"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"In general, MinimumConstantLiar and KrigingBelieverLowerBound tend to favor exploitation while MaximumConstantLiar and KrigingBelieverUpperBound tend to favor exploration. MeanConstantLiar and KrigingBeliever tend to be a compromise between the two.","category":"page"},{"location":"parallel/#Examples","page":"Parallel Optimization","title":"Examples","text":"","category":"section"},{"location":"parallel/","page":"Parallel Optimization","title":"Parallel Optimization","text":"using Surrogates\n\nlb = 0.0\nub = 10.0\nf = x -> log(x) * exp(x)\nx = sample(5, lb, ub, SobolSample())\ny = f.(x)\n\nmy_k = Kriging(x, y, lb, ub)\n\nfor _ in 1:10\n    new_x, eis = potential_optimal_points(EI(), MeanConstantLiar(), lb, ub, my_k, SobolSample(), 3)\n    add_point!(my_k, new_x, f.(new_x))\nend","category":"page"},{"location":"polychaos/#Polynomial-chaos-surrogate","page":"Polynomial Chaos","title":"Polynomial chaos surrogate","text":"","category":"section"},{"location":"polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"note: Note\nThis surrogate requires the 'SurrogatesPolyChaos' module which can be added by inputting \"]add SurrogatesPolyChaos\" from the Julia command line. ","category":"page"},{"location":"polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"We can create a surrogate using a polynomial expansion, with a different polynomial basis depending on the distribution of the data we are trying to fit. Under the hood, PolyChaos.jl has been used. It is possible to specify a type of polynomial for each dimension of the problem.","category":"page"},{"location":"polychaos/#Sampling","page":"Polynomial Chaos","title":"Sampling","text":"","category":"section"},{"location":"polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"We choose to sample f in 25 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy, this can be done by passing HaltonSample() to the sample function.","category":"page"},{"location":"polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"using Surrogates\nusing SurrogatesPolyChaos\nusing Plots\ndefault()\n\nn = 20\nlower_bound = 1.0\nupper_bound = 6.0\nx = sample(n,lower_bound,upper_bound,HaltonSample())\nf = x -> log(x)*x + sin(x)\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"polychaos/#Building-a-Surrogate","page":"Polynomial Chaos","title":"Building a Surrogate","text":"","category":"section"},{"location":"polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"poly1 = PolynomialChaosSurrogate(x,y,lower_bound,upper_bound)\npoly2 = PolynomialChaosSurrogate(x,y,lower_bound,upper_bound, op = SurrogatesPolyChaos.GaussOrthoPoly(5))\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(poly1, label=\"First polynomial\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(poly2, label=\"Second polynomial\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"kriging/#Kriging-surrogate-tutorial-(1D)","page":"Kriging","title":"Kriging surrogate tutorial (1D)","text":"","category":"section"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"Kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"We are going to use a Kriging surrogate to optimize f(x)=(6x-2)^2sin(12x-4). (function from Forrester et al. (2008)).","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"kriging/#Sampling","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"# https://www.sfu.ca/~ssurjano/forretal08.html\n# Forrester et al. (2008) Function\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\n\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 1.0\n\nxs = lower_bound:0.001:upper_bound\n\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17))\nplot!(xs, f.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"kriging/#Building-a-surrogate","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"With our sampled points we can build the Kriging surrogate using the Kriging function.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"kriging_surrogate behaves like an ordinary function which we can simply plot. A nice statistical property of this surrogate is being able to calculate the error of the function at each point, we plot this as a confidence interval using the ribbon argument.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(x, y, lower_bound, upper_bound);\n\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)\nplot!(xs, kriging_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p), legend=:top)","category":"page"},{"location":"kriging/#Optimizing","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample())\n\nscatter(x, y, label=\"Sampled points\", ylims=(-7, 7), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)\nplot!(xs, kriging_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p), legend=:top)","category":"page"},{"location":"kriging/#Kriging-surrogate-tutorial-(ND)","page":"Kriging","title":"Kriging surrogate tutorial (ND)","text":"","category":"section"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"First of all let's define the function we are going to build a surrogate for. Notice how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction branin(x)\n    x1=x[1]\n    x2=x[2]\n    a=1;\n    b=5.1/(4*π^2);\n    c=5/π;\n    r=6;\n    s=10;\n    t=1/(8π);\n    a*(x2-b*x1+c*x1-r)^2+s*(1-t)*cos(x1)+s\nend","category":"page"},{"location":"kriging/#Sampling-2","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"n_samples = 10\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, GoldenSample())\nzs = branin.(xys);","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"x, y = -5:10, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> branin((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> branin((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"kriging/#Building-a-surrogate-2","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(xys, zs, lower_bound, upper_bound, p=[2.0, 2.0], theta=[0.03, 0.003])","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"kriging/#Optimizing-2","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"With our surrogate we can now search for the minima of the branin function.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"size(xys)","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"surrogate_optimize(branin, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample(); maxiters = 100, num_new_samples = 10)\n","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"size(xys)","category":"page"},{"location":"kriging/","page":"Kriging","title":"Kriging","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = branin.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"sphere_function/#Sphere-function","page":"Sphere function","title":"Sphere function","text":"","category":"section"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"The sphere function of dimension d is defined as: f(x) = sum_i=1^d x_i^2 with lower bound -10 and upper bound 10.","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"Define the objective function:","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"function sphere_function(x)\n    return sum(x.^2)\nend","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"The 1D case is just a simple parabola, let's plot it:","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"n = 20\nlb = -10\nub = 10\nx = sample(n,lb,ub,SobolSample())\ny = sphere_function.(x)\nxs = lb:0.001:ub\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"Fitting RadialSurrogate with different radial basis:","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"rad_1d_linear = RadialBasis(x,y,lb,ub)\nrad_1d_cubic = RadialBasis(x,y,lb,ub,rad = cubicRadial())\nrad_1d_multiquadric = RadialBasis(x,y,lb,ub, rad = multiquadricRadial())\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)\nplot!(xs, rad_1d_linear.(xs), label=\"Radial surrogate with linear\", legend=:top)\nplot!(xs, rad_1d_cubic.(xs), label=\"Radial surrogate with cubic\", legend=:top)\nplot!(xs, rad_1d_multiquadric.(xs), label=\"Radial surrogate with multiquadric\", legend=:top)","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"Fitting Lobachevsky Surrogate with different values of hyperparameters alpha:","category":"page"},{"location":"sphere_function/","page":"Sphere function","title":"Sphere function","text":"loba_1 = LobachevskySurrogate(x,y,lb,ub)\nloba_2 = LobachevskySurrogate(x,y,lb,ub,alpha = 1.5, n = 6)\nloba_3 = LobachevskySurrogate(x,y,lb,ub,alpha = 0.3, n = 6)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky surrogate 1\", legend=:top)\nplot!(xs, loba_2.(xs), label=\"Lobachevsky surrogate 2\", legend=:top)\nplot!(xs, loba_3.(xs), label=\"Lobachevsky surrogate 3\", legend=:top)","category":"page"},{"location":"variablefidelity/#Variable-fidelity-Surrogates","page":"Variable Fidelity","title":"Variable fidelity Surrogates","text":"","category":"section"},{"location":"variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"With the variable fidelity surrogate, we can specify two different surrogates: one for high fidelity data and one for low fidelity data. By default, the first half samples are considered high fidelity and the second half low fidelity.","category":"page"},{"location":"variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"n = 20\nlower_bound = 1.0\nupper_bound = 6.0\nx = sample(n,lower_bound,upper_bound,SobolSample())\nf = x -> 1/3*x\ny = f.(x)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"varfid = VariableFidelitySurrogate(x,y,lower_bound,upper_bound)","category":"page"},{"location":"variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(varfid, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"cantilever/#Cantilever-Beam-Function","page":"Cantilever beam","title":"Cantilever Beam Function","text":"","category":"section"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"The Cantilever Beam function is defined as: f(wt) = frac4L^3Ewt*sqrt (fracYt^2)^2 + (fracXw^2)^2  With parameters L,E,X and Y given.","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"using Surrogates\nusing SurrogatesPolyChaos\nusing Plots\ndefault()","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Define the objective function:","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"function f(x)\n    t = x[1]\n    w = x[2]\n    L = 100.0\n    E = 2.770674127819261e7\n    X = 530.8038576066307\n    Y = 997.8714938733949\n    return (4*L^3)/(E*w*t)*sqrt( (Y/t^2)^2 + (X/w^2)^2)\nend","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Let's plot it:","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"n = 100\nlb = [1.0,1.0]\nub = [8.0,8.0]\nxys = sample(n,lb,ub,SobolSample());\nzs = f.(xys);\nx, y = 0.0:8.0, 0.0:8.0\np1 = surface(x, y, (x1,x2) -> f((x1,x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> f((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Fitting different Surrogates:","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"mypoly = PolynomialChaosSurrogate(xys, zs,  lb, ub)\nloba = LobachevskySurrogate(xys, zs,  lb, ub)\nrad = RadialBasis(xys,zs,lb,ub)","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Plotting:","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Polynomial expansion\")","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Lobachevsky\")","category":"page"},{"location":"cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> rad([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> rad([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Inverse distance\")","category":"page"},{"location":"LinearSurrogate/#Linear-Surrogate","page":"Linear","title":"Linear Surrogate","text":"","category":"section"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"Linear Surrogate is a linear approach to modeling the relationship between a scalar response or dependent variable and one or more explanatory variables. We will use Linear Surrogate to optimize following function:","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"f(x) = sin(x) + log(x)","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"First of all we have to import these two packages: Surrogates and Plots.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"LinearSurrogate/#Sampling","page":"Linear","title":"Sampling","text":"","category":"section"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"We choose to sample f in 20 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"f(x) = sin(x) + log(x)\nn_samples = 20\nlower_bound = 5.2\nupper_bound = 12.5\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Building-a-Surrogate","page":"Linear","title":"Building a Surrogate","text":"","category":"section"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"With our sampled points we can build the Linear Surrogate using the LinearSurrogate function.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"We can simply calculate linear_surrogate for any value.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"my_linear_surr_1D = LinearSurrogate(x, y, lower_bound, upper_bound)\nadd_point!(my_linear_surr_1D,4.0,7.2)\nadd_point!(my_linear_surr_1D,[5.0,6.0],[8.3,9.7])\nval = my_linear_surr_1D(5.0)","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"Now, we will simply plot linear_surrogate:","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(my_linear_surr_1D, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Optimizing","page":"Linear","title":"Optimizing","text":"","category":"section"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, my_linear_surr_1D, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(my_linear_surr_1D, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Linear-Surrogate-tutorial-(ND)","page":"Linear","title":"Linear Surrogate tutorial (ND)","text":"","category":"section"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"First of all we will define the Egg Holder function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction egg(x)\n    x1=x[1]\n    x2=x[2]\n    term1 = -(x2+47) * sin(sqrt(abs(x2+x1/2+47)));\n    term2 = -x1 * sin(sqrt(abs(x1-(x2+47))));\n    y = term1 + term2;\nend","category":"page"},{"location":"LinearSurrogate/#Sampling-2","page":"Linear","title":"Sampling","text":"","category":"section"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -10, 5, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"n_samples = 50\nlower_bound = [-10.0, 0.0]\nupper_bound = [5.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = egg.(xys);","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"x, y = -10:5, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> egg((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> egg((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"LinearSurrogate/#Building-a-surrogate","page":"Linear","title":"Building a surrogate","text":"","category":"section"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"my_linear_ND = LinearSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"p1 = surface(x, y, (x, y) -> my_linear_ND([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> my_linear_ND([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"LinearSurrogate/#Optimizing-2","page":"Linear","title":"Optimizing","text":"","category":"section"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"size(xys)","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"surrogate_optimize(egg, SRBF(), lower_bound, upper_bound, my_linear_ND, SobolSample(), maxiters=10)","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"size(xys)","category":"page"},{"location":"LinearSurrogate/","page":"Linear","title":"Linear","text":"p1 = surface(x, y, (x, y) -> my_linear_ND([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = egg.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> my_linear_ND([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"optimizations/#Optimization-techniques","page":"Optimization","title":"Optimization techniques","text":"","category":"section"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"SRBF","category":"page"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::SRBF,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function, SRBF, Any, Any, AbstractSurrogate, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"The main idea is to pick the new evaluations from a set of candidate points where each candidate point is generated as an N(0, sigma^2) distributed perturbation from the current best solution. The value of sigma is modified based on progress and follows the same logic as in many trust region methods: we increase sigma if we make a lot of progress (the surrogate is accurate) and decrease sigma when we aren’t able to make progress (the surrogate model is inaccurate). More details about how sigma is updated is given in the original papers.\n\nAfter generating the candidate points, we predict their objective function value and compute the minimum distance to the previously evaluated point. Let the candidate points be denoted by C and let the function value predictions be s(x_i) and the distance values be d(x_i), both rescaled through a linear transformation to the interval [0,1]. This is done to put the values on the same scale. The next point selected for evaluation is the candidate point x that minimizes the weighted-distance merit function:\n\nmerit(x) = ws(x) + (1-w)(1-d(x))\n\nwhere 0 leq w leq 1. That is, we want a small function value prediction and a large minimum distance from the previously evaluated points. The weight w is commonly cycled between a few values to achieve both exploitation and exploration. When w is close to zero, we do pure exploration, while w close to 1 corresponds to exploitation.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"LCBS","category":"page"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::LCBS,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function, LCBS, Any, Any, Any, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Lower Confidence Bound (LCB), a popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to minimize:\n\nLCB(x) = Ex - k * sqrt(Vx)\n\ndefault value k = 2.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"EI","category":"page"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::EI,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function, EI, Any, Any, Any, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Expected Improvement (EI), arguably the most popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to maximize expected improvement:\n\nEI(x) = Emax(f_best-f(x)0)\n\n\n\n\n\n","category":"method"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"DYCORS","category":"page"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::DYCORS,lb,ub,surrn::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function, DYCORS, Any, Any, AbstractSurrogate, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"  surrogate_optimize(obj::Function,::DYCORS,lb::Number,ub::Number,surr1::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)\n\nThis is an implementation of the DYCORS strategy by Regis and Shoemaker: Rommel G Regis and Christine A Shoemaker. Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization. Engineering Optimization, 45(5): 529–555, 2013. This is an extension of the SRBF strategy that changes how the candidate points are generated. The main idea is that many objective functions depend only on a few directions so it may be advantageous to perturb only a few directions. In particular, we use a perturbation probability to perturb a given coordinate and decrease this probability after each function evaluation so fewer coordinates are perturbed later in the optimization.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"SOP","category":"page"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,sop1::SOP,lb::Number,ub::Number,surrSOP::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=min(500*1,5000))","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function, SOP, Number, Number, AbstractSurrogate, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"surrogateoptimize(obj::Function,::SOP,lb::Number,ub::Number,surr::AbstractSurrogate,sampletype::SamplingAlgorithm;maxiters=100,numnewsamples=100)\n\nSOP Surrogate optimization method, following closely the following papers:\n\n- SOP: parallel surrogate global optimization with Pareto center selection for computationally expensive single objective problems by Tipaluck Krityakierne\n- Multiobjective Optimization Using Evolutionary Algorithms by Kalyan Deb\n\n#Suggested number of new_samples = min(500*d,5000)\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#Adding-another-optimization-method","page":"Optimization","title":"Adding another optimization method","text":"","category":"section"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"To add another optimization method, you just need to define a new SurrogateOptimizationAlgorithm and write its corresponding algorithm, overloading the following:","category":"page"},{"location":"optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::NewOptimizationType,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"rosenbrock/#Rosenbrock-function","page":"Rosenbrock","title":"Rosenbrock function","text":"","category":"section"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"The Rosenbrock function is defined as: f(x) = sum_i=1^d-1 (x_i+1-x_i)^2 + (x_i - 1)^2","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"I will treat the 2D version, which is commonly defined as: f(xy) = (1-x)^2 + 100(y-x^2)^2 Let's import Surrogates and Plots:","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Surrogates\nusing SurrogatesPolyChaos\nusing Plots\ndefault()","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Define the objective function:","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"function f(x)\n    x1 = x[1]\n    x2 = x[2]\n    return (1-x1)^2 + 100*(x2-x1^2)^2\nend","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Let's plot it:","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"n = 100\nlb = [0.0,0.0]\nub = [8.0,8.0]\nxys = sample(n,lb,ub,SobolSample());\nzs = f.(xys);\nx, y = 0:8, 0:8\np1 = surface(x, y, (x1,x2) -> f((x1,x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> f((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Fitting different Surrogates:","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"mypoly = PolynomialChaosSurrogate(xys, zs,  lb, ub)\nloba = LobachevskySurrogate(xys, zs, lb, ub)\ninver = InverseDistanceSurrogate(xys, zs,  lb, ub)","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Plotting:","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Polynomial expansion\")","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Lobachevsky\")","category":"page"},{"location":"rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> inver([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> inver([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Inverse distance\")","category":"page"},{"location":"secondorderpoly/#Second-order-polynomial-tutorial","page":"SecondOrderPolynomial","title":"Second order polynomial tutorial","text":"","category":"section"},{"location":"secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"The square polynomial model can be expressed by: y = Xβ + ϵ Where X is the matrix of the linear model augmented by adding 2d columns, containing pair by pair product of variables and variables squared.","category":"page"},{"location":"secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"secondorderpoly/#Sampling","page":"SecondOrderPolynomial","title":"Sampling","text":"","category":"section"},{"location":"secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"f = x -> 3*sin(x) + 10/x\nlb = 3.0\nub = 6.0\nn = 10\nx = sample(n,lb,ub,HaltonSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub))\nplot!(f, label=\"True function\", xlims=(lb, ub))","category":"page"},{"location":"secondorderpoly/#Building-the-surrogate","page":"SecondOrderPolynomial","title":"Building the surrogate","text":"","category":"section"},{"location":"secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"sec = SecondOrderPolynomialSurrogate(x, y, lb, ub)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub))\nplot!(f, label=\"True function\",  xlims=(lb, ub))\nplot!(sec, label=\"Surrogate function\",  xlims=(lb, ub))","category":"page"},{"location":"secondorderpoly/#Optimizing","page":"SecondOrderPolynomial","title":"Optimizing","text":"","category":"section"},{"location":"secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"@show surrogate_optimize(f, SRBF(), lb, ub, sec, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lb, ub))\nplot!(sec, label=\"Surrogate function\",  xlims=(lb, ub))","category":"page"},{"location":"secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"The optimization method successfully found the minima.","category":"page"},{"location":"gekpls/#GEKPLS-Surrogate-Tutorial","page":"GEKPLS","title":"GEKPLS Surrogate Tutorial","text":"","category":"section"},{"location":"gekpls/","page":"GEKPLS","title":"GEKPLS","text":"Gradient Enhanced Kriging with Partial Least Squares Method (GEKPLS) is a surrogate modelling technique that brings down computation time and returns improved accuracy for high-dimensional problems. The Julia implementation of GEKPLS is adapted from the Python version by SMT which is based on this paper.  ","category":"page"},{"location":"gekpls/","page":"GEKPLS","title":"GEKPLS","text":"The following are the inputs when building a GEKPLS surrogate: ","category":"page"},{"location":"gekpls/","page":"GEKPLS","title":"GEKPLS","text":"x - The vector containing the training points\ny - The vector containing the training outputs associated with each of the training points\ngrads - The gradients at each of the input X training points\nn_comp - Number of components to retain for the partial least squares regression (PLS)\ndelta_x -  The step size to use for the first order Taylor approximation\nlb - The lower bound for the training points\nub - The upper bound for the training points\nextra_points - The number of additional points to use for the PLS \ntheta - The hyperparameter to use for the correlation model","category":"page"},{"location":"gekpls/#Basic-GEKPLS-Usage","page":"GEKPLS","title":"Basic GEKPLS Usage","text":"","category":"section"},{"location":"gekpls/","page":"GEKPLS","title":"GEKPLS","text":"The following example illustrates how to use GEKPLS:","category":"page"},{"location":"gekpls/","page":"GEKPLS","title":"GEKPLS","text":"\nusing Surrogates\nusing Zygote\n\nfunction water_flow(x)\n    r_w = x[1]\n    r = x[2]\n    T_u = x[3]\n    H_u = x[4]\n    T_l = x[5]\n    H_l = x[6]\n    L = x[7]\n    K_w = x[8]\n    log_val = log(r/r_w)\n    return (2*pi*T_u*(H_u - H_l))/ ( log_val*(1 + (2*L*T_u/(log_val*r_w^2*K_w)) + T_u/T_l))\nend\n\nn = 1000\nlb = [0.05,100,63070,990,63.1,700,1120,9855]\nub = [0.15,50000,115600,1110,116,820,1680,12045]\nx = sample(n,lb,ub,SobolSample())\ngrads = gradient.(water_flow, x)\ny = water_flow.(x)\nn_test = 100 \nx_test = sample(n_test,lb,ub,GoldenSample()) \ny_true = water_flow.(x_test)\nn_comp = 2\ndelta_x = 0.0001\nextra_points = 2\ninitial_theta = [0.01 for i in 1:n_comp]\ng = GEKPLS(x, y, grads, n_comp, delta_x, lb, ub, extra_points, initial_theta)\ny_pred = g.(x_test)\nrmse = sqrt(sum(((y_pred - y_true).^2)/n_test)) #root mean squared error\nprintln(rmse) #0.0347\n","category":"page"},{"location":"gekpls/#Using-GEKPLS-With-Surrogate-Optimization","page":"GEKPLS","title":"Using GEKPLS With Surrogate Optimization","text":"","category":"section"},{"location":"gekpls/","page":"GEKPLS","title":"GEKPLS","text":"GEKPLS can also be used to find the minimum of a function with the surrogates.jl optimization function. This next example demonstrates how this can be accomplished.","category":"page"},{"location":"gekpls/","page":"GEKPLS","title":"GEKPLS","text":"\n    using Surrogates\n    using Zygote\n\n    function sphere_function(x)\n        return sum(x .^ 2)\n    end\n\n    lb = [-5.0, -5.0, -5.0]\n    ub = [5.0, 5.0, 5.0]\n    n_comp = 2\n    delta_x = 0.0001\n    extra_points = 2\n    initial_theta = [0.01 for i in 1:n_comp]\n    n = 100\n    x = sample(n, lb, ub, SobolSample())\n    grads = gradient.(sphere_function, x)\n    y = sphere_function.(x)\n    g = GEKPLS(x, y, grads, n_comp, delta_x, lb, ub, extra_points, initial_theta)\n    x_point, minima = surrogate_optimize(sphere_function, SRBF(), lb, ub, g,\n                                         RandomSample(); maxiters = 20,\n                                         num_new_samples = 20, needs_gradient = true)\n    println(minima)\n","category":"page"},{"location":"Salustowicz/#Salustowicz-Benchmark-Function","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark Function","text":"","category":"section"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"The true underlying function HyGP had to approximate is the 1D Salustowicz function. The function can be evaluated in the given domain: x in 0 10.","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"The Salustowicz benchmark function is as follows:","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"f(x) = e^-x x^3 cos(x) sin(x) (cos(x) sin^2(x) - 1)","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Let's import these two packages  Surrogates and Plots:","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Now, let's define our objective function:","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"function salustowicz(x)\n    term1 = 2.72^(-x) * x^3 * cos(x) * sin(x);\n    term2 = (cos(x) * sin(x)*sin(x) - 1);\n    y = term1 * term2;\nend","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Let's sample f in 30 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol Sample, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"n_samples = 30\nlower_bound = 0\nupper_bound = 10\nnum_round = 2\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = salustowicz.(x)\nxs = lower_bound:0.001:upper_bound\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Now, let's fit Salustowicz Function with different Surrogates:","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"InverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\nlobachevsky_surrogate = LobachevskySurrogate(x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:topright)\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:topright)\nplot!(xs, InverseDistance.(xs), label=\"InverseDistanceSurrogate\", legend=:topright)\nplot!(xs, lobachevsky_surrogate.(xs), label=\"Lobachevsky\", legend=:topright)","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Not's let's see Kriging Surrogate with different hyper parameter:","category":"page"},{"location":"Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"kriging_surrogate1 = Kriging(x, y, lower_bound, upper_bound, p=0.9);\nkriging_surrogate2 = Kriging(x, y, lower_bound, upper_bound, p=1.5);\nkriging_surrogate3 = Kriging(x, y, lower_bound, upper_bound, p=1.9);\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:topright)\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:topright)\nplot!(xs, kriging_surrogate1.(xs), label=\"kriging_surrogate1\", ribbon=p->std_error_at_point(kriging_surrogate1, p), legend=:topright)\nplot!(xs, kriging_surrogate2.(xs), label=\"kriging_surrogate2\", ribbon=p->std_error_at_point(kriging_surrogate2, p), legend=:topright)\nplot!(xs, kriging_surrogate3.(xs), label=\"kriging_surrogate3\", ribbon=p->std_error_at_point(kriging_surrogate3, p), legend=:topright)","category":"page"},{"location":"abstractgps/#Gaussian-Process-Surrogate-Tutorial","page":"Gaussian Process","title":"Gaussian Process Surrogate Tutorial","text":"","category":"section"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"note: Note\nThis surrogate requires the 'SurrogatesAbstractGPs' module which can be added by inputting \"]add SurrogatesAbstractGPs\" from the Julia command line. ","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"Gaussian Process regression in Surrogates.jl is implemented as a simple wrapper around the AbstractGPs.jl package. AbstractGPs comes with a variety of covariance functions (kernels). See KernelFunctions.jl for examples.","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"tip: Tip\nThe examples below demonstrate the use of AbstractGPs with out-of-the-box settings without hyperparameter optimization (i.e. without changing parameters like lengthscale, signal variance and noise variance.) Beyond hyperparameter optimization, careful initialization of hyperparameters and priors on the parameters is required for this surrogate to work properly. For more details on how to fit GPs in practice, check out A Practical Guide to Gaussian Processes.Also see this example to understand hyperparameter optimization with AbstractGPs.","category":"page"},{"location":"abstractgps/#1D-Example","page":"Gaussian Process","title":"1D Example","text":"","category":"section"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"In the example below, the 'gp_surrogate' assignment code can be commented / uncommented to see how the different kernels influence the predictions. ","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"using Surrogates\nusing Plots\ndefault()\nusing AbstractGPs #required to access different types of kernels\nusing SurrogatesAbstractGPs\n\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 1.0\nxs = lower_bound:0.001:upper_bound\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n#gp_surrogate = AbstractGPSurrogate(x,y, gp=GP(SqExponentialKernel()), Σy=0.05) #example of Squared Exponential Kernel\n#gp_surrogate = AbstractGPSurrogate(x,y, gp=GP(MaternKernel()), Σy=0.05) #example of MaternKernel\ngp_surrogate = AbstractGPSurrogate(x,y, gp=GP(PolynomialKernel(; c=2.0, degree=5)), Σy=0.25)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)\nplot!(0:0.001:1, gp_surrogate.gp_posterior; label=\"Posterior\", ribbon_scale=2)","category":"page"},{"location":"abstractgps/#Optimization-Example","page":"Gaussian Process","title":"Optimization Example","text":"","category":"section"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"This example shows the use of AbstractGP Surrogates to find the minima of a function:","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"using Surrogates\nusing Plots\nusing AbstractGPs\nusing SurrogatesAbstractGPs\n\nf(x) = (x-2)^2\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 4.0\nxs = lower_bound:0.1:upper_bound\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\ngp_surrogate = AbstractGPSurrogate(x,y)\n@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, gp_surrogate, SobolSample())","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"Plotting the function and the sampled points: ","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"scatter(gp_surrogate.x, gp_surrogate.y, label=\"Sampled points\", ylims=(-1.0, 5.0), legend=:top)\nplot!(xs, gp_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(gp_surrogate, p), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"abstractgps/#ND-Example","page":"Gaussian Process","title":"ND Example","text":"","category":"section"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"using Plots\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\")\nusing Surrogates\nusing AbstractGPs\nusing SurrogatesAbstractGPs\n\n\nhypot_func = z -> 3*hypot(z...)+1\nn_samples = 50\nlower_bound = [-1.0, -1.0]\nupper_bound = [1.0, 1.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = hypot_func.(xys);\n\nx, y = -2:2, -2:2 \np1 = surface(x, y, (x1,x2) -> hypot_func((x1,x2))) \nxs = [xy[1] for xy in xys] \nys = [xy[2] for xy in xys] \nscatter!(xs, ys, zs) \np2 = contour(x, y, (x1,x2) -> hypot_func((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"Now let's see how our surrogate performs:","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"gp_surrogate = AbstractGPSurrogate(xys, zs)\np1 = surface(x, y, (x, y) -> gp_surrogate([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> gp_surrogate([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Surrogate\")","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"@show gp_surrogate((0.2,0.2))","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"@show hypot_func((0.2,0.2))","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"And this is our log marginal posterior predictive probability:","category":"page"},{"location":"abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"@show logpdf_surrogate(gp_surrogate)","category":"page"},{"location":"surrogate/#Surrogate","page":"Surrogates","title":"Surrogate","text":"","category":"section"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Every surrogate has a different definition depending on the parameters needed. However, they have in common:","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"add_point!(::AbstractSurrogate,x_new,y_new)\nAbstractSurrogate(value)","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"The first function adds a sample point to the surrogate, thus changing the internal coefficients. The second one calculates the approximation at value.","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Linear surrogate","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"LinearSurrogate(x,y,lb,ub)","category":"page"},{"location":"surrogate/#Surrogates.LinearSurrogate-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.LinearSurrogate","text":"LinearSurrogate(x,y,lb,ub)\n\nBuilds a linear surrogate using GLM.jl\n\n\n\n\n\n","category":"method"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Radial basis function surrogate","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"RadialBasis(x, y, lb, ub; rad::RadialFunction = linearRadial, scale_factor::Real=1.0, sparse = false)","category":"page"},{"location":"surrogate/#Surrogates.RadialBasis-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.RadialBasis","text":"RadialBasis(x,y,lb,ub,rad::RadialFunction, scale_factor::Float = 1.0)\n\nConstructor for RadialBasis surrogate, of the form\n\nf(x) = sum_i=1^N w_i phi(x - boldc_i) boldv^T + boldv^mathrmT  0 boldx \n\nwhere w_i are the weights of polyharmonic splines phi(x) and boldv are coefficients of a polynomial term.\n\nReferences: https://en.wikipedia.org/wiki/Polyharmonic_spline\n\n\n\n\n\n","category":"method"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Kriging surrogate","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Kriging(x,y,p,theta)","category":"page"},{"location":"surrogate/#Surrogates.Kriging-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.Kriging","text":"Kriging(x,y,lb,ub;p=collect(one.(x[1])),theta=collect(one.(x[1])))\n\nConstructor for Kriging surrogate.\n\n(x,y): sampled points\np: array of values 0<=p<2 modeling the    smoothness of the function being approximated in the i-th variable.    low p -> rough, high p -> smooth\ntheta: array of values > 0 modeling how much the function is         changing in the i-th variable.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Lobachevsky surrogate","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"LobachevskySurrogate(x,y,lb,ub; alpha = collect(one.(x[1])),n::Int = 4, sparse = false)\nlobachevsky_integral(loba::LobachevskySurrogate,lb,ub)","category":"page"},{"location":"surrogate/#Surrogates.LobachevskySurrogate-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.LobachevskySurrogate","text":"LobachevskySurrogate(x,y,alpha,n::Int,lb,ub,sparse = false)\n\nBuild the Lobachevsky surrogate with parameters alpha and n.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Surrogates.lobachevsky_integral-Tuple{LobachevskySurrogate, Any, Any}","page":"Surrogates","title":"Surrogates.lobachevsky_integral","text":"lobachevsky_integral(loba::LobachevskySurrogate,lb,ub)\n\nCalculates the integral of the Lobachevsky surrogate, which has a closed form.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Support vector machine surrogate, requires using LIBSVM and using SurrogatesSVM","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"SVMSurrogate(x,y,lb::Number,ub::Number)","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Random forest surrogate, requires using XGBoost and using SurrogatesRandomForest","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"RandomForestSurrogate(x,y,lb,ub;num_round::Int = 1)","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Neural network surrogate, requires using Flux and using SurrogatesFlux","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"NeuralSurrogate(x,y,lb,ub; model = Chain(Dense(length(x[1]),1), first), loss = (x,y) -> Flux.mse(model(x), y),opt = Descent(0.01),n_echos::Int = 1)","category":"page"},{"location":"surrogate/#Creating-another-surrogate","page":"Surrogates","title":"Creating another surrogate","text":"","category":"section"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"It's great that you want to add another surrogate to the library! You will need to:","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Define a new mutable struct and a constructor function\nDefine add_point!(your_surrogate::AbstractSurrogate,x_new,y_new)\nDefine your_surrogate(value) for the approximation","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"Example","category":"page"},{"location":"surrogate/","page":"Surrogates","title":"Surrogates","text":"mutable struct NewSurrogate{X,Y,L,U,C,A,B} <: AbstractSurrogate\n  x::X\n  y::Y\n  lb::L\n  ub::U\n  coeff::C\n  alpha::A\n  beta::B\nend\n\nfunction NewSurrogate(x,y,lb,ub,parameters)\n    ...\n    return NewSurrogate(x,y,lb,ub,calculated\\_coeff,alpha,beta)\nend\n\nfunction add_point!(NewSurrogate,x\\_new,y\\_new)\n\n  nothing\nend\n\nfunction (s::NewSurrogate)(value)\n  return s.coeff*value + s.alpha\nend","category":"page"},{"location":"gramacylee/#Gramacy-and-Lee-Function","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"","category":"section"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Gramacy & Lee Function is a continuous function. It is not convex. The function is defined on 1-dimensional space. It is an unimodal. The function can be defined on any input domain but it is usually evaluated on x in -05 25.","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"The Gramacy & Lee is as follows: f(x) = fracsin(10pi x)2x + (x-1)^4.","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Let's import these two packages Surrogates and Plots:","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"using Surrogates\nusing SurrogatesPolyChaos\nusing Plots\ndefault()","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Now, let's define our objective function:","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"function gramacylee(x)\n    term1 = sin(10*pi*x) / 2*x;\n    term2 = (x - 1)^4;\n    y = term1 + term2;\nend","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Let's sample f in 25 points between -0.5 and 2.5 using the sample function. The sampling points are chosen using a Sobol Sample, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"n = 25\nlower_bound = -0.5\nupper_bound = 2.5\nx = sample(n, lower_bound, upper_bound, SobolSample())\ny = gramacylee.(x)\nxs = lower_bound:0.001:upper_bound\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-5, 20), legend=:top)\nplot!(xs, gramacylee.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Now, let's fit Gramacy & Lee Function with different Surrogates:","category":"page"},{"location":"gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"my_pol = PolynomialChaosSurrogate(x, y, lower_bound, upper_bound)\nloba_1 = LobachevskySurrogate(x, y, lower_bound, upper_bound)\nkrig = Kriging(x, y, lower_bound, upper_bound)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-5, 20), legend=:top)\nplot!(xs, gramacylee.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_pol.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"water_flow/#Water-flow-function","page":"Water Flow function","title":"Water flow function","text":"","category":"section"},{"location":"water_flow/","page":"Water Flow function","title":"Water Flow function","text":"The water flow function is defined as: f(r_wrT_uH_uT_lH_lLK_w) = frac2*pi*T_u(H_u - H_l)log(fracrr_w)*1 + frac2LT_ulog(fracrr_w)*r_w^2*K_w+ fracT_uT_l ","category":"page"},{"location":"water_flow/","page":"Water Flow function","title":"Water Flow function","text":"It has 8 dimensions.","category":"page"},{"location":"water_flow/","page":"Water Flow function","title":"Water Flow function","text":"using Surrogates\nusing SurrogatesPolyChaos\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"water_flow/","page":"Water Flow function","title":"Water Flow function","text":"Define the objective function:","category":"page"},{"location":"water_flow/","page":"Water Flow function","title":"Water Flow function","text":"function f(x)\n    r_w = x[1]\n    r = x[2]\n    T_u = x[3]\n    H_u = x[4]\n    T_l = x[5]\n    H_l = x[6]\n    L = x[7]\n    K_w = x[8]\n    log_val = log(r/r_w)\n    return (2*pi*T_u*(H_u - H_l))/ ( log_val*(1 + (2*L*T_u/(log_val*r_w^2*K_w)) + T_u/T_l))\nend","category":"page"},{"location":"water_flow/","page":"Water Flow function","title":"Water Flow function","text":"n = 180\nd = 8\nlb = [0.05,100,63070,990,63.1,700,1120,9855]\nub = [0.15,50000,115600,1110,116,820,1680,12045]\nx = sample(n,lb,ub,SobolSample())\ny = f.(x)\nn_test = 1000\nx_test = sample(n_test,lb,ub,GoldenSample());\ny_true = f.(x_test);","category":"page"},{"location":"water_flow/","page":"Water Flow function","title":"Water Flow function","text":"my_rad = RadialBasis(x,y,lb,ub)\ny_rad = my_rad.(x_test)\nmy_poly = PolynomialChaosSurrogate(x,y,lb,ub)\ny_poly = my_poly.(x_test)\nmse_rad = norm(y_true - y_rad,2)/n_test\nmse_poly = norm(y_true - y_poly, 2)/n_test\nprintln(\"MSE Radial: $mse_rad\")\nprintln(\"MSE Radial: $mse_poly\")","category":"page"},{"location":"radials/#Radial-Surrogates","page":"Radials","title":"Radial Surrogates","text":"","category":"section"},{"location":"radials/","page":"Radials","title":"Radials","text":"The Radial Basis Surrogate model represents the interpolating function as a linear combination of basis functions, one for each training point. Let's start with something easy to get our hands dirty. I want to build a surrogate for:","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"f(x) = log(x) cdot x^2+x^3","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"Let's choose the Radial Basis Surrogate for 1D. First of all we have to import these two packages: Surrogates and Plots,","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"We choose to sample f in 30 points between 5 to 25 using sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"f(x) = log(x)*x^2 + x^3\nn_samples = 30\nlower_bound = 5.0\nupper_bound = 25.0\nx = sort(sample(n_samples, lower_bound, upper_bound, SobolSample()))\ny = f.(x)\nscatter(x, y, label=\"Sampled Points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(x, y, label=\"True function\", legend=:top)","category":"page"},{"location":"radials/#Building-Surrogate","page":"Radials","title":"Building Surrogate","text":"","category":"section"},{"location":"radials/","page":"Radials","title":"Radials","text":"With our sampled points we can build the Radial Surrogate using the RadialBasis function.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"We can simply calculate radial_surrogate for any value.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"radial_surrogate = RadialBasis(x, y, lower_bound, upper_bound)\nval = radial_surrogate(5.4)","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"We can also use cubic radial basis functions.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"radial_surrogate = RadialBasis(x, y, lower_bound, upper_bound, rad = cubicRadial())\nval = radial_surrogate(5.4)","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"Currently available radial basis functions are linearRadial (the default), cubicRadial, multiquadricRadial, and thinplateRadial.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"Now, we will simply plot radial_surrogate:","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(radial_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"radials/#Optimizing","page":"Radials","title":"Optimizing","text":"","category":"section"},{"location":"radials/","page":"Radials","title":"Radials","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, radial_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\", legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(radial_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"radials/#Radial-Basis-Surrogate-tutorial-(ND)","page":"Radials","title":"Radial Basis Surrogate tutorial (ND)","text":"","category":"section"},{"location":"radials/","page":"Radials","title":"Radials","text":"First of all we will define the Booth function we are going to build the surrogate for:","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"f(x) = (x_1 + 2*x_2 - 7)^2 + (2*x_1 + x_2 - 5)^2","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"Notice, how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction booth(x)\n    x1=x[1]\n    x2=x[2]\n    term1 = (x1 + 2*x2 - 7)^2;\n    term2 = (2*x1 + x2 - 5)^2;\n    y = term1 + term2;\nend","category":"page"},{"location":"radials/#Sampling","page":"Radials","title":"Sampling","text":"","category":"section"},{"location":"radials/","page":"Radials","title":"Radials","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 80 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"n_samples = 80\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = booth.(xys);","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"x, y = -5.0:10.0, 0.0:15.0 # hide\np1 = surface(x, y, (x1,x2) -> booth((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> booth((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"radials/#Building-a-surrogate","page":"Radials","title":"Building a surrogate","text":"","category":"section"},{"location":"radials/","page":"Radials","title":"Radials","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"radial_basis = RadialBasis(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"p1 = surface(x, y, (x, y) -> radial_basis([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> radial_basis([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"radials/#Optimizing-2","page":"Radials","title":"Optimizing","text":"","category":"section"},{"location":"radials/","page":"Radials","title":"Radials","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"size(xys)","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"surrogate_optimize(booth, SRBF(), lower_bound, upper_bound, radial_basis, RandomSample(), maxiters=50)","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"size(xys)","category":"page"},{"location":"radials/","page":"Radials","title":"Radials","text":"p1 = surface(x, y, (x, y) -> radial_basis([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = booth.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> radial_basis([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"multi_objective_opt/#Multi-objective-optimization","page":"Multi objective optimization","title":"Multi objective optimization","text":"","category":"section"},{"location":"multi_objective_opt/#Case-1:-Non-colliding-objective-functions","page":"Multi objective optimization","title":"Case 1: Non colliding objective functions","text":"","category":"section"},{"location":"multi_objective_opt/","page":"Multi objective optimization","title":"Multi objective optimization","text":"using Surrogates\n\nm = 10\nf  = x -> [x^i for i = 1:m]\nlb = 1.0\nub = 10.0\nx  = sample(50, lb, ub, GoldenSample())\ny  = f.(x)\nmy_radial_basis_ego = RadialBasis(x, y, lb, ub)\npareto_set, pareto_front = surrogate_optimize(f, SMB(),lb,ub,my_radial_basis_ego,SobolSample(); maxiters = 10, n_new_look = 100)\n\nm = 5\nf  = x -> [x^i for i =1:m]\nlb = 1.0\nub = 10.0\nx  = sample(50, lb, ub, SobolSample())\ny  = f.(x)\nmy_radial_basis_rtea = RadialBasis(x, y, lb, ub)\nZ = 0.8\nK = 2\np_cross = 0.5\nn_c = 1.0\nsigma = 1.5\nsurrogate_optimize(f,RTEA(Z,K,p_cross,n_c,sigma),lb,ub,my_radial_basis_rtea,SobolSample())\n","category":"page"},{"location":"multi_objective_opt/#Case-2:-objective-functions-with-conflicting-minima","page":"Multi objective optimization","title":"Case 2: objective functions with conflicting minima","text":"","category":"section"},{"location":"multi_objective_opt/","page":"Multi objective optimization","title":"Multi objective optimization","text":"\nf  = x -> [sqrt((x[1] - 4)^2 + 25*(x[2])^2),\n           sqrt((x[1]+4)^2 + 25*(x[2])^2),\n           sqrt((x[1]-3)^2 + (x[2]-1)^2)]\nlb = [2.5,-0.5]\nub = [3.5,0.5]\nx  = sample(50, lb, ub, SobolSample())\ny  = f.(x)\nmy_radial_basis_ego = RadialBasis(x, y, lb, ub)\n#I can find my pareto set and pareto front by calling again the surrogate_optimize function:\npareto_set, pareto_front = surrogate_optimize(f,SMB(),lb,ub,my_radial_basis_ego, SobolSample(); maxiters = 10, n_new_look = 100);","category":"page"},{"location":"BraninFunction/#Branin-Function","page":"Branin function","title":"Branin Function","text":"","category":"section"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"The Branin Function is commonly used as a test function for metamodelling in computer experiments, especially in the context of optimization.","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"The expression of the Branin Function is given as: f(x) = (x_2 - frac514pi^2x_1^2 + frac5pix_1 - 6)^2 + 10(1-frac18pi)cos(x_1) + 10","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"where x = (x_1 x_2) with -5leq x_1 leq 10 0 leq x_2 leq 15","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"First of all we will import these two packages Surrogates and Plots.","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"Now, let's define our objective function:","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"function branin(x)\n      x1 = x[1]\n      x2 = x[2]\n      b = 5.1 / (4*pi^2);\n      c = 5/pi;\n      r = 6;\n      a = 1;\n      s = 10;\n      t = 1 / (8*pi);\n      term1 = a * (x2 - b*x1^2 + c*x1 - r)^2;\n      term2 = s*(1-t)*cos(x1);\n      y = term1 + term2 + s;\nend","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"Now, let's plot it:","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"n_samples = 80\nlower_bound = [-5, 0]\nupper_bound = [10,15]\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = branin.(xys);\nx, y = -5.00:10.00, 0.00:15.00\np1 = surface(x, y, (x1,x2) -> branin((x1,x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs)\np2 = contour(x, y, (x1,x2) -> branin((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"Now it's time to try fitting different surrogates and then we will plot them. We will have a look at the radial basis surrogate Radial Basis Surrogate. :","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"radial_surrogate = RadialBasis(xys, zs, lower_bound, upper_bound)","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> radial_surrogate([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> radial_surrogate([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Radial Surrogate\")","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"Now, we will have a look on Inverse Distance Surrogate:","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"InverseDistance = InverseDistanceSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Inverse Distance Surrogate\") # hide","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"Now, let's talk about Lobachevsky Surrogate:","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"Lobachevsky = LobachevskySurrogate(xys, zs,  lower_bound, upper_bound, alpha = [2.8,2.8], n=8)","category":"page"},{"location":"BraninFunction/","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Lobachevsky Surrogate\") # hide","category":"page"},{"location":"gek/#Gradient-Enhanced-Kriging","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"","category":"section"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Gradient-enhanced Kriging is an extension of kriging which supports gradient information. GEK is usually more accurate than kriging, however, it is not computationally efficient when the number of inputs, the number of sampling points, or both, are high. This is mainly due to the size of the corresponding correlation matrix that increases proportionally with both the number of inputs and the number of sampling points.","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Let's have a look at the following function to use Gradient Enhanced Surrogate: f(x) = sin(x) + 2*x^2","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"First of all, we will import Surrogates and Plots packages:","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"gek/#Sampling","page":"Gradient Enhanced Kriging","title":"Sampling","text":"","category":"section"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"We choose to sample f in 8 points between 0 to 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"n_samples = 10\nlower_bound = 2\nupper_bound = 10\nxs = lower_bound:0.001:upper_bound\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\nf(x) = x^3 - 6x^2 + 4x + 12\ny1 = f.(x)\nder = x -> 3*x^2 - 12*x + 4\ny2 = der.(x)\ny = vcat(y1,y2)\nscatter(x, y1, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"gek/#Building-a-surrogate","page":"Gradient Enhanced Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"With our sampled points we can build the Gradient Enhanced Kriging surrogate using the GEK function.","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"\nmy_gek = GEK(x, y, lower_bound, upper_bound, p = 1.4);\nscatter(x, y1, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(my_gek, label=\"Surrogate function\", ribbon=p->std_error_at_point(my_gek, p), xlims=(lower_bound, upper_bound), legend=:top)\n","category":"page"},{"location":"gek/#Gradient-Enhanced-Kriging-Surrogate-Tutorial-(ND)","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging Surrogate Tutorial (ND)","text":"","category":"section"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"First of all let's define the function we are going to build a surrogate for.","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Now, let's define the function:","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"function leon(x)\n      x1 = x[1]\n      x2 = x[2]\n      term1 = 100*(x2 - x1^3)^2\n      term2 = (1 - x1)^2\n      y = term1 + term2\nend","category":"page"},{"location":"gek/#Sampling-2","page":"Gradient Enhanced Kriging","title":"Sampling","text":"","category":"section"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 10, and 0, 10 for the second dimension. We are taking 80 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"n_samples = 45\nlower_bound = [0, 0]\nupper_bound = [10, 10]\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny1 = leon.(xys);","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"x, y = 0:10, 0:10 # hide\np1 = surface(x, y, (x1,x2) -> leon((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, y1) # hide\np2 = contour(x, y, (x1,x2) -> leon((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"gek/#Building-a-surrogate-2","page":"Gradient Enhanced Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"grad1 = x1 -> 2*(300*(x[1])^5 - 300*(x[1])^2*x[2] + x[1] -1)\ngrad2 = x2 -> 200*(x[2] - (x[1])^3)\nd = 2\nn = 10\nfunction create_grads(n, d, grad1, grad2, y)\n      c = 0\n      y2 = zeros(eltype(y[1]),n*d)\n      for i in 1:n\n            y2[i + c] = grad1(x[i])\n            y2[i + c + 1] = grad2(x[i])\n            c = c + 1\n      end\n      return y2\nend\ny2 = create_grads(n, d, grad2, grad2, y)\ny = vcat(y1,y2)","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"my_GEK = GEK(xys, y, lower_bound, upper_bound, p=[1.9, 1.9])","category":"page"},{"location":"gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"p1 = surface(x, y, (x, y) -> my_GEK([x y])) # hide\nscatter!(xs, ys, y1, marker_z=y1) # hide\np2 = contour(x, y, (x, y) -> my_GEK([x y])) # hide\nscatter!(xs, ys, marker_z=y1) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"lp/#Lp-norm-function","page":"Lp norm","title":"Lp norm function","text":"","category":"section"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"The Lp norm function is defined as: f(x) = sqrtp sum_i=1^d vert x_i vert ^p","category":"page"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"using Surrogates\nusing SurrogatesPolyChaos\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"Define the objective function:","category":"page"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"function f(x,p)\n    return norm(x,p)\nend","category":"page"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"Let's see a simple 1D case:","category":"page"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"n = 30\nlb = -5.0\nub = 5.0\np = 1.3\nx = sample(n,lb,ub,SobolSample())\ny = f.(x,p)\nxs = lb:0.001:ub\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 5), legend=:top)\nplot!(xs,f.(xs,p), label=\"True function\", legend=:top)","category":"page"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"Fitting different Surrogates:","category":"page"},{"location":"lp/","page":"Lp norm","title":"Lp norm","text":"my_pol = PolynomialChaosSurrogate(x,y,lb,ub)\nloba_1 = LobachevskySurrogate(x,y,lb,ub)\nkrig = Kriging(x,y,lb,ub)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 5), legend=:top)\nplot!(xs,f.(xs,p), label=\"True function\", legend=:top)\nplot!(xs, my_pol.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"#Surrogates.jl:-Surrogate-models-and-optimization-for-scientific-machine-learning","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"","category":"section"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"A surrogate model is an approximation method that mimics the behavior of a computationally expensive simulation. In more mathematical terms: suppose we are attempting to optimize a function  f(p), but each calculation of  f is very expensive. It may be the case that we need to solve a PDE for each point or use advanced numerical linear algebra machinery, which is usually costly. The idea is then to develop a surrogate model  g which approximates  f by training on previous data collected from evaluations of  f. The construction of a surrogate model can be seen as a three-step process:","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"Sample selection\nConstruction of the surrogate model\nSurrogate optimization","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"The sampling methods are super important for the behavior of the Surrogate. Sampling can be done through QuasiMonteCarlo.jl, all the functions available there can be used in Surrogates.jl.","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"The available surrogates are:","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"Linear\nRadial Basis\nKriging\nCustom Kriging provided with Stheno\nNeural Network\nSupport Vector Machine\nRandom Forest\nSecond Order Polynomial\nInverse Distance","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"After the surrogate is built, we need to optimize it with respect to some objective function. That is, simultaneously looking for a minimum and sampling the most unknown region. The available optimization methods are:","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"Stochastic RBF (SRBF)\nLower confidence bound strategy (LCBS)\nExpected improvement (EI)\nDynamic coordinate search (DYCORS)","category":"page"},{"location":"#Multi-output-Surrogates","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Multi-output Surrogates","text":"","category":"section"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"In certain situations, the function being modeled may have a multi-dimensional output space. In such a case, the surrogate models can take advantage of correlations between the observed output variables to obtain more accurate predictions.","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"When constructing the original surrogate, each element of the passed y vector should itself be a vector. For example, the following y are all valid.","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"using Surrogates\nusing StaticArrays\n\nx = sample(5, [0.0; 0.0], [1.0; 1.0], SobolSample())\nf_static = (x) -> StaticVector(x[1], log(x[2]*x[1]))\nf = (x) -> [x, log(x)/2]\n\ny = f_static.(x)\ny = f.(x)","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"Currently, the following are implemented as multi-output surrogates:","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"Radial Basis\nNeural Network (via Flux)\nSecond Order Polynomial\nInverse Distance\nCustom Kriging (via Stheno)","category":"page"},{"location":"#Gradients","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Gradients","text":"","category":"section"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"The surrogates implemented here are all automatically differentiable via Zygote. Because of this property, surrogates are useful models for processes which aren't explicitly differentiable, and can be used as layers in, for instance, Flux models.","category":"page"},{"location":"#Installation","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Installation","text":"","category":"section"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"Surrogates is registered in the Julia General Registry. In the REPL:","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"using Pkg\nPkg.add(\"Surrogates\")","category":"page"},{"location":"#Contributing","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Contributing","text":"","category":"section"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Quick-example","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Quick example","text":"","category":"section"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"using Surrogates\nnum_samples = 10\nlb = 0.0\nub = 10.0\n\n#Sampling\nx = sample(num_samples,lb,ub,SobolSample())\nf = x-> log(x)*x^2+x^3\ny = f.(x)\n\n#Creating surrogate\nalpha = 2.0\nn = 6\nmy_lobachevsky = LobachevskySurrogate(x,y,lb,ub,alpha=alpha,n=n)\n\n#Approximating value at 5.0\nvalue = my_lobachevsky(5.0)\n\n#Adding more data points\nsurrogate_optimize(f,SRBF(),lb,ub,my_lobachevsky,RandomSample())\n\n#New approximation\nvalue = my_lobachevsky(5.0)","category":"page"},{"location":"#Reproducibility","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"</details>","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"</details>","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"</details>","category":"page"},{"location":"","page":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","title":"Surrogates.jl: Surrogate models and optimization for scientific machine learning","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"moe/#Mixture-of-Experts-(MOE)","page":"MOE","title":"Mixture of Experts (MOE)","text":"","category":"section"},{"location":"moe/","page":"MOE","title":"MOE","text":"note: Note\nThis surrogate requires the 'SurrogatesMOE' module which can be added by inputting \"]add SurrogatesMOE\" from the Julia command line. ","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"The Mixture of Experts (MOE) Surrogate model represents the interpolating function as a combination of other surrogate models. SurrogatesMOE is a Julia implementation of the Python version from SMT.","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"MOE is most useful when we have a discontinuous function. For example, let's say we want to build a surrogate for the following function:","category":"page"},{"location":"moe/#1D-Example","page":"MOE","title":"1D Example","text":"","category":"section"},{"location":"moe/","page":"MOE","title":"MOE","text":"function discont_1D(x)\n    if x < 0.0\n        return -5.0\n    elseif x >= 0.0\n        return 5.0\n    end\nend\n\nnothing # hide","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"Let's choose the MOE Surrogate for 1D. Note that we have to import the SurrogatesMOE package in addition to Surrogates and Plots.","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"using Surrogates\nusing SurrogatesMOE\nusing Plots\ndefault()\n\nlb = -1.0\nub = 1.0\nx = sample(50, lb, ub, SobolSample())\ny = discont_1D.(x)\nscatter(x, y, label=\"Sampled Points\", xlims=(lb, ub), ylims=(-6.0, 7.0), legend=:top)","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"How does a regular surrogate perform on such a dataset?","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"RAD_1D = RadialBasis(x, y, lb, ub, rad = linearRadial(), scale_factor = 1.0, sparse = false)\nRAD_at0 = RAD_1D(0.0) #true value should be 5.0","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"As we can see, the prediction is far away from the ground truth. Now, how does the MOE perform?","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"expert_types = [\n        RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n                             sparse = false),\n        RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n                        sparse = false)\n    ]\n\nMOE_1D_RAD_RAD = MOE(x, y, expert_types)\nMOE_at0 = MOE_1D_RAD_RAD(0.0)","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"As we can see the accuracy is significantly better. ","category":"page"},{"location":"moe/#Under-the-Hood-How-SurrogatesMOE-Works","page":"MOE","title":"Under the Hood - How SurrogatesMOE Works","text":"","category":"section"},{"location":"moe/","page":"MOE","title":"MOE","text":"First, we create Gaussian Mixture Models for the number of expert types provided using the x and y values. For example, in the above example, we create two clusters. Then, using a small test dataset kept aside from the input data, we choose the best surrogate model for each of the clusters. At prediction time, we use the appropriate surrogate model based on the cluster to which the new point belongs.","category":"page"},{"location":"moe/#N-Dimensional-Example","page":"MOE","title":"N-Dimensional Example","text":"","category":"section"},{"location":"moe/","page":"MOE","title":"MOE","text":"using Surrogates\nusing SurrogatesMOE\n\n# helper to test accuracy of predictors\nfunction rmse(a, b)\n    a = vec(a)\n    b = vec(b)\n    if (size(a) != size(b))\n        println(\"error in inputs\")\n        return\n    end\n    n = size(a, 1)\n    return sqrt(sum((a - b) .^ 2) / n)\nend\n\n# multidimensional input function\nfunction discont_NDIM(x)\n    if (x[1] >= 0.0 && x[2] >= 0.0)\n        return sum(x .^ 2) + 5\n    else\n        return sum(x .^ 2) - 5\n    end\nend\nlb = [-1.0, -1.0]\nub = [1.0, 1.0]\nn = 150\nx = sample(n, lb, ub, RandomSample())\ny = discont_NDIM.(x)\nx_test = sample(10, lb, ub, GoldenSample())\n\nexpert_types = [\n    RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n                            sparse = false),\n    RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n                            sparse = false),\n]\nmoe_nd_rad_rad = MOE(x, y, expert_types, ndim = 2)\nmoe_pred_vals = moe_nd_rad_rad.(x_test)\ntrue_vals = discont_NDIM.(x_test)\nmoe_rmse = rmse(true_vals, moe_pred_vals)\nrbf = RadialBasis(x, y, lb, ub)\nrbf_pred_vals = rbf.(x_test)\nrbf_rmse = rmse(true_vals, rbf_pred_vals)\nprintln(rbf_rmse > moe_rmse)","category":"page"},{"location":"moe/#Usage-Notes-Example-With-Other-Surrogates","page":"MOE","title":"Usage Notes - Example With Other Surrogates","text":"","category":"section"},{"location":"moe/","page":"MOE","title":"MOE","text":"From the above example, simply change or add to the expert types:","category":"page"},{"location":"moe/","page":"MOE","title":"MOE","text":"using Surrogates\n#To use Inverse Distance and Radial Basis Surrogates\nexpert_types = [\n    KrigingStructure(p = [1.0, 1.0], theta = [1.0, 1.0]),\n    InverseDistanceStructure(p = 1.0)\n]\n\n#With 3 Surrogates\nexpert_types = [\n    RadialBasisStructure(radial_function = linearRadial(), scale_factor = 1.0,\n                            sparse = false),\n    LinearStructure(),\n    InverseDistanceStructure(p = 1.0),\n]\nnothing # hide","category":"page"},{"location":"neural/#Neural-network-tutorial","page":"NeuralSurrogate","title":"Neural network tutorial","text":"","category":"section"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"note: Note\nThis surrogate requires the 'SurrogatesFlux' module which can be added by inputting \"]add SurrogatesFlux\" from the Julia command line. ","category":"page"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"It's possible to define a neural network as a surrogate, using Flux. This is useful because we can call optimization methods on it.","category":"page"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"First of all we will define the Schaffer function we are going to build surrogate for.","category":"page"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"using Plots\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates\nusing Flux\nusing SurrogatesFlux\n\nfunction schaffer(x)\n    x1=x[1]\n    x2=x[2]\n    fact1 = x1 ^2;\n    fact2 = x2 ^2;\n    y = fact1 + fact2;\nend","category":"page"},{"location":"neural/#Sampling","page":"NeuralSurrogate","title":"Sampling","text":"","category":"section"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 8, and 0, 8 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"n_samples = 60\nlower_bound = [0.0, 0.0]\nupper_bound = [8.0, 8.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);","category":"page"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"x, y = 0:8, 0:8 # hide\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"neural/#Building-a-surrogate","page":"NeuralSurrogate","title":"Building a surrogate","text":"","category":"section"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"You can specify your own model, optimization function, loss functions and epochs. As always, getting the model right is hardest thing.","category":"page"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"model1 = Chain(\n  Dense(2, 5, σ),\n  Dense(5,2,σ),\n  Dense(2, 1)\n)\nneural = NeuralSurrogate(xys, zs, lower_bound, upper_bound, model = model1, n_echos = 10)","category":"page"},{"location":"neural/#Optimization","page":"NeuralSurrogate","title":"Optimization","text":"","category":"section"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"We can now call an optimization function on the neural network:","category":"page"},{"location":"neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, neural, SobolSample(), maxiters=20, num_new_samples=10)","category":"page"}]
}
