var documenterSearchIndex = {"docs":
[{"location":"tutorials/#Surrogates-101-1","page":"Basics","title":"Surrogates 101","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's start with something easy to get our hands dirty. I want to build a surrogate for f(x) = log(x)*x^2+x^3. Let's choose the radial basis surrogate.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> log(x)*x^2+x^3\nlb = 1.0\nub = 10.0\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub,rad=thinplateRadial)\n\n#I want an approximation at 5.4\napprox = my_radial_basis(5.4)","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now see an example in 2D.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing LinearAlgebra\nf = x -> x[1]*x[2]\nlb = [1.0,2.0]\nub = [10.0,8.5]\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at (1.0,1.4)\napprox = my_radial_basis((1.0,1.4))","category":"page"},{"location":"tutorials/#Kriging-standard-error-1","page":"Basics","title":"Kriging standard error","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now use the Kriging surrogate, which is a single-output Gaussian process. This surrogate has a nice feature: not only does it approximate the solution at a point, it also calculates the standard error at such point. Let's see an example:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> exp(x)*x^2+x^3\nlb = 0.0\nub = 10.0\nx = sample(100,lb,ub,UniformSample())\ny = f.(x)\np = 1.9\nmy_krig = Kriging(x,y,lb,ub,p=p)\n\n#I want an approximation at 5.4\napprox = my_krig(5.4)\n\n#I want to find the standard error at 5.4\nstd_err = std_error_at_point(my_krig,5.4)","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now optimize the Kriging surrogate using Lower confidence bound method, this is just a one-liner:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"surrogate_optimize(f,LCBS(),lb,ub,my_krig,UniformSample())","category":"page"},{"location":"tutorials/#Lobachesky-integral-1","page":"Basics","title":"Lobachesky integral","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"The Lobachesky surrogate has the nice feature of having a closed formula for its integral, which is something that other surrogates are missing. Let's compare it with QuadGK:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing QuadGK\nobj = x -> 3*x + log(x)\na = 1.0\nb = 4.0\nx = sample(2000,a,b,SobolSample())\ny = obj.(x)\nalpha = 2.0\nn = 6\nmy_loba = LobacheskySurrogate(x,y,a,b,alpha=alpha,n=n)\n\n#1D integral\nint_1D = lobachesky_integral(my_loba,a,b)\nint = quadgk(obj,a,b)\nint_val_true = int[1]-int[2]\n@assert int_1D ≈ int_val_true","category":"page"},{"location":"tutorials/#Example-of-NeuralSurrogate-1","page":"Basics","title":"Example of NeuralSurrogate","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Basic example of fitting a neural network on a simple function of two variables.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing Flux\nusing Statistics\n\nf = x -> x[1]^2 + x[2]^2\nbounds = Float32[-1.0, -1.0], Float32[1.0, 1.0]\n# Flux models are in single precision by default.\n# Thus, single precision will also be used here for our training samples.\n\nx_train = sample(100, bounds..., SobolSample())\ny_train = f.(x_train)\n\n# Perceptron with one hidden layer of 20 neurons.\nmodel = Chain(Dense(2, 20, relu), Dense(20, 1))\nloss(x, y) = Flux.mse(model(x), y)\n\n# Training of the neural network\nlearning_rate = 0.1\noptimizer = Descent(learning_rate)  # Simple gradient descent. See Flux documentation for other options.\nn_epochs = 50\nsgt = NeuralSurrogate(x_train, y_train, bounds..., model=model, loss=loss, opt=optimizer, n_echos=n_epochs)\n\n# Testing the new model\nx_test = sample(30, bounds..., SobolSample())\ntest_error = mean(abs2, sgt(x)[1] - f(x) for x in x_test)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"The Inverse Distance Surrogate is an interpolating method and in this method the unknown points are calculated with a weighted average of the sampling points. This model uses the inverse distance between the unknown and training points to predict the unknown point. We do not need to fit this model because the response of an unknown point x is computed with respect to the distance between x and the training points.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Let's optimize following function to use Inverse Distance Surrogate:","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"f(x) = sin(x) + sin(x)^2 + sin(x)^3","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":".","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"First of all, we have to import these two packages: Surrogates and Plots.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"InverseDistance/#Sampling-1","page":"InverseDistance","title":"Sampling","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"We choose to sample f in 25 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy, this can be done by passing LowDiscrepancySample() to the sample function.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"f(x) = sin(x) + sin(x)^2 + sin(x)^3\r\n\r\nn_samples = 25\r\nlower_bound = 0.0\r\nupper_bound = 10.0\r\nx = sample(n_samples, lower_bound, upper_bound, LowDiscrepancySample(2))\r\ny = f.(x)\r\n\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Building-a-Surrogate-1","page":"InverseDistance","title":"Building a Surrogate","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"InverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\r\nadd_point!(InverseDistance, 5.0, f(5.0))\r\nadd_point!(InverseDistance, [5.1,5.2], [f(5.1),f(5.2)])\r\nprediction = InverseDistance(5.0)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Now, we will simply plot InverseDistance:","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(InverseDistance, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Optimizing-1","page":"InverseDistance","title":"Optimizing","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample())\r\nscatter(x, y, label=\"Sampled points\", legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(InverseDistance, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"InverseDistance/#Inverse-Distance-Surrogate-Tutorial-(ND):-1","page":"InverseDistance","title":"Inverse Distance Surrogate Tutorial (ND):","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"First of all we will define the Schaffer function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide\r\n\r\nfunction schaffer(x)\r\n    x1=x[1]\r\n    x2=x[2]\r\n    fact1 = (sin(x1^2-x2^2))^2 - 0.5;\r\n    fact2 = (1 + 0.001*(x1^2+x2^2))^2;\r\n    y = 0.5 + fact1/fact2;\r\nend","category":"page"},{"location":"InverseDistance/#Sampling-2","page":"InverseDistance","title":"Sampling","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"n_samples = 60\r\nlower_bound = [-5.0, 0.0]\r\nupper_bound = [10.0, 15.0]\r\n\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = schaffer.(xys);","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"x, y = -5:10, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"InverseDistance/#Building-a-surrogate-1","page":"InverseDistance","title":"Building a surrogate","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"InverseDistance = InverseDistanceSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"InverseDistance/#Optimizing-2","page":"InverseDistance","title":"Optimizing","text":"","category":"section"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"With our surrogate we can now search for the minimas of the function.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"size(xys)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample(), maxiters=10)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"size(xys)","category":"page"},{"location":"InverseDistance/#","page":"InverseDistance","title":"InverseDistance","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nzs = schaffer.(xys) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2) # hide","category":"page"},{"location":"contributing/#Contributions-1","page":"Contributing","title":"Contributions","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Contributions are very welcome! There are many ways do help:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Opening/solving issues\nMaking the code more efficient\nOpening a new PR with a new Sampling technique, Surrogate or optimization method\nWriting more tutorials with your own unique use case of the library\nYour own idea!","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"You can also contact me on the Julia slack channel at @ludoro.","category":"page"},{"location":"contributing/#List-of-contributors-1","page":"Contributing","title":"List of contributors","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Ludovico Bessi (@ludoro)\nChris Rackauckas (@ChrisRackauckas)\nRohit Singh Rathaur (@TeAmp0is0N)\nAndrea Cognolato (@mrandri19)\nKanav Gupta (@kanav99)","category":"page"},{"location":"randomforest/#Random-forests-surrogate-tutorial-1","page":"RandomForestSurrogate","title":"Random forests surrogate tutorial","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"Random forests is a supervised learning algorithm that randomly creates and merges multiple decision trees into one forest.","category":"page"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"We are going to use a Random forests surrogate to optimize f(x)=sin(x)+sin(103 * x).","category":"page"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"randomforest/#Sampling-1","page":"RandomForestSurrogate","title":"Sampling","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"f(x) = sin(x) + sin(10/3 * x)\nn_samples = 5\nlower_bound = 2.7\nupper_bound = 7.5\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"randomforest/#Building-a-surrogate-1","page":"RandomForestSurrogate","title":"Building a surrogate","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"With our sampled points we can build the Random forests surrogate using the RandomForestSurrogate function.","category":"page"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"randomforest_surrogate behaves like an ordinary function which we can simply plot. Addtionally you can specify the number of trees created using the parameter num_round","category":"page"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"num_round = 2\nrandomforest_surrogate = RandomForestSurrogate(x ,y ,lower_bound, upper_bound, num_round = 2)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(randomforest_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"randomforest/#Optimizing-1","page":"RandomForestSurrogate","title":"Optimizing","text":"","category":"section"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"randomforest/#","page":"RandomForestSurrogate","title":"RandomForestSurrogate","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, randomforest_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(randomforest_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"sphere_function/#Sphere-function-1","page":"Sphere function","title":"Sphere function","text":"","category":"section"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"The sphere function of dimension d is defined as: $ f(x) = \\sum{i=1}^d xi $ with lower bound -10 and upper bound 10.","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"Define the objective function:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"function sphere_function(x)\n    return sum(x.^2)\nend","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"The 1D case is just a simple parabola, let's plot it:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"n = 20\nlb = -10\nub = 10\nx = sample(n,lb,ub,SobolSample())\ny = sphere_function.(x)\nxs = lb:0.001:ub\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120))\nplot!(xs,sphere_function.(xs), label=\"True function\")","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"Fitting RadialSurrogate with different radial basis:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"rad_1d_linear = RadialBasis(x,y,lb,ub)\nrad_1d_cubic = RadialBasis(x,y,lb,ub,rad = cubicRadial)\nrad_1d_multiquadric = RadialBasis(x,y,lb,ub, rad = multiquadricRadial)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120))\nplot!(xs,sphere_function.(xs), label=\"True function\")\nplot!(xs, rad_1d_linear.(xs), label=\"Radial surrogate with linear\")\nplot!(xs, rad_1d_cubic.(xs), label=\"Radial surrogate with cubic\")\nplot!(xs, rad_1d_multiquadric.(xs), label=\"Radial surrogate with multiquadric\")","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"Fitting Lobachesky Surrogate with different values of hyperparameters alpha:","category":"page"},{"location":"sphere_function/#","page":"Sphere function","title":"Sphere function","text":"loba_1 = LobacheskySurrogate(x,y,lb,ub)\nloba_2 = LobacheskySurrogate(x,y,lb,ub,alpha = 1.5, n = 6)\nloba_3 = LobacheskySurrogate(x,y,lb,ub,alpha = 0.3, n = 6)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120))\nplot!(xs,sphere_function.(xs), label=\"True function\")\nplot!(xs, loba_1.(xs), label=\"Lobachesky surrogate 1\")\nplot!(xs, loba_2.(xs), label=\"Lobachesky surrogate 2\")\nplot!(xs, loba_3.(xs), label=\"Lobachesky surrogate 3\")","category":"page"},{"location":"kriging/#Kriging-surrogate-tutorial-(1D)-1","page":"Kriging","title":"Kriging surrogate tutorial (1D)","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"We are going to use a Kriging surrogate to optimize f(x)=(6x-2)^2sin(12x-4). (function from Forrester et al. (2008)).","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"kriging/#Sampling-1","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"# https://www.sfu.ca/~ssurjano/forretal08.html\n# Forrester et al. (2008) Function\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\n\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 1.0\n\nxs = lower_bound:0.001:upper_bound\n\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17))\nplot!(xs, f.(xs), label=\"True function\")","category":"page"},{"location":"kriging/#Building-a-surrogate-1","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"With our sampled points we can build the Kriging surrogate using the Kriging function.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"kriging_surrogate behaves like an ordinary function which we can simply plot. A nice statistical property of this surrogate is being able to calculate the error of the function at each point, we plot this as a confidence interval using the ribbon argument.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(x, y, lower_bound, upper_bound, p=1.9);\n\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17))\nplot!(xs, f.(xs), label=\"True function\")\nplot!(xs, kriging_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p))","category":"page"},{"location":"kriging/#Optimizing-1","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample())\n\nscatter(x, y, label=\"Sampled points\", ylims=(-7, 7))\nplot!(xs, f.(xs), label=\"True function\")\nplot!(xs, kriging_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p))","category":"page"},{"location":"kriging/#Kriging-surrogate-tutorial-(ND)-1","page":"Kriging","title":"Kriging surrogate tutorial (ND)","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"First of all let's define the function we are going to build a surrogate for. Notice how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction branin(x)\n    x1=x[1]\n    x2=x[2]\n    a=1;\n    b=5.1/(4*π^2);\n    c=5/π;\n    r=6;\n    s=10;\n    t=1/(8π);\n    a*(x2-b*x1+c*x1-r)^2+s*(1-t)*cos(x1)+s\nend","category":"page"},{"location":"kriging/#Sampling-2","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"n_samples = 50\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = branin.(xys);","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"x, y = -5:10, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> branin((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> branin((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"kriging/#Building-a-surrogate-2","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(xys, zs, lower_bound, upper_bound, p=[1.9, 1.9])","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"kriging/#Optimizing-2","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"With our surrogate we can now search for the minimas of the branin function.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"size(xys)","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"surrogate_optimize(branin, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample(), maxiters=10)","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"size(xys)","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = branin.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"LinearSurrogate/#Linear-Surrogate-1","page":"LinearSurrogate","title":"Linear Surrogate","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"Linear Surrogate is a linear approach to modeling the relationship between a scalar response or dependent variable and one or more explanatory variables. We will use Linear Surrogate to optimize following function:","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"f(x) = sin(x) + log(x)","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":".","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"First of all we have to import these two packages: Surrogates and Plots.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"LinearSurrogate/#Sampling-1","page":"LinearSurrogate","title":"Sampling","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"We choose to sample f in 20 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"f(x) = sin(x) + log(x)\r\nn_samples = 20\r\nlower_bound = 5.2\r\nupper_bound = 12.5\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny = f.(x)\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\r\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Building-a-Surrogate-1","page":"LinearSurrogate","title":"Building a Surrogate","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"With our sampled points we can build the Linear Surrogate using the LinearSurrogate function.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"We can simply calculate linear_surrogate for any value.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"my_linear_surr_1D = LinearSurrogate(x, y, lower_bound, upper_bound)\r\nadd_point!(my_linear_surr_1D,4.0,7.2)\r\nadd_point!(my_linear_surr_1D,[5.0,6.0],[8.3,9.7])\r\nval = my_linear_surr_1D(5.0)","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"Now, we will simply plot linear_surrogate:","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\r\nplot!(my_linear_surr_1D, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Optimizing-1","page":"LinearSurrogate","title":"Optimizing","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, my_linear_surr_1D, SobolSample())\r\nscatter(x, y, label=\"Sampled points\")\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\r\nplot!(my_linear_surr_1D, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"LinearSurrogate/#Linear-Surrogate-tutorial-(ND)-1","page":"LinearSurrogate","title":"Linear Surrogate tutorial (ND)","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"First of all we will define the Egg Holder function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide\r\n\r\nfunction egg(x)\r\n    x1=x[1]\r\n    x2=x[2]\r\n    term1 = -(x2+47) * sin(sqrt(abs(x2+x1/2+47)));\r\n    term2 = -x1 * sin(sqrt(abs(x1-(x2+47))));\r\n    y = term1 + term2;\r\nend","category":"page"},{"location":"LinearSurrogate/#Sampling-2","page":"LinearSurrogate","title":"Sampling","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -10, 5, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"n_samples = 50\r\nlower_bound = [-10.0, 0.0]\r\nupper_bound = [5.0, 15.0]\r\n\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = egg.(xys);","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"x, y = -10:5, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> egg((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> egg((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"LinearSurrogate/#Building-a-surrogate-1","page":"LinearSurrogate","title":"Building a surrogate","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"my_linear_ND = LinearSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"p1 = surface(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"LinearSurrogate/#Optimizing-2","page":"LinearSurrogate","title":"Optimizing","text":"","category":"section"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"With our surrogate we can now search for the minimas of the function.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"size(xys)","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"surrogate_optimize(egg, SRBF(), lower_bound, upper_bound, my_linear_ND, SobolSample(), maxiters=10)","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"size(xys)","category":"page"},{"location":"LinearSurrogate/#","page":"LinearSurrogate","title":"LinearSurrogate","text":"p1 = surface(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nzs = egg.(xys) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2) # hide","category":"page"},{"location":"optimizations/#Optimization-techniques-1","page":"Optimization","title":"Optimization techniques","text":"","category":"section"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"SRBF","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::SRBF,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,SRBF,Any,Any,AbstractSurrogate,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"The main idea is to pick the new evaluations from a set of candidate points where each candidate point is generated as an N(0, sigma^2) distributed perturbation from the current best solution. The value of sigma is modified based on progress and follows the same logic as in many trust region methods: we increase sigma if we make a lot of progress (the surrogate is accurate) and decrease sigma when we aren’t able to make progress (the surrogate model is inaccurate). More details about how sigma is updated is given in the original papers.\n\nAfter generating the candidate points, we predict their objective function value and compute the minimum distance to the previously evaluated point. Let the candidate points be denoted by C and let the function value predictions be s(x_i) and the distance values be d(x_i), both rescaled through a linear transformation to the interval [0,1]. This is done to put the values on the same scale. The next point selected for evaluation is the candidate point x that minimizes the weighted-distance merit function:\n\nmerit(x) = ws(x) + (1-w)(1-d(x))\n\nwhere 0 leq w leq 1. That is, we want a small function value prediction and a large minimum distance from the previously evaluated points. The weight w is commonly cycled between a few values to achieve both exploitation and exploration. When w is close to zero, we do pure exploration, while w close to 1 corresponds to exploitation.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"LCBS","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::LCBS,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,LCBS,Any,Any,Any,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Lower Confidence Bound (LCB), a popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to minimize:\n\nLCB(x) = Ex - k * sqrt(Vx)\n\ndefault value k = 2.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"EI","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::EI,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,EI,Any,Any,Any,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Expected Improvement (EI), arguably the most popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to maximize expected improvement:\n\nEI(x) = Emax(f_best-f(x)0)\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"DYCORS","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::DYCORS,lb,ub,surrn::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,DYCORS,Any,Any,AbstractSurrogate,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"  surrogate_optimize(obj::Function,::DYCORS,lb::Number,ub::Number,surr1::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)\n\nThis is an implementation of the DYCORS strategy by Regis and Shoemaker: Rommel G Regis and Christine A Shoemaker. Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization. Engineering Optimization, 45(5): 529–555, 2013. This is an extension of the SRBF strategy that changes how the candidate points are generated. The main idea is that many objective functions depend only on a few directions so it may be advantageous to perturb only a few directions. In particular, we use a perturbation probability to perturb a given coordinate and decrease this probability after each function evaluation so fewer coordinates are perturbed later in the optimization.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#Adding-another-optimization-method-1","page":"Optimization","title":"Adding another optimization method","text":"","category":"section"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"To add another optimization method, you just need to define a new SurrogateOptimizationAlgorithm and write its corresponding algorithm, overloading the following:","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::NewOptimizatonType,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"surrogate/#Surrogate-1","page":"Surrogates","title":"Surrogate","text":"","category":"section"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Every surrogate has a different definition depending on the parameters needed. However, they have in common:","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"add_point!(::AbstractSurrogate,x_new,y_new)\nAbstractSurrogate(value)","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"The first function adds a sample point to the surrogate, thus changing the internal coefficients. The second one calculates the approximation at value.","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Linear surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"LinearSurrogate(x,y,lb,ub)","category":"page"},{"location":"surrogate/#Surrogates.LinearSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.LinearSurrogate","text":"LinearSurrogate(x,y,lb,ub)\n\nBuilds a linear surrogate using GLM.jl\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Radial basis function surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"RadialBasis(x, y, lb, ub; rad::RadialFunction = linearRadial, scale_factor::Real=1.0, sparse = false)","category":"page"},{"location":"surrogate/#Surrogates.RadialBasis-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.RadialBasis","text":"RadialBasis(x,y,lb,ub,rad::RadialFunction, scale_factor::Float = 1.0)\n\nConstructor for RadialBasis surrogate\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Kriging surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Kriging(x,y,p,theta)","category":"page"},{"location":"surrogate/#Surrogates.Kriging-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.Kriging","text":"Kriging(x,y,lb,ub;p=collect(one.(x[1])),theta=collect(one.(x[1])))\n\nConstructor for Kriging surrogate.\n\n(x,y): sampled points\np: array of values 0<=p<2 modeling the    smoothness of the function being approximated in the i-th variable.    low p -> rough, high p -> smooth\ntheta: array of values > 0 modeling how much the function is         changing in the i-th variable.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Lobachesky surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"LobacheskySurrogate(x,y,lb,ub; alpha = collect(one.(x[1])),n::Int = 4, sparse = false)\nlobachesky_integral(loba::LobacheskySurrogate,lb,ub)","category":"page"},{"location":"surrogate/#Surrogates.LobacheskySurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.LobacheskySurrogate","text":"LobacheskySurrogate(x,y,alpha,n::Int,lb,ub,sparse = false)\n\nBuild the Lobachesky surrogate with parameters alpha and n.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Surrogates.lobachesky_integral-Tuple{LobacheskySurrogate,Any,Any}","page":"Surrogates","title":"Surrogates.lobachesky_integral","text":"lobachesky_integral(loba::LobacheskySurrogate,lb,ub)\n\nCalculates the integral of the Lobachesky surrogate, which has a closed form.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Support vector machine surrogate, requires using LIBSVM","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"SVMSurrogate(x,y,lb::Number,ub::Number)","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Random forest surrogate, requires using XGBoost","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"RandomForestSurrogate(x,y,lb,ub;num_round::Int = 1)","category":"page"},{"location":"surrogate/#Surrogates.RandomForestSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.RandomForestSurrogate","text":"RandomForestSurrogate(x,y,lb,ub,num_round)\n\nBuild Random forest surrogate. num_round is the number of trees.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Neural network surrogate, requires using Flux","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"NeuralSurrogate(x,y,lb,ub; model = Chain(Dense(length(x[1]),1), first), loss = (x,y) -> Flux.mse(model(x), y),opt = Descent(0.01),n_echos::Int = 1)","category":"page"},{"location":"surrogate/#Surrogates.NeuralSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.NeuralSurrogate","text":"NeuralSurrogate(x,y,lb,ub,model,loss,opt,n_echos)\n\nmodel: Flux layers\nloss: loss function\nopt: optimization function\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Creating-another-surrogate-1","page":"Surrogates","title":"Creating another surrogate","text":"","category":"section"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"It's great that you want to add another surrogate to the library! You will need to:","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Define a new mutable struct and a constructor function\nDefine add_point!(your_surrogate::AbstactSurrogate,x_new,y_new)\nDefine your_surrogate(value) for the approximation","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Example","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"mutable struct NewSurrogate{X,Y,L,U,C,A,B} <: AbstractSurrogate\n  x::X\n  y::Y\n  lb::L\n  ub::U\n  coeff::C\n  alpha::A\n  beta::B\nend\n\nfunction NewSurrogate(x,y,lb,ub,parameters)\n    ...\n    return NewSurrogate(x,y,lb,ub,calculated\\_coeff,alpha,beta)\nend\n\nfunction add_point!(NewSurrogate,x\\_new,y\\_new)\n\n  nothing\nend\n\nfunction (s::NewSurrogate)(value)\n  return s.coeff*value + s.alpha\nend","category":"page"},{"location":"lobachesky/#Lobachevsky-surrogate-tutorial-1","page":"Lobachesky","title":"Lobachevsky surrogate tutorial","text":"","category":"section"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"Lobachevsky splines function is a function that used for univariate and multivariate scattered interpolation. Introduced by Lobachevsky in 1842 to investigate errors in astronomical measurements.","category":"page"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"We are going to use a Lobachevsky surrogate to optimize f(x)=sin(x)+sin(103 * x).","category":"page"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"lobachesky/#Sampling-1","page":"Lobachesky","title":"Sampling","text":"","category":"section"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"We choose to sample f in 4 points between 0 and 4 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"f(x) = sin(x) + sin(10/3 * x)\nn_samples = 5\nlower_bound = 1.0\nupper_bound = 4.0\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"lobachesky/#Building-a-surrogate-1","page":"Lobachesky","title":"Building a surrogate","text":"","category":"section"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"With our sampled points we can build the Lobachevsky surrogate using the LobachevskySurrogate function.","category":"page"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"lobachevsky_surrogate behaves like an ordinary function which we can simply plot. Alpha is the shape parameters and n specify how close you want lobachevsky function to radial basis function.","category":"page"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"alpha = 2.0\nn = 6\nlobachevsky_surrogate = LobacheskySurrogate(x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(lobachevsky_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"lobachesky/#Optimizing-1","page":"Lobachesky","title":"Optimizing","text":"","category":"section"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"lobachesky/#","page":"Lobachesky","title":"Lobachesky","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, lobachevsky_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(lobachevsky_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"radials/#Radial-Surrogates-1","page":"Radials","title":"Radial Surrogates","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Let's start with something easy to get our hands dirty. I want to build a surrogate for f(x) = log(x)*x^2+x^3. Let's choose the Radial Basis Surrogate for 1D.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"using Surrogates\r\nf = x -> log(x)*x^2+x^3\r\nlb = 1.0\r\nub = 10.0\r\nx = sample(50,lb,ub,SobolSample())\r\ny = f.(x)\r\nmy_radial_basis = RadialBasis(x,y,lb,ub)\r\n\r\n#I want an approximation at 5.4\r\napprox = my_radial_basis(5.4)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"For each Surrogates we can call it with different inputs: either (xylbub) or with it's parameters,","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"different for each Surrogates. Let's see for Radial Basis Surrogates:","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"my_radial_basis = RadialBasis(x,y,lb,ub,rad=thinplateRadial)\r\n\r\n#We want an approximation at 5.4\r\napprox = my_radial_basis(5.4)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Now, Let's choose the Radial Basis Surrogate for 2D.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"using Surrogates\r\nusing LinearAlgebra\r\nf = x -> x[1]*x[2]\r\nlb = [1.0,2.0]\r\nub = [10.0,8.5]\r\nx = sample(50,lb,ub,SobolSample())\r\ny = f.(x)\r\nmy_radial_basis = RadialBasis(x,y,lb,ub)\r\n\r\n#I want an approximation at (1.0,1.4)\r\napprox = my_radial_basis((1.0,1.4))","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Let's see an Optimization method for 1D:","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"using Surrogates, LinearAlgebra\r\n##### For 1D #####\r\nlb = 0.0\r\nub = 15.0\r\nobjective_function = x -> 2*x+1\r\nx = [2.5,4.0,6.0]\r\ny = [6.0,9.0,13.0]\r\na = 0.0\r\nb = 6.0\r\n\r\nmy_rad_SRBF1 = RadialBasis(x,y,a,b,rad = linearRadial)\r\nsurrogate_optimize(objective_function,SRBF(),a,b,my_rad_SRBF1,UniformSample())","category":"page"},{"location":"samples/#Samples-1","page":"Samples","title":"Samples","text":"","category":"section"},{"location":"samples/#","page":"Samples","title":"Samples","text":"The syntax for sampling in an interval or region is the following:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,S::SamplingAlgorithm)","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"where lb and ub are, respectively, the lower and upper bounds. There are many sampling algorithms to choose from:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Grid sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"GridSample{T}\nsample(n,lb,ub,S::GridSample)","category":"page"},{"location":"samples/#Surrogates.GridSample","page":"Samples","title":"Surrogates.GridSample","text":"GridSample{T}\n\nT is the step dx for lb:dx:ub\n\n\n\n\n\n","category":"type"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,GridSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,S::GridSample)\n\nReturns a tuple containing numbers in a grid.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Uniform sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::UniformSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,UniformSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::UniformRandom)\n\nReturns a Tuple containing uniform random numbers.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Sobol sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::SobolSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,SobolSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::SobolSampling)\n\nReturns a Tuple containing Sobol sequences.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Latin Hypercube sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::LatinHypercubeSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,LatinHypercubeSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::LatinHypercube)\n\nReturns a Tuple containing LatinHypercube sequences.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Low Discrepancy sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"LowDiscrepancySample{T}\nsample(n,lb,ub,S::LowDiscrepancySample)","category":"page"},{"location":"samples/#Surrogates.LowDiscrepancySample","page":"Samples","title":"Surrogates.LowDiscrepancySample","text":"LowDiscrepancySample{T}\n\nT is the base for the sequence\n\n\n\n\n\n","category":"type"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,LowDiscrepancySample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,S::LowDiscrepancySample)\n\nLow discrepancy sample:\n\nDimension 1: Van der Corput sequence\nDimension > 1: Halton sequence\n\nIf dimension d > 1, all bases must be coprime with each other.\n\n\n\n\n\n","category":"method"},{"location":"samples/#Adding-a-new-sampling-method-1","page":"Samples","title":"Adding a new sampling method","text":"","category":"section"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Adding a new sampling method is a two- step process:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Adding a new SamplingAlgorithm type\nOverloading the sample function with the new type.","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Example","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"struct NewAmazingSamplingAlgorithm{OPTIONAL} <: SamplingAlgorithm end\n\nfunction sample(n,lb,ub,::NewAmazingSamplingAlgorithm)\n    if lb is  Number\n        ...\n        return x\n    else\n        ...\n        return Tuple.(x)\n    end\nend","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"(Image: SurrogatesLogo)","category":"page"},{"location":"#Overview-1","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"A surrogate model is an approximation method that mimics the behavior of a computationally expensive simulation. In more mathematical terms: suppose we are attempting to optimize a function  f(p), but each calculation of  f is very expensive. It may be the case that we need to solve a PDE for each point or use advanced numerical linear algebra machinery, which is usually costly. The idea is then to develop a surrogate model  g which approximates  f by training on previous data collected from evaluations of  f. The construction of a surrogate model can be seen as a three-step process:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Sample selection\nConstruction of the surrogate model\nSurrogate optimization","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The sampling methods are super important for the behavior of the Surrogate. At the moment they are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Grid sample\nUniform sample\nSobol sample\nLatin Hypercube sample\nLow discrepancy sample","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The available surrogates are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Linear\nRadial Basis\nKriging\nCustom Kriging provided with Stheno\nNeural Network\nSupport Vector Machine\nRandom Forest\nSecond Order Polynomial\nInverse Distance","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"After the surrogate is built, we need to optimize it with respect to some objective function. That is, simultaneously looking for a minimum and sampling the most unknown region. The available optimization methods are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Stochastic RBF (SRBF)\nLower confidence bound strategy (LCBS)\nExpected improvement (EI)\nDynamic coordinate search (DYCORS)","category":"page"},{"location":"#Multi-output-Surrogates-1","page":"Overview","title":"Multi-output Surrogates","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"In certain situations, the function being modeled may have a multi-dimensional output space. In such a case, the surrogate models can take advantage of correlations between the observed output variables to obtain more accurate predictions.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"When constructing the original surrogate, each element of the passed y vector should itself be a vector. For example, the following y are all valid.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using Surrogates\nusing StaticArrays\n\nx = sample(5, [0.0; 0.0], [1.0; 1.0], SobolSample())\nf_static = (x) -> StaticVector(x[1], log(x[2]*x[1]))\nf = (x) -> [x, log(x)/2]\n\ny = f_static.(x)\ny = f.(x)","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Currently, the following are implemented as multi-output surrogates:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Radial Basis\nNeural Network (via Flux)\nSecond Order Polynomial\nInverse Distance\nCustom Kriging (via Stheno)","category":"page"},{"location":"#Gradients-1","page":"Overview","title":"Gradients","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"The surrogates implemented here are all automatically differentiable via Zygote. Because of this property, surrogates are useful models for processes which aren't explicitly differentiable, and can be used as layers in, for instance, Flux models.","category":"page"},{"location":"#Installation-1","page":"Overview","title":"Installation","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"Surrogates is registered in the Julia General Registry. In the REPL:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"]add Surrogates","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"You can obtain the current master with:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"]add https://github.com/JuliaDiffEq/Surrogates.jl#master","category":"page"},{"location":"#Quick-example-1","page":"Overview","title":"Quick example","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"using Surrogates\nnum_samples = 10\nlb = 0.0\nub = 10.0\n\n#Sampling\nx = sample(num_samples,lb,ub,SobolSample())\nf = x-> log(x)*x^2+x^3\ny = f.(x)\n\n#Creating surrogate\nalpha = 2.0\nn = 6\nmy_lobachesky = LobacheskySurrogate(x,y,lb,ub,alpha=alpha,n=n)\n\n#Approximating value at 5.0\nvalue = my_lobachesky(5.0)\n\n#Adding more data points\nsurrogate_optimize(f,SRBF(),lb,ub,my_lobachesky,UniformSample())\n\n#New approximation\nvalue = my_lobachesky(5.0)","category":"page"}]
}
