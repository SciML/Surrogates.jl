using Zygote

#=
Kriging (Gaussian process interpolation/regression) method, following these papers:
"Efficient Global Optimization of Expensive Black Box Functions" and
"A Taxonomy of Global Optimization Methods Based on Response Surfaces"
both by DONALD R. JONES
=#
mutable struct Kriging{X, Y, L, U, P, T, M, B, S, R, N} <: AbstractSurrogate
    x::X
    y::Y
    lb::L
    ub::U
    p::P
    theta::T
    mu::M
    b::B
    sigma::S
    inverse_of_R::R
    noise_variance::N
end

"""
Gives the current estimate for array 'val' with respect to the Kriging object k.
"""
function (k::Kriging)(val)

    # Check to make sure dimensions of input matches expected dimension of surrogate
    _check_dimension(k, val)

    n = length(k.x)
    d = length(val)

    return k.mu +
           sum(k.b[i] *
               exp(-sum(k.theta[j] * norm(val[j] - k.x[i][j])^k.p[j] for j in 1:d))
               for i in 1:n)
end

"""
    Returns sqrt of expected mean_squared_error error at the point.
"""
function std_error_at_point(k::Kriging, val)

    # Check to make sure dimensions of input matches expected dimension of surrogate
    _check_dimension(k, val)

    n = length(k.x)
    d = length(k.x[1])
    r = zeros(eltype(k.x[1]), n, 1)
    r = [let
             sum = zero(eltype(k.x[1]))
             for l in 1:d
                 sum = sum + k.theta[l] * norm(val[l] - k.x[i][l])^(k.p[l])
             end
             exp(-sum)
         end
         for i in 1:n]

    one = ones(eltype(k.x[1]), n, 1)
    one_t = one'
    a = r' * k.inverse_of_R * r
    b = one_t * k.inverse_of_R * one

    mean_squared_error = k.sigma * (1 - a[1] + (1 - a[1])^2 / b[1])
    return sqrt(abs(mean_squared_error))
end

"""
Gives the current estimate for 'val' with respect to the Kriging object k.
"""
function (k::Kriging)(val::Number)
    # Check to make sure dimensions of input matches expected dimension of surrogate
    _check_dimension(k, val)
    n = length(k.x)
    return k.mu + sum(k.b[i] * exp(-sum(k.theta * abs(val - k.x[i])^k.p)) for i in 1:n)
end

"""
    Returns sqrt of expected mean_squared_error error at the point.
"""
function std_error_at_point(k::Kriging, val::Number)
    # Check to make sure dimensions of input matches expected dimension of surrogate
    _check_dimension(k, val)
    n = length(k.x)
    r = [exp(-k.theta * abs(val - k.x[i])^k.p) for i in 1:n]
    one = ones(eltype(k.x), n)
    one_t = one'
    a = r' * k.inverse_of_R * r
    b = one_t * k.inverse_of_R * one

    mean_squared_error = k.sigma * (1 - a[1] + (1 - a[1])^2 / b[1])
    return sqrt(abs(mean_squared_error))
end

"""
    Kriging(x, y, lb::Number, ub::Number; p::Number=2.0, theta::Number = 0.5/var(x))

Constructor for type Kriging.

#Arguments:
-(x,y): sampled points
-p: value between 0 and 2 modelling the
   smoothness of the function being approximated, 0-> rough  2-> C^infinity
- theta: value > 0 modeling how much the function is changing in the i-th variable.
"""
function Kriging(x, y, lb::Number, ub::Number; p = 1.99,
                 theta = 0.5 / max(1e-6 * abs(ub - lb), std(x))^p,
                 noise_variance = 0.0)

    # Need autodiff to ignore these checks. When optimizing hyperparameters, these won't
    # matter as the optiization will be constrained to satisfy these by default.
    Zygote.ignore() do
        if length(x) != length(unique(x))
            println("There exists a repetion in the samples, cannot build Kriging.")
            return
        end

        if p > 2.0 || p < 0.0
            throw(ArgumentError("Hyperparameter p must be between 0 and 2! Got: $p."))
        end

        if theta ≤ 0
            throw(ArgumentError("Hyperparameter theta must be positive! Got: $theta"))
        end
    end

    mu, b, sigma, inverse_of_R = _calc_kriging_coeffs(x, y, p, theta, noise_variance)
    Kriging(x, y, lb, ub, p, theta, mu, b, sigma, inverse_of_R, noise_variance)
end

function _calc_kriging_coeffs(x, y, p::Number, theta::Number, noise_variance)
    n = length(x)

    R = [exp(-theta * abs(x[i] - x[j])^p) + (i == j) * noise_variance
         for i in 1:n, j in 1:n]

    # Estimate nugget based on maximum allowed condition number
    # This regularizes R to allow for points being close to each other without R becoming
    # singular, at the cost of slightly relaxing the interpolation condition
    # Derived from "An analytic comparison of regularization methods for Gaussian Processes"
    # by Mohammadi et al (https://arxiv.org/pdf/1602.00853.pdf)
    λ = eigen(R).values
    λmax = λ[end]
    λmin = λ[1]

    κmax = 1e8
    λdiff = λmax - κmax * λmin
    if λdiff ≥ 0
        nugget = λdiff / (κmax - 1)
    else
        nugget = 0.0
    end

    one = ones(eltype(x[1]), n)
    one_t = one'

    R = R + Diagonal(nugget * one)

    inverse_of_R = inv(R)

    mu = (one_t * inverse_of_R * y) / (one_t * inverse_of_R * one)
    b = inverse_of_R * (y - one * mu)
    sigma = ((y - one * mu)' * b) / n
    mu[1], b, sigma[1], inverse_of_R
end

"""
    Kriging(x,y,lb,ub;p=collect(one.(x[1])),theta=collect(one.(x[1])))

Constructor for Kriging surrogate.

- (x,y): sampled points
- p: array of values 0<=p<2 modeling the
     smoothness of the function being approximated in the i-th variable.
     low p -> rough, high p -> smooth
- theta: array of values > 0 modeling how much the function is
          changing in the i-th variable.
"""
function Kriging(x, y, lb, ub; p = 1.99 .* collect(one.(x[1])),
                 theta = [0.5 / max(1e-6 * norm(ub .- lb), std(x_i[i] for x_i in x))^p[i]
                          for i in 1:length(x[1])],
                 noise_variance = 0.0)

    # Need autodiff to ignore these checks. When optimizing hyperparameters, these won't
    # matter as the optiization will be constrained to satisfy these by default.
    Zygote.ignore() do
        if length(x) != length(unique(x))
            println("There exists a repetition in the samples, cannot build Kriging.")
            return
        end

        for i in 1:length(x[1])
            if p[i] > 2.0 || p[i] < 0.0
                throw(ArgumentError("All p must be between 0 and 2! Got: $p."))
            end

            if theta[i] ≤ 0.0
                throw(ArgumentError("All theta must be positive! Got: $theta."))
            end
        end
    end

    mu, b, sigma, inverse_of_R = _calc_kriging_coeffs(x, y, p, theta, noise_variance)
    Kriging(x, y, lb, ub, p, theta, mu, b, sigma, inverse_of_R, noise_variance)
end

function _calc_kriging_coeffs(x, y, p, theta, noise_variance)
    n = length(x)
    d = length(x[1])

    R = [let
             sum = zero(eltype(x[1]))
             for l in 1:d
                 sum = sum + theta[l] * norm(x[i][l] - x[j][l])^p[l]
             end
             exp(-sum) + (i == j) * noise_variance
         end
         for j in 1:n, i in 1:n]

    # Estimate nugget based on maximum allowed condition number
    # This regularizes R to allow for points being close to each other without R becoming
    # singular, at the cost of slightly relaxing the interpolation condition
    # Derived from "An analytic comparison of regularization methods for Gaussian Processes"
    # by Mohammadi et al (https://arxiv.org/pdf/1602.00853.pdf)
    λ = eigen(R).values

    λmax = λ[end]
    λmin = λ[1]

    κmax = 1e8
    λdiff = λmax - κmax * λmin
    if λdiff ≥ 0
        nugget = λdiff / (κmax - 1)
    else
        nugget = 0.0
    end

    one = ones(eltype(x[1]), n)
    one_t = one'

    R = R + Diagonal(nugget * one[:, 1])
    inverse_of_R = inv(R)

    mu = (one_t * inverse_of_R * y) / (one_t * inverse_of_R * one)

    y_minus_1μ = y - one * mu

    b = inverse_of_R * y_minus_1μ

    sigma = (y_minus_1μ' * b) / n

    mu[1], b, sigma[1], inverse_of_R
end

"""
    add_point!(k::Kriging,new_x,new_y)

Adds the new point and its respective value to the sample points.
Warning: If you are just adding a single point, you have to wrap it with [].
Returns the updated Kriging model.

"""
function add_point!(k::Kriging, new_x, new_y)
    if new_x in k.x
        println("Adding a sample that already exists, cannot build Kriging.")
        return
    end
    if (length(new_x) == 1 && length(new_x[1]) == 1) ||
       (length(new_x) > 1 && length(new_x[1]) == 1 && length(k.theta) > 1)
        push!(k.x, new_x)
        push!(k.y, new_y)
    else
        append!(k.x, new_x)
        append!(k.y, new_y)
    end
    k.mu, k.b, k.sigma, k.inverse_of_R = _calc_kriging_coeffs(k.x, k.y, k.p, k.theta,
                                                              k.noise_variance)

    nothing
end

"""
    kriging_log_likelihood(x, y, p, theta, noise_variance = 0.0)
Compute the likelihood of the parameters p, theta and noise variance given the data x and y
for a kriging model. Useful as an objective function in hyperparameter optimization.
"""
function kriging_log_likelihood(x, y, p, theta, noise_variance = 0.0)
    mu, b, sigma, inverse_of_R = _calc_kriging_coeffs(x, y, p, theta, noise_variance)

    n = length(y)
    y_minus_1μ = y - ones(length(y), 1) * mu
    Rinv = inverse_of_R

    term1 = only(-(y_minus_1μ' * inverse_of_R * y_minus_1μ) / 2 / sigma)
    term2 = -log((2π * sigma)^(n / 2) / sqrt(det(Rinv)))

    logpdf = term1 + term2
    return logpdf
end
